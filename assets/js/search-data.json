{"0": {
    "doc": "Not found",
    "title": "404 Not Found",
    "content": "We cannot find the page you are looking for! . Click here to go back home . ",
    "url": "/404.html#404-not-found",
    
    "relUrl": "/404.html#404-not-found"
  },"1": {
    "doc": "Not found",
    "title": "Not found",
    "content": " ",
    "url": "/404.html",
    
    "relUrl": "/404.html"
  },"2": {
    "doc": "Welcome!",
    "title": "Welcome!",
    "content": " ",
    "url": "/",
    
    "relUrl": "/"
  },"3": {
    "doc": "Introduction",
    "title": "Introduction",
    "content": "Elastic Cloud Enterprise by German Edge Cloud (ECE) is a cloud platform hosted and operated by GEC, on which you can run your Elastic workloads in the form of deployments. A deployment is an isolated Elastic Cluster managed by ECE. It consists of several components of the Elastic Stack. For each deployment you can specify the following individually: . | Which of the Elastic Stack components below to use | The size of each component | The number of Availability Zones (1 through 3) over which to distribute them | Which of the GEC provided versions for the Elastic Stack to use | . Each deployment gets its own publicly accessible URLs for Elasticsearch, Kibana, Fleet, and Enterprise Search. ",
    "url": "/ece/intro",
    
    "relUrl": "/ece/intro"
  },"4": {
    "doc": "Introduction",
    "title": "Components of the Elastic Stack",
    "content": "ECE includes all server components currently offered by Elastic: . | Elasticsearch in the 4 tiers Hot, Warm, Cold and Frozen as well as master nodes and coordinating/ingest nodes | Kibana | Machine Learning | Integration Server (Fleet and APM) | Enterprise Search | . Information on the Elasticsearch data tiers is available here: https://www.elastic.co/guide/en/elasticsearch/reference/current/data-tiers.html. ",
    "url": "/ece/intro#components-of-the-elastic-stack",
    
    "relUrl": "/ece/intro#components-of-the-elastic-stack"
  },"5": {
    "doc": "Introduction",
    "title": "Functional Scope of the Components",
    "content": "All ECE deployments are equipped with the Elastic Enterprise license, which means that all components contain all the functionalities of the highest-level Elastic license. This includes machine learning, searchable snapshots in the frozen tier (a very cost-efficient variant for storing rarely accessed data), cross-cluster search, all SIEM (Security Information and Event Management) functionalities of the Elastic Stack, Watcher and Alerting, the brand-new AI Assistant and much more. You can find an overview of the components here: https://www.elastic.co/de/pricing/, and a detailed list here: https://www.elastic.co/de/subscriptions. ",
    "url": "/ece/intro#functional-scope-of-the-components",
    
    "relUrl": "/ece/intro#functional-scope-of-the-components"
  },"6": {
    "doc": "Introduction",
    "title": "Calculation of Deployment Size and Pricing Model",
    "content": "The deployment size is measured in GB RAM. Each Elastic component is also allocated CPU and storage, derived from the RAM size by certain factors. This mapping factor can be different for each component. In most use cases, RAM is the main determining factor for the performance of elastic workloads. Since the mapping is based on fixed factors, one parameter (GB RAM) is sufficient to calculate the other parameters (CPU, storage). This makes the pricing model very transparent and easy to understand. All you need to know is the RAM size of the deployments. Your cost is based on GB of RAM used in your deployment. Consumption is recorded and billed to the hour. The snapshot storage usage in our object storage cluster is billed separately. Internet traffic (in and out) is free of charge. ",
    "url": "/ece/intro#calculation-of-deployment-size-and-pricing-model",
    
    "relUrl": "/ece/intro#calculation-of-deployment-size-and-pricing-model"
  },"7": {
    "doc": "Introduction",
    "title": "Autoscaling Feature",
    "content": "The Elasticsearch and machine learning components come with autoscaling enabled. You don’t have this feature with a self-hosted Elastic Stack or you would have to develop it yourself. With autoscaling, ECE monitors the disk space used by the Elasticsearch components and automatically adds more resources when a threshold is exceeded. As a result, you do not have to worry about the otherwise very common case of a deployment outage due to lack of disk space, even when your data volume grows. Machine learning workloads are initiated at the proper size only when actually needed. The resources for your deployment and thus your costs grow with your data volume. This enables efficient use of resources and cost-effective operation of your workloads on the ECE platform. For each component with autoscaling, an upper limit for scaling can be defined. This way, your invoice amount does not get out of control, e.g. in case of a misconfiguration. ",
    "url": "/ece/intro#autoscaling-feature",
    
    "relUrl": "/ece/intro#autoscaling-feature"
  },"8": {
    "doc": "Introduction",
    "title": "Your Advantages with ECE from GEC",
    "content": "With the ECE solution from GEC you get all the functionalities of the software from Elastic, the market leader in search, SIEM and observability, including the enterprise license. GEC hosts exclusively in highly secure German data centers and carries out all operational tasks from Germany. You get everything from one source from a German contractual partner. In contrast to a “classic” deployment of the Elastic Stack in a cluster with virtual machines, which are limited to the sizes specified by the respective cloud platform, ECE uses Docker containers. These can scale automatically and quickly, if necessary. Fine-grained autoscaling allows you to use resources efficiently and run workloads cost-effectively. ",
    "url": "/ece/intro#your-advantages-with-ece-from-gec",
    
    "relUrl": "/ece/intro#your-advantages-with-ece-from-gec"
  },"9": {
    "doc": "Introduction",
    "title": "Services Offered by GEC",
    "content": ". | GEC provides the cloud environment on which your Elastic workloads run in its secure and certified data centers in Germany. | GEC allows you to distribute your workloads across 3 Availability Zones for high availability. | GEC takes care that enough resources are available, in case your deployment needs to scale up. | GEC monitors the ECE environment and ensures that it is available (99.85 %). Outside office hours, an on-call service is automatically alerted to solve unforeseen problems with the ECE platform as quickly as possible. | GEC ensures regular updates and security patches for the servers. | GEC makes new versions of the Elastic Stack available for installation, typically only a few days after Elastic releases them. However, we do not perform automatic or unsolicited version updates of your deployment (except for EOL versions). | GEC provides URLs that can be reached from the Internet for all deployments, including a TLS certificate for encrypted communication (customer-specific certificates are unfortunately not possible). | GEC offers backup and snapshot storage in the redundant GEC Object Storage Cluster. By default, each deployment has a snapshot policy enabled (which you can change) that automatically backs up your Elasticsearch data to the object storage at regular intervals. This protects you very well against data loss. | All your deployments in the GEC ECE cloud come with the Elastic Enterprise license. You don’t have to buy licenses from Elastic. GEC provides you with everything from a single source. | . ",
    "url": "/ece/intro#services-offered-by-gec",
    
    "relUrl": "/ece/intro#services-offered-by-gec"
  },"10": {
    "doc": "Introduction",
    "title": "Your Responsibilities as a Customer",
    "content": "As a customer, you are responsible for everything that happens within your deployment, in particular: . | Collecting data from client systems and loading that data into Elasticsearch | The configuration of Lifecycle Policies | The creation of users and the assignment of authorizations | Monitoring shard health and repairing it if necessary (for more information, see the elastic.co documentation) | Configuration of alerts - if desired | Restore snapshots when needed | Monitoring the performance of your deployment if you have special requirements for this. We cannot know which performance is acceptable for your application or use case. | . More provisions can be found in the General Terms and Conditions of GEC. ",
    "url": "/ece/intro#your-responsibilities-as-a-customer",
    
    "relUrl": "/ece/intro#your-responsibilities-as-a-customer"
  },"11": {
    "doc": "Introduction",
    "title": "Service Description",
    "content": "For more information, please contact Sales. ",
    "url": "/ece/intro#service-description",
    
    "relUrl": "/ece/intro#service-description"
  },"12": {
    "doc": "Introduction",
    "title": "Steps To Be Performed After the ECE Deployment",
    "content": "We recommend performing the following steps to increase the security of your ECE deployment and load data into Elastic: . Increasing Security . | Create the required spaces, roles and users in Kibana. For more information, see: https://www.elastic.co/guide/en/kibana/current/tutorial-secure-access-to-kibana.html. | Do not use the elastic superuser for daily work in Kibana or to deliver data. | If desired, configure Single Sign-On using SAML, LDAP, Active Directory, OpenID Connect or Kerberos as well as dynamic role mappings, if you need role-based or attribute-based access control. For the time being, to set those parameters in your deployment, contact our support. | If desired, have our support configure traffic filters. This prevents unwanted access to your deployment. | Create service accounts for data delivery (when not using Agent and Fleet, cf. below) and generate API keys for them. For more information see: Grant access using API keys. | . Loading Data into Elastic . | Load data into your cluster. We recommend using the Elastic Agent in cooperation with the Fleet Server. You can find detailed instructions here: Ship Data to ECE. | To avoid connection problems to your cluster, make sure to disable sniffing in the Elasticsearch clients used (e.g. fluentd). Elastic Agent, Filebeat etc. automatically behave correctly. | If you want to migrate data from your existing Elastic cluster to your new deployment, you can find some methods in the Elastic documentation: https://www.elastic.co/guide/en/cloud-enterprise/current/ece-migrating-data.html. | In Kibana, review your index templates and lifecycle policies so you can assign your data to your desired Hot, Warm, Cold, Frozen (and delete) tiers. The lifecycle policies automatically created by Elastic often only have one hot tier configured without an expiry date (see: https://www.elastic.co/guide/en/elasticsearch/reference/current/index-lifecycle-management.html). This applies in particular to the automatically deployed lifecycle policies logs and metrics, which are the most relevant policies when using the Elastic Agent. For an example of configuring a policy that deletes data after 70 days, see: Update the “Default” ILM. | . ",
    "url": "/ece/intro#steps-to-be-performed-after-the-ece-deployment",
    
    "relUrl": "/ece/intro#steps-to-be-performed-after-the-ece-deployment"
  },"13": {
    "doc": "Introduction",
    "title": "Support Services by GEC",
    "content": "For support requests, you can contact our support team, which is available on workdays from 8 a.m. to 6 p.m. The GEC Support can assist you with all topics that are GEC’s responsibility within the scope of the ECE service: . | Availability and accessibility of the platform | Questions about using ECE | Bugs in the Elastic software (we would forward these to Elastic accordingly) | . While we are working on providing you with a self-service interface for managing your ECE deployment in the future, you must initially request the following settings for your deployment from our support. During this time, the services are also included in the scope of support: . | Extended configuration of the deployment (i.e. adjustments to the elasticsearch.yml or kibana.yml) | Entries in the Elasticsearch keystore | Update of the Elastic version of your deployment | Configuration of traffic filters for your deployment | Configuration of trust relationships between deployments so you can use cross-cluster search or cross cluster replication | . The following is not covered by GEC Support: . | General consulting about the Elastic software or using it in your deployment | . If applicable, upon request we may offer some of the uncovered services as paid consulting services. Please contact Sales, if needed. Likewise, we cannot offer 24/7 on-call service for customers. This should also not be necessary, since the proper functionality of the ECE platform is controlled by automated monitoring. If a restriction of the service is noticed, our internal on-call service is automatically alerted and starts rectifying the issue. ",
    "url": "/ece/intro#support-services-by-gec",
    
    "relUrl": "/ece/intro#support-services-by-gec"
  },"14": {
    "doc": "Shipping data to Elastic",
    "title": "Shipping data to Elastic",
    "content": "Once your deployment is created, you might want to ship data to it from your log sources with Elastic Agent and Fleet. This document guides you through the process. ",
    "url": "/ece/shipdata/",
    
    "relUrl": "/ece/shipdata/"
  },"15": {
    "doc": "Shipping data to Elastic",
    "title": "Definition of Terms",
    "content": "Before we start, let’s define some terms you will encounter throughout this document. | TERM | DEFINITION | . | Elastic Agent | The log collection component that will be installed client-side on log sources. It is the only component to install since it brings all the beats and manages them and their configurations on the client. | . | Fleet Server | A component in your deployment that manages all agents downstream. All settings in Fleet are made in Kibana. | . | Integration | In this context, it is a pre-configured (mostly provided by Elastic) set of agent settings, ingest pipelines and Kibana dashboards for a specific type of log source (e.g. Tomcat, nginx, MySQL, Cisco Equipment, …). Integrations are readily available for most common log sources (see: https://www.elastic.co/integrations/data-integrations) | . | Agent Policy | A collection of agent settings and integrations (in the form of a policy) that tells the agent and the underlying beats what data to collect and from where. You need a separate agent policy for each type of log source. When, for example, all your MySQL Databases store their logs in the same local path (which they should anyway), one agent policy for MySQL databases is sufficient. An agent policy can contain multiple integrations and thus an agent can collect logs from multiple applications on the same log source. You cannot have more than one policy for one agent. Let’s assume that on some hosts you only have Apache httpd installed, while on other hosts you have httpd and Tomcat installed. Then you need (at least) two agent policies: one for httpd and one for httpd+tomcat. | . | Enrollment Token | Is required by the agent when starting on the client node. It serves two purposes: First, it provides the initial authentication of the agent to the Fleet and Elastic servers. Second, it references exactly one agent policy, so that Fleet server can tell the agent which log sources to collect data from. Each agent policy has at least one enrollment token. Enrollment tokens are sensitive data by nature and should therefore be stored securely. | . ",
    "url": "/ece/shipdata/#definition-of-terms",
    
    "relUrl": "/ece/shipdata/#definition-of-terms"
  },"16": {
    "doc": "Shipping data to Elastic",
    "title": "Create an Agent Policy",
    "content": "As the agent policy defines for the agent which logs to collect, it must be defined first. Define one agent policy per type of log source. In Kibana, go to Fleet → Agent Policies. Create a new agent policy and give it a meaningful name (it should not be necessary to change the Advanced options). Then click on your new integration, and you will see that one integration is already pre-configured: the system integration, which collects system logs and metrics from the client machine itself. Linux, Windows or macOS? No need to worry, the agent will figure this out and configure correctly. Depending on the type of log source you want to collect data from, add other integrations, e.g. here for Apache products: . By clicking on the tile, you will not only get an overview of the integration’s features and components. After clicking on Add Integration, you can also customize its settings on the following page. The content of that page of course depends a lot on the type of Integration. However, if - for example - you store your MySQL Logs in a different path than the default, you can change that here. It will probably look something like this: . ",
    "url": "/ece/shipdata/#create-an-agent-policy",
    
    "relUrl": "/ece/shipdata/#create-an-agent-policy"
  },"17": {
    "doc": "Shipping data to Elastic",
    "title": "Get the Enrollment Token",
    "content": "For each agent policy, an enrollment token named Default is automatically created. It can be retrieved from Kibana anytime. ",
    "url": "/ece/shipdata/#get-the-enrollment-token",
    
    "relUrl": "/ece/shipdata/#get-the-enrollment-token"
  },"18": {
    "doc": "Shipping data to Elastic",
    "title": "Install the Agent on your Client Node",
    "content": "A wizard in Kibana guides you through the process of installing the agent on the client. For the first rollout of such type, start the installation from Kibana → Fleet → Agents → Add Agent. For more agents using the same policy, you can also automate this, as the installation commands and parameters are identical. Select the correct agent policy for the client you want to deploy. Keep Enroll in Fleet checked. Then, Kibana will present you with different options for the commands to run on the client machine: . The URL Parameter is automatically set to the deployment you are currently in, and the enrollment token points to the agent policy. Once these commands are executed on the client machine, the agent registers itself at the Fleet server and appears in the agent list in Kibana. All subsequent deployments of agents with the same agent policy have the same set of commands and parameters. You don’t have to go through the wizard again. This means you can automate this part in the tool of your choice. ",
    "url": "/ece/shipdata/#install-the-agent-on-your-client-node",
    
    "relUrl": "/ece/shipdata/#install-the-agent-on-your-client-node"
  },"19": {
    "doc": "Shipping data to Elastic",
    "title": "Post-Installation Tasks",
    "content": "Make sure to check the Lifecycle Policies (ILM), including all those deployed by the system like logs and metrics. By default, they are deployed with infinite lifetime. We recommend to change that. Discover the fancy new dashboards that come with each additional integration. Familiarize yourself with the Fleet Menu in Kibana. Check for possible updates of agent versions or integrations. All of this can be done through Fleet in Kibana, you don’t have to log in on the client computers. ",
    "url": "/ece/shipdata/#post-installation-tasks",
    
    "relUrl": "/ece/shipdata/#post-installation-tasks"
  },"20": {
    "doc": "Changing Lifecycle Policies",
    "title": "Changing (default) Lifecycle Policies",
    "content": "This little guide will show you how to adapt the standard Index Lifecycle Policies to your specific needs. | Definition of Terms | Getting the Current ILM Policy | Creating a New ILM Policy . | Using the UI . | Hot Phase | Warm Phase | Cold Phase | Frozen Phase | Delete Phase | . | Using Dev Tools | . | Assigning the Policy to an Index | Rollover the Data Stream | . ",
    "url": "/ece/updateilm/#changing-default-lifecycle-policies",
    
    "relUrl": "/ece/updateilm/#changing-default-lifecycle-policies"
  },"21": {
    "doc": "Changing Lifecycle Policies",
    "title": "Definition of Terms",
    "content": "Before we start, let’s define some terms you will encounter throughout this guide. | Term | Definition | . | ILM | Abbreviation for Index Lifecycle Management. Determines the lifecycle of data and its allocation to the Elastic Data tiers. It is defined in Index Lifecycle Policies. | . | Data stream | A collection of single indices. It could be viewed as a small DNS (Domain Name System) for index routing requests. A data stream can contain multiple indices. | . | Rollover | Process of (automatically) creating a new index within a data stream in the Hot Phase so a single index does not grow indefinitely. It is defined in the Hot Phase of the Index Lifecycle Policy. | . ",
    "url": "/ece/updateilm/#definition-of-terms",
    
    "relUrl": "/ece/updateilm/#definition-of-terms"
  },"22": {
    "doc": "Changing Lifecycle Policies",
    "title": "Getting the Current ILM Policy",
    "content": "When using the Elastic Agent and Fleet, default policies are automatically applied to the data streams. These policies are kept simple and contain only a few phases, often only the hot phase. The policy applied to the data streams can be found in Kibana. | Go to Kibana. | Open the menu and go to Stack Management. | Open Index Management and go to Data Streams. | On the popup on the right you can see the assigned ILM. | . ",
    "url": "/ece/updateilm/#getting-the-current-ilm-policy",
    
    "relUrl": "/ece/updateilm/#getting-the-current-ilm-policy"
  },"23": {
    "doc": "Changing Lifecycle Policies",
    "title": "Creating a New ILM Policy",
    "content": "Using the UI . | To create a new ILM, select Stack Management. | Select Index Lifecycle Management and click Create Policy. | Provide a new name for the policy. | . Hot Phase . The hot phase stores all data that is newly indexed and data that is accessed very frequently. It is always the first phase and thus mandatory. In our example, we have deactivated the setting “Use recommended defaults” to reduce the maximum age of data before rollover from the default 30 days to 7 days. Warm Phase . After the data has been collected in the index for 7 days or the index reaches a size of 50 GB, we want the index to immediately transition to the second phase, called the warm phase. In this tier we lower the priority of the indices and set them to read-only. Cold Phase . After 7 days in the warm phase, data moves to the next phase, called the cold phase. In this phase, we enable searchable snapshots to reduce the storage footprint on the elastic instances. The data is now a searchable snapshot stored in the Object Storage which is already highly available itself. Therefore, we can set the number of replicas and the priority to zero. Frozen Phase . In this phase, actual data is removed from the Elasticsearch nodes and only a small cache remains. When the data is accessed, it is retrieved from the snapshot to make it accessible. Delete Phase . This phase removes indices that have reached a certain age. Using Dev Tools . A new ILM Policy can also be created in the Kibana Dev Tools. The console can be found at Management → Dev Tools. PUT _ilm/policy/&lt;add-some-name-here&gt; { \"policy\": { \"phases\": { \"hot\": { \"min_age\": \"0ms\", \"actions\": { \"rollover\": { \"max_primary_shard_size\": \"50gb\", \"max_age\": \"7d\" }, \"set_priority\": { \"priority\": 100 } } }, \"warm\": { \"min_age\": \"0d\", \"actions\": { \"set_priority\": { \"priority\": 50 }, \"readonly\": {} } }, \"cold\": { \"min_age\": \"7d\", \"actions\": { \"readonly\": {}, \"searchable_snapshot\": { \"snapshot_repository\": \"found-snapshots\", \"force_merge_index\": true }, \"set_priority\": { \"priority\": 0 }, \"allocate\": { \"number_of_replicas\": 0 } } }, \"frozen\": { \"min_age\": \"14d\", \"actions\": { \"searchable_snapshot\": { \"snapshot_repository\": \"found-snapshots\", \"force_merge_index\": true } } }, \"delete\": { \"min_age\": \"70d\", \"actions\": { \"delete\": { \"delete_searchable_snapshot\": true } } } } } } . ",
    "url": "/ece/updateilm/#creating-a-new-ilm-policy",
    
    "relUrl": "/ece/updateilm/#creating-a-new-ilm-policy"
  },"24": {
    "doc": "Changing Lifecycle Policies",
    "title": "Assigning the Policy to an Index",
    "content": "The newly created policy can then be assigned to an Index Template from the Index Lifecycle Policies menu. ",
    "url": "/ece/updateilm/#assigning-the-policy-to-an-index",
    
    "relUrl": "/ece/updateilm/#assigning-the-policy-to-an-index"
  },"25": {
    "doc": "Changing Lifecycle Policies",
    "title": "Rollover the Data Stream",
    "content": "In order to use the new ILM Policy immediately, we need to trigger a rollover for the data stream. This can be done in the Dev Tools, here for the data stream auditbeat-8.5.0: . POST auditbeat-8.5.0/_rollover . ",
    "url": "/ece/updateilm/#rollover-the-data-stream",
    
    "relUrl": "/ece/updateilm/#rollover-the-data-stream"
  },"26": {
    "doc": "Changing Lifecycle Policies",
    "title": "Changing Lifecycle Policies",
    "content": " ",
    "url": "/ece/updateilm/",
    
    "relUrl": "/ece/updateilm/"
  },"27": {
    "doc": "GKS Changelog v2.21",
    "title": "Supported Kubernetes Versions",
    "content": "With the current release, we support the following Kubernetes versions: . | 1.22.15 | 1.23.12 | 1.24.6 | . ",
    "url": "/gks/about/changelog-v2.21/#supported-kubernetes-versions",
    
    "relUrl": "/gks/about/changelog-v2.21/#supported-kubernetes-versions"
  },"28": {
    "doc": "GKS Changelog v2.21",
    "title": "End of Life Announcements",
    "content": "We will end the support of Kubernetes version 1.22 on October 10, 2022. Make sure to upgrade all existing clusters running on version 1.22 at least to version 1.23 before that date. ",
    "url": "/gks/about/changelog-v2.21/#end-of-life-announcements",
    
    "relUrl": "/gks/about/changelog-v2.21/#end-of-life-announcements"
  },"29": {
    "doc": "GKS Changelog v2.21",
    "title": "New Features",
    "content": "Project overview site . We added a project overview page that gives an overview of the content of the project. It is possible to change back to the old page in the user setting. New CNI Versionen . Canal was updated to v3.23 and Cilium to v1.12. Please update your CNI in the cluster. ",
    "url": "/gks/about/changelog-v2.21/#new-features",
    
    "relUrl": "/gks/about/changelog-v2.21/#new-features"
  },"30": {
    "doc": "GKS Changelog v2.21",
    "title": "Kubernetes Related Changes",
    "content": "Upgrade Notes for Kubernetes 1.24 . If you are planning to upgrade to Kubernetes 1.24, refer to the What’s new section of the official Kubernetes v1.24 release notes and make sure that you familiarise yourself with the upcoming changes. For an overview of the changes, refer to the Changes by Kind section of the Changelog. | Urgent Upgrade Notes | Deprecations | API changes | Features | . ",
    "url": "/gks/about/changelog-v2.21/#kubernetes-related-changes",
    
    "relUrl": "/gks/about/changelog-v2.21/#kubernetes-related-changes"
  },"31": {
    "doc": "GKS Changelog v2.21",
    "title": "GKS Changelog v2.21",
    "content": " ",
    "url": "/gks/about/changelog-v2.21/",
    
    "relUrl": "/gks/about/changelog-v2.21/"
  },"32": {
    "doc": "GKS Changelog v2.20",
    "title": "New URL",
    "content": "The iMKE dashboard will be renamed to GEC Kubernetes Services (GKS) dashboard, as the new name reflects the affiliation to German Edge Cloud better than the old one. With this name change, the domain changes as well. As of July 1, 2022, the service is reachable with the URL https://gks.gec.io. ",
    "url": "/gks/about/changelog-v2.20/#new-url",
    
    "relUrl": "/gks/about/changelog-v2.20/#new-url"
  },"33": {
    "doc": "GKS Changelog v2.20",
    "title": "Supported Kubernetes Versions",
    "content": "With the current release, we support the following Kubernetes versions: . | 1.21.8 | 1.22.5 | 1.23.6 | . ",
    "url": "/gks/about/changelog-v2.20/#supported-kubernetes-versions",
    
    "relUrl": "/gks/about/changelog-v2.20/#supported-kubernetes-versions"
  },"34": {
    "doc": "GKS Changelog v2.20",
    "title": "End of Life Announcements",
    "content": "We will end the support of Kubernetes version 1.21 on June 28, 2022. Make sure to upgrade all existing clusters running on version 1.21 at least to version 1.22 before that date. ",
    "url": "/gks/about/changelog-v2.20/#end-of-life-announcements",
    
    "relUrl": "/gks/about/changelog-v2.20/#end-of-life-announcements"
  },"35": {
    "doc": "GKS Changelog v2.20",
    "title": "New Features",
    "content": "This is a technical release without any new features. ",
    "url": "/gks/about/changelog-v2.20/#new-features",
    
    "relUrl": "/gks/about/changelog-v2.20/#new-features"
  },"36": {
    "doc": "GKS Changelog v2.20",
    "title": "Bugfixes",
    "content": ". | For user clusters that use etcd 3.5 (Kubernetes 1.22 clusters), etcd corruption checks are turned on to detect etcd data consistency issues. Checks run at etcd startup and every 4 hours (#13766) | . ",
    "url": "/gks/about/changelog-v2.20/#bugfixes",
    
    "relUrl": "/gks/about/changelog-v2.20/#bugfixes"
  },"37": {
    "doc": "GKS Changelog v2.20",
    "title": "Kubernetes Related Changes",
    "content": "Upgrade Notes for Kubernetes 1.23 . If you are planning to upgrade to Kubernetes 1.23, refer to the What’s new section of the official Kubernetes v1.23 release notes and make sure that you familiarise yourself with the upcoming changes. For an overview of the changes, refer to the Changes by Kind section of the Changelog. | Urgent Upgrade Notes | Deprecations | API changes | Features | . ",
    "url": "/gks/about/changelog-v2.20/#kubernetes-related-changes",
    
    "relUrl": "/gks/about/changelog-v2.20/#kubernetes-related-changes"
  },"38": {
    "doc": "GKS Changelog v2.20",
    "title": "GKS Changelog v2.20",
    "content": " ",
    "url": "/gks/about/changelog-v2.20/",
    
    "relUrl": "/gks/about/changelog-v2.20/"
  },"39": {
    "doc": "GKS Changelog v2.19",
    "title": "Supported Kubernetes versions",
    "content": "With the current release, we support the following Kubernetes versions: . | 1.21.8 | 1.22.5 | . ",
    "url": "/gks/about/changelog-v2.19/#supported-kubernetes-versions",
    
    "relUrl": "/gks/about/changelog-v2.19/#supported-kubernetes-versions"
  },"40": {
    "doc": "GKS Changelog v2.19",
    "title": "End of Life announcements",
    "content": "We will end the support of Kubernetes version 1.21 on the 28th June 2022. Please make sure to upgrade all existing clusters running 1.21 at least to 1.22 before that date. ",
    "url": "/gks/about/changelog-v2.19/#end-of-life-announcements",
    
    "relUrl": "/gks/about/changelog-v2.19/#end-of-life-announcements"
  },"41": {
    "doc": "GKS Changelog v2.19",
    "title": "New features",
    "content": ". | CNI update functionality: the CNI canal can now be updated via the dashboard | . ",
    "url": "/gks/about/changelog-v2.19/#new-features",
    
    "relUrl": "/gks/about/changelog-v2.19/#new-features"
  },"42": {
    "doc": "GKS Changelog v2.19",
    "title": "Bugfixes",
    "content": ". | Updated supported Kubernetes versions to newest patch-levels to mitigate several CVEs (CVE-2021-44716, CVE-2021-44717, CVE-2021-3711, CVE-2021-3712, CVE-2021-33910). Cluster control planes have been automatically updated. Please update your Machine Deployments soon, at a time convenient for you. Updating your Machine Deployment will lead to worker node recreation and rolling restarts of your Deployments and Statefulsets. | . ",
    "url": "/gks/about/changelog-v2.19/#bugfixes",
    
    "relUrl": "/gks/about/changelog-v2.19/#bugfixes"
  },"43": {
    "doc": "GKS Changelog v2.19",
    "title": "Kubernetes related changes",
    "content": "Upgrade notes for Kubernetes 1.22 . If you are planning to upgrade to Kubernetes 1.22, please have a look at the What’s new section of the official Kubernetes v1.22 release notes and make sure you familiarise yourself with the upcoming changes. For an overview of the changes, please refer to the Changes by Kind section of the Changelog. | Urgent Upgrade Notes | Deprecations | API changes | Features | . ",
    "url": "/gks/about/changelog-v2.19/#kubernetes-related-changes",
    
    "relUrl": "/gks/about/changelog-v2.19/#kubernetes-related-changes"
  },"44": {
    "doc": "GKS Changelog v2.19",
    "title": "GKS Changelog v2.19",
    "content": " ",
    "url": "/gks/about/changelog-v2.19/",
    
    "relUrl": "/gks/about/changelog-v2.19/"
  },"45": {
    "doc": "GKS Changelog v2.18",
    "title": "Supported Kubernetes versions",
    "content": "With the current release, we support the following Kubernetes versions: . | 1.19.15 | 1.20.11 | 1.21.5 | 1.22.2 | . ",
    "url": "/gks/about/changelog-v2.18/#supported-kubernetes-versions",
    
    "relUrl": "/gks/about/changelog-v2.18/#supported-kubernetes-versions"
  },"46": {
    "doc": "GKS Changelog v2.18",
    "title": "End of Life announcements",
    "content": "We will end the support of Kubernetes version 1.19 on the 3rd November 2021. Please make sure to upgrade all existing clusters running 1.19 at least to 1.20 before that date. ",
    "url": "/gks/about/changelog-v2.18/#end-of-life-announcements",
    
    "relUrl": "/gks/about/changelog-v2.18/#end-of-life-announcements"
  },"47": {
    "doc": "GKS Changelog v2.18",
    "title": "New features",
    "content": ". | Cluster Templates are now supported | Older clusters (created before v1.17) can now be migrated to the external cloud controller manager | containerd is now supported as container runtime (How to migrate your clusters to containerd) | . ",
    "url": "/gks/about/changelog-v2.18/#new-features",
    
    "relUrl": "/gks/about/changelog-v2.18/#new-features"
  },"48": {
    "doc": "GKS Changelog v2.18",
    "title": "Bugfixes",
    "content": ". | Updated supported Kubernetes versions to newest patch-levels to mitigate CVE-2021-25741. Cluster control planes have been automatically updated. Please update your Machine Deployments soon, at a time convenient for you. Updating your Machine Deployment will lead to worker node recreation and rolling restarts of your Deployments and Statefulsets. | . ",
    "url": "/gks/about/changelog-v2.18/#bugfixes",
    
    "relUrl": "/gks/about/changelog-v2.18/#bugfixes"
  },"49": {
    "doc": "GKS Changelog v2.18",
    "title": "Kubernetes related changes",
    "content": "Upgrade notes for Kubernetes 1.21 . If you are planning to upgrade to Kubernetes 1.21, please have a look at the What’s new section of the official Kubernetes v1.21 release notes and make sure you familiarize yourself with the upcoming changes. For an overview of the changes, please refer to the Changes by Kind section of the Changelog. | Urgent Upgrade Notes | Deprecations | API changes | Features | . Upgrade notes for Kubernetes 1.22 . If you are planning to upgrade to Kubernetes 1.22, please have a look at the What’s new section of the official Kubernetes v1.22 release notes and make sure you familiarize yourself with the upcoming changes. For an overview of the changes, please refer to the Changes by Kind section of the Changelog. | Urgent Upgrade Notes | Deprecations | API changes | Features | . ",
    "url": "/gks/about/changelog-v2.18/#kubernetes-related-changes",
    
    "relUrl": "/gks/about/changelog-v2.18/#kubernetes-related-changes"
  },"50": {
    "doc": "GKS Changelog v2.18",
    "title": "GKS Changelog v2.18",
    "content": " ",
    "url": "/gks/about/changelog-v2.18/",
    
    "relUrl": "/gks/about/changelog-v2.18/"
  },"51": {
    "doc": "GKS Changelog v2.17",
    "title": "Supported Kubernetes versions",
    "content": "With the current release, we support the following Kubernetes versions: . | 1.18.18 | 1.19.10 | 1.19.11 | 1.20.7 | 1.21.2 | . ",
    "url": "/gks/about/changelog-v2.17/#supported-kubernetes-versions",
    
    "relUrl": "/gks/about/changelog-v2.17/#supported-kubernetes-versions"
  },"52": {
    "doc": "GKS Changelog v2.17",
    "title": "End of Life announcements",
    "content": "We will end the support of Kubernetes version 1.18 on the 15th September 2021. Please make sure to upgrade all existing clusters running 1.18 at least to 1.19 before that date. ",
    "url": "/gks/about/changelog-v2.17/#end-of-life-announcements",
    
    "relUrl": "/gks/about/changelog-v2.17/#end-of-life-announcements"
  },"53": {
    "doc": "GKS Changelog v2.17",
    "title": "New features",
    "content": ". | Added support for Kubernetes 1.20 | Added support for Kubernetes 1.21 | . ",
    "url": "/gks/about/changelog-v2.17/#new-features",
    
    "relUrl": "/gks/about/changelog-v2.17/#new-features"
  },"54": {
    "doc": "GKS Changelog v2.17",
    "title": "Bugfixes",
    "content": ". | the recurring cluster-event “failed to ensure CronJob kube-system/etcd-backup-$CLUSTERID: failed waiting for the cache to contain our latest changes: timed out waiting for the condition” has been fixed. | . ",
    "url": "/gks/about/changelog-v2.17/#bugfixes",
    
    "relUrl": "/gks/about/changelog-v2.17/#bugfixes"
  },"55": {
    "doc": "GKS Changelog v2.17",
    "title": "Kubernetes related changes",
    "content": "Upgrade notes for Kubernetes 1.20 . If you are planning to upgrade to Kubernetes 1.20, please have a look at the Upgrade Notes section of the official Kubernetes v1.20 release notes and make sure you familiarise yourself with the upcoming changes. For an overview of the changes, please refer to the Changes by Kind section of the Changelog. | Urgent Upgrade Notes | Deprecations | API changes | Features | . Upgrade notes for Kubernetes 1.21 . If you are planning to upgrade to Kubernetes 1.21, please have a look at the What’s new section of the official Kubernetes v1.21 release notes and make sure you familiarise yourself with the upcoming changes. For an overview of the changes, please refer to the Changes by Kind section of the Changelog. | Urgent Upgrade Notes | Deprecations | API changes | Features | . ",
    "url": "/gks/about/changelog-v2.17/#kubernetes-related-changes",
    
    "relUrl": "/gks/about/changelog-v2.17/#kubernetes-related-changes"
  },"56": {
    "doc": "GKS Changelog v2.17",
    "title": "GKS Changelog v2.17",
    "content": " ",
    "url": "/gks/about/changelog-v2.17/",
    
    "relUrl": "/gks/about/changelog-v2.17/"
  },"57": {
    "doc": "About GKS",
    "title": "About GKS",
    "content": " ",
    "url": "/gks/about/",
    
    "relUrl": "/gks/about/"
  },"58": {
    "doc": "About GKS",
    "title": "What Is the GKS Platform?",
    "content": "The GKS platform offers Managed Kubernetes Clusters. As a customer, you can create, update, and delete managed Kubernetes clusters with just a simple click in our web interface. What Is a Managed Kubernetes Cluster? . Kubernetes clusters in general provide a reliable and scalable environment to run containerized applications. With a managed Kubernetes cluster on our GKS platform, we: . | Take care of the underlying infrastructure and the Kubernetes installation | Ensure scalability and provide you the option to configure the amount and size of your worker nodes for your needs | Completely manage the control plane for you | . While we ensure that the cluster itself is up and running, you have full access to the cluster with a kubeconfig-file. You have control over your clusters all time. You can: . | Easily create and manage an unlimited amount of Kubernetes clusters with our web interface. | Even upgrade the Kubernetes version of existing clusters with just a click. | Instantly delete clusters that are not needed anymore. | Have full SSH-root access to your worker nodes, if needed (for example, for debugging your own applications). | . In case anything goes wrong, you can reach out to our 24/7 support or, as an optional service, schedule a session with our Professional Services team to deep-dive and discuss your technical questions and architecture. How Can I Use a Managed Kubernetes Cluster? . Of course you can use the GKS clusters like any Kubernetes cluster - just with the GKS platform, you can focus on your application instead of having to care about the underlying infrastructure. For example, you can install a pre-packaged containerized application with helm, such as: . | A full WordPress installation | A Drupal CMS | Your own private Nextcloud service | . Besides pre-packaged and ready-to-use applications, Kubernetes is also the natural environment to develop, build, and run your own, state-of-the-art “cloud-native” applications. For a development-environment, you can install the following apps to a Kubernetes cluster: . | gitlab to manage your sourcecode | Jenkins to automate your workflows | Artifactory to store your build results in | . Then you may create dev, staging, and production Kubernetes clusters with the GKS platform and use easy-to-install, existing components to support your application architecture, such as a: . | Complete Kafka setup | PostgreSQL-, MySQL- or MariaDB-database | Full-featured Prometheus-Monitoring-Stack incl. Prometheus, Alertmanager, and Grafana | . These are just examples. If you want to deep dive and get consulting on how to best build your app on Kubernetes, schedule a session with our Professional Services team. ",
    "url": "/gks/about/#what-is-the-gks-platform",
    
    "relUrl": "/gks/about/#what-is-the-gks-platform"
  },"59": {
    "doc": "About GKS",
    "title": "How Does the GKS Platform Work?",
    "content": "The GKS platform itself is based on Kubernetes as well. Internally, GKS runs the control plane of a customer cluster in a dedicated Kubernetes namespace with all required workloads running as deployments inside this namespace: . | Kubernetes API server | Etcd database | Controller-manager | Scheduler | Machine-controller | Other workloads | . Running the control plane in Kubernetes has many advantages for management, such as automated scheduling as well as scaling and self-healing capabilities. The worker nodes of a customer cluster are dedicated VMs in the Openstack tenant of a customer. The machine-controller automatically creates VMs there and adds them as worker nodes to the cluster. It scales the number of nodes up or down as required and performs any changes to a machine deployment as a rolling upgrade to minimize downtime. ",
    "url": "/gks/about/#how-does-the-gks-platform-work",
    
    "relUrl": "/gks/about/#how-does-the-gks-platform-work"
  },"60": {
    "doc": "About GKS",
    "title": "Certifications",
    "content": ". GKS is a product to efficiently set up and operate Kubernetes clusters on GEC Cloud. Customers have the option to use a powerful web interface to deploy and manage completely functional clusters that also pass the conformance checks and requirements by the Cloud Native Computing Foundation. Conformance results of our platform can be validated and checked at CNCF Kubernetes Conformance . ",
    "url": "/gks/about/#certifications",
    
    "relUrl": "/gks/about/#certifications"
  },"61": {
    "doc": "Supported Kubernetes Versions",
    "title": "Supported Kubernetes Versions",
    "content": "Current supported Kubernetes versions in GKS. | Version | GKS Deprecation | GKS End-of-Life | . | v1.24 |   |   | . | v1.23 | 31th Jan 2023 | 18th Apr 2023 | . | v1.22 | 19th Jul 2022 | 18th Oct 2022 | . | v1.21 | 4th Mar 2022 | 28th Jun 2022 | . | v1.20 | 22nd Nov 2021 | 1st Mar 2022 | . | v1.19 | 22nd Jul 2021 | 3rd Nov 2021 | . | v1.18 | 14th Jun 2021 | 15th Sep 2021 | . | v1.17 | 20th Apr 2021 | 31st May 2021 | . | v1.16 | 10th Dec 2020 | 10th Dec 2020 | . | v1.15 | 10th Dec 2020 | 10th Dec 2020 | . ",
    "url": "/gks/about/kubernetesversions/",
    
    "relUrl": "/gks/about/kubernetesversions/"
  },"62": {
    "doc": "Supported Kubernetes Versions",
    "title": "Force Upgrade Policy",
    "content": "If a customer cluster is not updated by its owner until the announced GKS End-of-Life date, it will be automatically upgraded to the next supported version. You can read more about the GKS deprecation and force upgrade policy here. ",
    "url": "/gks/about/kubernetesversions/#force-upgrade-policy",
    
    "relUrl": "/gks/about/kubernetesversions/#force-upgrade-policy"
  },"63": {
    "doc": "Getting Started",
    "title": "Getting Started",
    "content": "This documentation describes how to create your first GKS project with a first Kubernetes cluster, how to connect to that cluster, and how to clean up all resources afterwards. ",
    "url": "/gks/gettingstarted/",
    
    "relUrl": "/gks/gettingstarted/"
  },"64": {
    "doc": "Getting Started",
    "title": "Creating Your First Project",
    "content": "After logging into GKS for the very first time, the following window appears. Since a project is required to create your first Kubernetes cluster, you need to click on Add Project. A window opens, where you can name the project. In the example, we use Team Kubernetes. To finish, click on Save Project. (../gett) . Now GKS creates your project and adds it to the overview. With a click on the entry Team Kubernetes you enter the project. This opens a window showing the project. You see a list of all existing clusters and their users, as well as some other controls. At the moment, this list is empty until you create your first managed Kubernetes cluster. ",
    "url": "/gks/gettingstarted/#creating-your-first-project",
    
    "relUrl": "/gks/gettingstarted/#creating-your-first-project"
  },"65": {
    "doc": "Getting Started",
    "title": "Create Your First Cluster",
    "content": "To create the cluster, click on Create Cluster in the upper right corner. The first page of the cluster creation procedure opens. Choose the provider openstack. Then choose one of the three datacenters. In this example, we pick IX2. In the next step, you have to configure the cluster details. In our example, we call our cluster first-system and select the desired Kubernetes version. For occasional SSH access to worker nodes, you can optionally deploy an SSH Key. To add an SSH Key, click on Add SSH key. After that add the Public SSH Key and give it a memorable name. To allow GKS to request the required resources from OpenStack, add your OpenStack credentials. After that, the content of Project is refreshed automatically, and you can choose the OpenStack project where you want to run the cluster. By adding the credentials and selecting the OpenStack project, you could proceed to the next step. If you do so, a new and dedicated network, subnet, and security group will be automatically created for the cluster. It is also possible to use an existing network to create the cluster. For this, you have to select the network and the subnet from the dropdown menu, and attach them to a router. You can create a router from the Optimist dashboard or from the OpenStack command line. For details on how to create and attach the router, refer to our OpenStack documentation. In the next step, you define the number and the kind of virtual machines that will be initially available as worker nodes in the cluster. First, this so-called Machine Deployment needs a name. For your test cluster you use the random name generator. Next, specify the Replicas (number of worker nodes in your Kubernetes cluster) and the Flavor (machine type), which defines the amount of CPU and RAM for each worker node. Choose Flatcar as the operating system for the worker nodes. To finish, click on Next. After you verified all settings, click on Create Cluster. Now the cluster is being created. To access the information, return to the cluster view of the project and click your cluster’s name. This opens a page with all cluster details: . ",
    "url": "/gks/gettingstarted/#create-your-first-cluster",
    
    "relUrl": "/gks/gettingstarted/#create-your-first-cluster"
  },"66": {
    "doc": "Getting Started",
    "title": "Accessing Your First cluster",
    "content": "To access the cluster, you need to click on the Get Kubeconfig button in the top right corner. This way you download a file which is called kubeconfig in Kubernetes jargon. This file contains all end points, certificates and other information about the cluster. The kubectl command uses this file to connect to the cluster. To use kubeconfig, you need to register it on the console. There are two ways to do this: . | kubectl by default tries to use the file .kube/config in your home directory | You can temporarily use the kubeconfig by exporting it to the environment variable KUBECONFIG | . To keep things straightforward and to avoid changing standards on our system, choose the second method in the example. For this you need to open a terminal. In the screenshots we use iTerm2 on macOS, but the examples work the same way when using bash on Linux or Windows. First, you need to find the downloaded kubeconfig file. Browsers like Chrome or Firefox usually store it in the Downloads folder. The name is composed of two parts: . | kubeconfig-admin- | The cluster id. | . To register the kubeconfig, use the following command: . cd Downloads export KUBECONFIG=$(pwd)/kubeconfig-admin-CLUSTERID . Now you can interact with the cluster. The simplest command is: “show all the nodes that comprise my cluster”: . kubectl get nodes NAME STATUS ROLES AGE VERSION musing-kalam-XXXXXXXXX-ks4xz Ready &lt;none&gt; 10m v1.21.5 musing-kalam-XXXXXXXXX-txc4w Ready &lt;none&gt; 10m v1.21.5 musing-kalam-XXXXXXXXX-vc4g2 Ready &lt;none&gt; 10m v1.21.5 . ",
    "url": "/gks/gettingstarted/#accessing-your-first-cluster",
    
    "relUrl": "/gks/gettingstarted/#accessing-your-first-cluster"
  },"67": {
    "doc": "Getting Started",
    "title": "Cleanup",
    "content": "To clean up the cluster you created, click Delete in the GKS dashboard: . This opens a window where you need to enter the cluster name to avoid sudden and unwanted deletions: . Since we also want to free up the resources, leave both check boxes marked. That way, volumes and load balancers provided by OpenStack will be removed as well. ",
    "url": "/gks/gettingstarted/#cleanup",
    
    "relUrl": "/gks/gettingstarted/#cleanup"
  },"68": {
    "doc": "Managing GKS Projects",
    "title": "Managing GKS Projects",
    "content": "A GKS project is an abstraction layer to manage a set of clusters. All clusters in an GKS project: . | Share the same users (“members” of that project), who can be either administrators or have read-only rights. A project member can always log in to the GKS dashboard and will be able to download a kubeconfig for accessing all clusters in that project directly. | Share the same GKS service accounts, which can access the GKS API with a REST API and API token. | Can have SSH Keys assigned to them, which are organized via the project. | . Projects are only required for management and have no impact on how you can use the clusters within. You can have multiple projects. ",
    "url": "/gks/managingprojects/",
    
    "relUrl": "/gks/managingprojects/"
  },"69": {
    "doc": "Managing GKS Projects",
    "title": "Learn More",
    "content": ". | How to create a GKS project | How to manage users in a GKS project | How to manage service accounts in a GKS project | . ",
    "url": "/gks/managingprojects/#learn-more",
    
    "relUrl": "/gks/managingprojects/#learn-more"
  },"70": {
    "doc": "Creating a Project",
    "title": "Creating a Project",
    "content": "After logging into GKS, you see the following window, where you need to click on Add Project. This opens a window, where you can give the project a name. In the example, Team Kubernetes is used. To finish, click on Save Project. Now GKS creates your project and adds it to the Overview. With a click on the entry Team Kubernetes you enter the project and can create the cluster. This opens a view showing you the project. You find a list of all existing clusters and their users, as well as some other controls. This list is empty until you create your first managed Kubernetes cluster. Clicking on the sidebar opens the navigation in the Project view, which allows you to explore the other areas. ",
    "url": "/gks/managingprojects/creatingaproject/",
    
    "relUrl": "/gks/managingprojects/creatingaproject/"
  },"71": {
    "doc": "Managing Users in a Project",
    "title": "Managing Users in a Project",
    "content": " ",
    "url": "/gks/managingprojects/projectusermanagement/",
    
    "relUrl": "/gks/managingprojects/projectusermanagement/"
  },"72": {
    "doc": "Managing Users in a Project",
    "title": "Adding Users",
    "content": "You can add a user to an existing GKS project with a couple of clicks. To achieve this, you need: . | The project name | The user email | . The user needs to log in once, before it is usable. First, select the correct project. Then click on Members in the left sidebar. Next click Add Member on the top right. Finally, enter the user email and the desired role (Owner, Editor, or Viewer). ",
    "url": "/gks/managingprojects/projectusermanagement/#adding-users",
    
    "relUrl": "/gks/managingprojects/projectusermanagement/#adding-users"
  },"73": {
    "doc": "Managing Users in a Project",
    "title": "Removing Users",
    "content": "To remove a user from a project go back to the member section of the project. First, go to the project. Then click on Members in the left sidebar. Use the Deletion icon to remove the user. ",
    "url": "/gks/managingprojects/projectusermanagement/#removing-users",
    
    "relUrl": "/gks/managingprojects/projectusermanagement/#removing-users"
  },"74": {
    "doc": "Managing Users in a Project",
    "title": "Offboarding Users",
    "content": "You should remove the user from all projects before requesting to offboard the user. Note that you cannot see projects/clusters where only the user is member of the project. In case those projects do not contain any active cluster, they will be automatically deleted by the GKS support during the process of the offboarding requests. In case there are active clusters left, those will not be automatically deleted and support will ask how to proceed with those. ",
    "url": "/gks/managingprojects/projectusermanagement/#offboarding-users",
    
    "relUrl": "/gks/managingprojects/projectusermanagement/#offboarding-users"
  },"75": {
    "doc": "Managing Service Account Tokens",
    "title": "Managing Service Account Tokens",
    "content": " ",
    "url": "/gks/managingprojects/projectserviceaccounts/",
    
    "relUrl": "/gks/managingprojects/projectserviceaccounts/"
  },"76": {
    "doc": "Managing Service Account Tokens",
    "title": "Service Accounts",
    "content": "Service accounts allow using a long-lived token that you can use to authenticate with the GKS API. A service account is a special type of user account that belongs to the GKS project, instead of an individual end user. Your project resources assume the identity of the service account to call GKS APIs, so that the users are not directly involved. A service account can have one or more JWT tokens which are used to authenticate to the GKS API. The JWT token by default expires after 3 years. ",
    "url": "/gks/managingprojects/projectserviceaccounts/#service-accounts",
    
    "relUrl": "/gks/managingprojects/projectserviceaccounts/#service-accounts"
  },"77": {
    "doc": "Managing Service Account Tokens",
    "title": "Core Concept",
    "content": "Service accounts are considered a project’s resource. Only the owner of the project can create a service account. There is no need to create a new group for an SA; we want to assign a service account to one of the already defined groups: Project Manager, Editor, or Viewer. A service account is linked to the project automatically by a UserProjectBinding which specifies a binding between a service account and a project. A service account will be automatically deleted after project removal. ",
    "url": "/gks/managingprojects/projectserviceaccounts/#core-concept",
    
    "relUrl": "/gks/managingprojects/projectserviceaccounts/#core-concept"
  },"78": {
    "doc": "Managing Service Account Tokens",
    "title": "Creating a Service Account with Token",
    "content": ". | Select the project. | Go to the Service Accounts page. | Use the Create Service Account button. | Enter a name for the service account and select the group (either Project Manager, Editor or Viewer). | Click Add Service Account. | . Now the service account has been created. If you want to associate a token to it, proceed as follows: . | Select the service account you just created. | Click on + Add Token. | Enter a name for the token and click + Add Token. | Now the generated token is shown. Download it with the Download Tokenbutton or copy it. | . Important note: Make sure to save this token at a safe place on your own device. It cannot be displayed again after closing the dashboard window. ",
    "url": "/gks/managingprojects/projectserviceaccounts/#creating-a-service-account-with-token",
    
    "relUrl": "/gks/managingprojects/projectserviceaccounts/#creating-a-service-account-with-token"
  },"79": {
    "doc": "Managing Service Account Tokens",
    "title": "Accessing the API with the Service Account Token",
    "content": "A client that wants to authenticate itself with a server, can do so by including an Authorization request header field with the service account token: . Authorization: Bearer aaa.bbb.ccc . Example: To get a list of your clusters, you can use the following API call: . curl -X GET \"https://gks.gec.io/api/v1/projects?displayAll=true\" -H \"accept: application/json\" -H \"authorization: Bearer eyJhbXxXXxXxX...\" | jq . The result will be similar to: . [ { \"id\": \"q3jXY4ZYx8\", \"name\": \"My-project\", \"creationTimestamp\": \"2020-12-08T21:55:47Z\", \"status\": \"Active\", \"owners\": [ { \"name\": \"your.email@your-company.de\", \"creationTimestamp\": \"0001-01-01T00:00:00Z\", \"email\": \"your.email@your-company.de\" } ], \"clustersNumber\": 1 } ] . ",
    "url": "/gks/managingprojects/projectserviceaccounts/#accessing-the-api-with-the-service-account-token",
    
    "relUrl": "/gks/managingprojects/projectserviceaccounts/#accessing-the-api-with-the-service-account-token"
  },"80": {
    "doc": "Managing Service Account Tokens",
    "title": "Keeping Track of Service Accounts and Tokens",
    "content": "It is possible to create multiple service accounts for the given project. The service account name must be unique for the project scope. The service account can have multiple tokens with unique names. The display name of the service account and token is a good way to capture additional information, such as the purpose of the service account or token. ",
    "url": "/gks/managingprojects/projectserviceaccounts/#keeping-track-of-service-accounts-and-tokens",
    
    "relUrl": "/gks/managingprojects/projectserviceaccounts/#keeping-track-of-service-accounts-and-tokens"
  },"81": {
    "doc": "Managing Service Account Tokens",
    "title": "Managing Service Accounts and Tokens",
    "content": "It is possible to delete a service account and then create a new service account with the same name. You can do the same with the service account token. You can change the service account and token names when once created. The service account token is only visible to the user during creation. The user can also regenerate a token, but the previous one will be revoked. ",
    "url": "/gks/managingprojects/projectserviceaccounts/#managing-service-accounts-and-tokens",
    
    "relUrl": "/gks/managingprojects/projectserviceaccounts/#managing-service-accounts-and-tokens"
  },"82": {
    "doc": "Cluster Lifecycle",
    "title": "Cluster Lifecycle",
    "content": "The management of an GKS-Kubernetes-Cluster is handled by the platform itself, but some tasks still require customer interaction. This is not only true for the cluster creation and deletion tasks, but also true for cluster updates which could require a (rolling) restart of worker nodes, such as Kubernetes updates. ",
    "url": "/gks/clusterlifecycle/",
    
    "relUrl": "/gks/clusterlifecycle/"
  },"83": {
    "doc": "Cluster Lifecycle",
    "title": "Learn More",
    "content": ". | Creating a Cluster | Using Cluster Templates | Updating a Cluster/Kubernetes Updates | Choosing a CNI | Updating a Cluster/CNI Updates | Control Plane Connector | Updating a Cluster/Changing Openstack Credentials | Deleting a Cluster | Deprecation Policy | Migrating the Container Runtime Engine | . ",
    "url": "/gks/clusterlifecycle/#learn-more",
    
    "relUrl": "/gks/clusterlifecycle/#learn-more"
  },"84": {
    "doc": "Creating a Cluster",
    "title": "Creating a Cluster",
    "content": "You can create a cluster in GKS with a couple of clicks. Before you can do that you need a project. If you do not have a project yet, create a project first. To create the cluster, click on Create Cluster in the top right corner. The first page of the cluster creation procedure opens. Choose the provider openstack. Then choose one of the three datacenters. In this example, we pick IX2. In the next step, you have to configure the cluster details. In the example, we call our cluster first-system and select the desired Kubernetes version. For occasional SSH access to worker nodes, you can optionally deploy an SSH Key. To add an SSH Key, click on Add SSH key. After that add the Public SSH Key and give it a memorable name. To allow GKS to request the required resources from OpenStack, add your OpenStack credentials. After that the content of Project is refreshed automatically, and you can choose the OpenStack project where you want to run the cluster. By adding the credentials and selecting the OpenStack project, you could proceed to the next step. If you do so, a new and dedicated network, subnet, and security group will be automatically created for the cluster. It is also possible to use an existing network to create the cluster. For this, you have to select the network and the subnet from the dropdown menu, and attach them to a router. You can create a router from the Optimist dashboard, or from the OpenStack command line. For details on how to create and attach the router, refer to our OpenStack documentation. In the next step, you define the number and the kind of virtual machines that will be initially available as worker nodes in the cluster. First, this so-called Machine Deployment needs a name. For your test cluster you use the random name generator. Next, specify the Replicas (number of worker nodes in your Kubernetes cluster) and the Flavor (machine type), which defines the amount of CPU and RAM for each worker node. Choose Flatcar as the operating system for the worker nodes. To finish, click on Next. After you verified all settings, click on Create Cluster. Now the cluster is being created. To access the information, return to the cluster view of the project and click your cluster’s name. This opens a page with all cluster details. ",
    "url": "/gks/clusterlifecycle/creatingacluster/",
    
    "relUrl": "/gks/clusterlifecycle/creatingacluster/"
  },"85": {
    "doc": "Creating a Cluster",
    "title": "Summary",
    "content": "Congratulations! You learned and achieved the following: . | What is a GKS cluster | How to create an GKS cluster | . The following sections describe cluster usage examples. ",
    "url": "/gks/clusterlifecycle/creatingacluster/#summary",
    
    "relUrl": "/gks/clusterlifecycle/creatingacluster/#summary"
  },"86": {
    "doc": "Cluster Templates",
    "title": "Cluster Templates",
    "content": " ",
    "url": "/gks/clusterlifecycle/clustertemplates/",
    
    "relUrl": "/gks/clusterlifecycle/clustertemplates/"
  },"87": {
    "doc": "Cluster Templates",
    "title": "What Are Cluster Templates?",
    "content": "Cluster templates are templates that enable a fast and uniform creation of Kubernetes clusters. With cluster templates, you can create clusters with a few clicks without having to re-enter settings such as credentials, network settings, and availability zones each time. ",
    "url": "/gks/clusterlifecycle/clustertemplates/#what-are-cluster-templates",
    
    "relUrl": "/gks/clusterlifecycle/clustertemplates/#what-are-cluster-templates"
  },"88": {
    "doc": "Cluster Templates",
    "title": "Creating Cluster Templates",
    "content": "To create a Cluster Template, select the Cluster Templates menu item in the sidebar and click the Create Cluster button. The cluster creation process known from section Creating a cluster opens. Enter all required data for the cluster creation. In the last step “Summary”, do not click on Create Cluster but on Save Cluster Template. Now the dialog Save Cluster Template opens. Here you can define the name and storage scope. Templates can be saved in 2 different scopes: . | On project level: All users of the project can use the template. | On user level: The template can be used in all projects where the user has write access. Other users cannot use the template. | . Confirm the selection with the Save Cluster Template button. The cluster template has now been created. ",
    "url": "/gks/clusterlifecycle/clustertemplates/#creating-cluster-templates",
    
    "relUrl": "/gks/clusterlifecycle/clustertemplates/#creating-cluster-templates"
  },"89": {
    "doc": "Cluster Templates",
    "title": "Creating Clusters from Templates",
    "content": "New clusters can now be easily created from the template you just created. Select the menu item Cluster Templates in the sidebar. Then, select the desired template and click the button Create Cluster from Template. You are asked how many clusters you want to create from this template. Enter a number and confirm the selection with Create Clusters. The cluster or clusters are then created. Clusters created from cluster templates are recognizable by the template-instance-id label. Note: Another way to create clusters from templates is available in the Cluster menu with the button Create Clusters from Template. The function is not different from the one just shown and is just a shortcut in the interface. ",
    "url": "/gks/clusterlifecycle/clustertemplates/#creating-clusters-from-templates",
    
    "relUrl": "/gks/clusterlifecycle/clustertemplates/#creating-clusters-from-templates"
  },"90": {
    "doc": "Cluster Templates",
    "title": "Deleting Cluster Templates",
    "content": "To delete cluster templates, select the menu item Cluster Templates in the sidebar and the corresponding template. To delete, click the Delete Cluster Template button. ",
    "url": "/gks/clusterlifecycle/clustertemplates/#deleting-cluster-templates",
    
    "relUrl": "/gks/clusterlifecycle/clustertemplates/#deleting-cluster-templates"
  },"91": {
    "doc": "Kubernetes Updates",
    "title": "Kubernetes Updates",
    "content": "Cluster security is paramount, and new features come with each release. In order to stay safe and up-to-date, Kubernetes updates have to be installed on a regular basis. In especially critical cases, we will automatically update the Cluster API to the latest minor version in order to keep our own infrastructure up-to-date. In this case the below section The Cluster can be skipped. However, nodes must still be updated manually by you. Before you upgrade a cluster, refer to the target version’s changelog and familiarize yourself with the upcoming changes. One tool that can help to prepare the update is kubepug. It checks all deployed resources against the new Kubernetes version and warns about removals and deprecations. ",
    "url": "/gks/clusterlifecycle/upgradingacluster/",
    
    "relUrl": "/gks/clusterlifecycle/upgradingacluster/"
  },"92": {
    "doc": "Kubernetes Updates",
    "title": "The Cluster",
    "content": "In Kubernetes, the infrastructure is divided into master (= Kubernetes control plane) and (worker) nodes. The master is managed by GKS itself. Since several versions for the master are offered, you can choose the version in the GKS web interface. An update of the master can be done with a few mouse clicks. First, select the cluster you’d like to update. Then click on the field Control Plane, and choose a new version for the master. We recommend selecting Upgrade Machine Deployments, as this will upgrade the worker nodes as well. Now GKS automatically updates the master and optionally the worker nodes. ",
    "url": "/gks/clusterlifecycle/upgradingacluster/#the-cluster",
    
    "relUrl": "/gks/clusterlifecycle/upgradingacluster/#the-cluster"
  },"93": {
    "doc": "Kubernetes Updates",
    "title": "The Nodes",
    "content": "If the master has been updated without upgrading the Machine Deployments, or if a scheduled maintenance of the GKS platform has led to an implicit upgrade of the master (normally this just updates a patchlevel), you must still update the nodes. The GKS web interface helps you here as well. It’s worth noting that this update process deletes the old nodes and replaces them with new ones. This also means that all pods will be restarted. First, click on Machine Deployments. Next, click on the pencil icon to open the update view. Now, under kubelet Version select the version, for example 1.23.6, which matches the cluster’s master version. Confirm the update by clicking Save Changes. Now GKS automatically updates the node group to the new version, amd Kubernetes takes care of deploying your applications to the new nodes. ",
    "url": "/gks/clusterlifecycle/upgradingacluster/#the-nodes",
    
    "relUrl": "/gks/clusterlifecycle/upgradingacluster/#the-nodes"
  },"94": {
    "doc": "Kubernetes Updates",
    "title": "Two-Node Cluster",
    "content": "Watch out for clusters with two nodes or less. GKS uses a rolling update strategy. This means one node after another is swapped. In a cluster with two or less nodes this means that the first updated node must be fully scheduled before the second one is ready. A solution to this is a simple bash script, which per-namespace triggers the regeneration of all pods. https://github.com/truongnh1992/playing-with-istio/blob/master/upgrade-sidecar.sh . We use this after the cluster has been completely updated in a terminal with kubectl configured. To get kubectl working with your cluster, look at chapter Connecting to a Cluster. curl -o upgrade-node.sh https://raw.githubusercontent.com/truongnh1992/playing-with-istio/master/upgrade-sidecar.sh chmod +x upgrade-node.sh echo -e \"#\\!/bin/bash\\n$(cat upgrade-node.sh)\" &gt; upgrade-node.sh . Now you must use this script on all of your namespaces. kubectl get namespace NAME STATUS AGE default Active 36m kube-node-lease Active 36m kube-public Active 36m kube-system Active 36m # So for default namespace we would run: ./upgrade-node.sh default Refreshing pods in all Deployments . Now all pods are cleanly distributed across your nodes. ",
    "url": "/gks/clusterlifecycle/upgradingacluster/#two-node-cluster",
    
    "relUrl": "/gks/clusterlifecycle/upgradingacluster/#two-node-cluster"
  },"95": {
    "doc": "CNI Choices",
    "title": "CNI Choices",
    "content": "Here you find an overview of what CNIs are, why they are used and what features they provide. ",
    "url": "/gks/clusterlifecycle/cnichoices/",
    
    "relUrl": "/gks/clusterlifecycle/cnichoices/"
  },"96": {
    "doc": "CNI Choices",
    "title": "What Is a CNI and Why Do I Need It?",
    "content": "CNI is the abbreviation for ContainerNetworkInterface. Simply put, it is a way for Kubernetes to describe what network functionality is needed to implement pod-to-pod networking inside a Kubernetes cluster. There are many different (network) environments where Kubernetes can run. To focus more on the Kubernetes product the Kubernetes developers decided to just specify what is required in terms of network functionality and let others do the implementation. This resulted in many different implementations for Kubernetes networking (CNIs) with very different features for very different environments. Our Kubernetes platform only supported one CNI implementation for the longest time and therefore did not provide any means to change that. Recently we added a second one so now you can choose which CNI best suits your use-case. ",
    "url": "/gks/clusterlifecycle/cnichoices/#what-is-a-cni-and-why-do-i-need-it",
    
    "relUrl": "/gks/clusterlifecycle/cnichoices/#what-is-a-cni-and-why-do-i-need-it"
  },"97": {
    "doc": "CNI Choices",
    "title": "Canal",
    "content": "The CNI solution we used in the past is called canal, which is a combination of the two CNIs flannel and calico. In these setups flannel is used to implement the pod-to-pod communication while calico is used to enable network policies. This CNI has been around a long time and is thus mature and battle-tested. Canal supports both iptables as well as ipvs as proxy modes. If you are unsure what to choose, this CNI is the conservative choice. ",
    "url": "/gks/clusterlifecycle/cnichoices/#canal",
    
    "relUrl": "/gks/clusterlifecycle/cnichoices/#canal"
  },"98": {
    "doc": "CNI Choices",
    "title": "Cilium",
    "content": "Cilium is a fairly new CNI which leverages eBPF instead of traditional ways to manage traffic between Kubernetes pods and nodes. It has advanced observability features like a dedicated looking-glass addon called hubble and improved health checks. Cilium also supports network policies to allow tighter control of the traffic flow into or out of your cluster as well as inter-cluster traffic. If you want to take full advantage of the Cilium features, you need to set the proxy mode to eBPF after choosing the CNI (this is only possible if Konnectivity is chosen as well). Another requirement for Cilium (and especially the eBFP proxy mode) is that the OS image needs to run a fairly recent kernel to be able to support all functionality of the CNI which is the case with our flatcar images. Cilium is under heavy development so new features as well as bug-fixes are released regularly. If you are interested in a deeper understanding of the network flows inside your Kubernetes cluster, then this CNI is for you. ",
    "url": "/gks/clusterlifecycle/cnichoices/#cilium",
    
    "relUrl": "/gks/clusterlifecycle/cnichoices/#cilium"
  },"99": {
    "doc": "CNI Choices",
    "title": "Installing a CNI",
    "content": "In the cluster creation process, the second step enables you to choose between the two CNIs described above. After choosing your CNI you may need to choose a proxy mode as well, depending on the CNI chosen in the previous step. The choice of the eBPF proxy implicitly requires the use of Konnectivity as your control-plane connector as well, which is the default in all newly created Kubernetes clusters. More information on the control-plane connector and Konnectivity is available here. ",
    "url": "/gks/clusterlifecycle/cnichoices/#installing-a-cni",
    
    "relUrl": "/gks/clusterlifecycle/cnichoices/#installing-a-cni"
  },"100": {
    "doc": "CNI Choices",
    "title": "Installing a Hubble Addon",
    "content": "If Cilium is chosen as a CNI, you can install the graphical visualization addon Hubble. This can be done after the cluster creation has finished successfully with the Addons tab at the bottom of the cluster overview. ",
    "url": "/gks/clusterlifecycle/cnichoices/#installing-a-hubble-addon",
    
    "relUrl": "/gks/clusterlifecycle/cnichoices/#installing-a-hubble-addon"
  },"101": {
    "doc": "CNI Choices",
    "title": "Final Note",
    "content": "Choosing an CNI can only be done once in the creation process of the cluster. While it is technically possible to switch or change CNIs in a running cluster, we do not support this in our platform. So choose your CNI carefully as this choice cannot be changed without deleting and re-creating the cluster. ",
    "url": "/gks/clusterlifecycle/cnichoices/#final-note",
    
    "relUrl": "/gks/clusterlifecycle/cnichoices/#final-note"
  },"102": {
    "doc": "CNI Choices",
    "title": "Learn More",
    "content": ". | Flannel GitHub page | Canal GitHub page | Canal installation docs | Cilium Documentation | . ",
    "url": "/gks/clusterlifecycle/cnichoices/#learn-more",
    
    "relUrl": "/gks/clusterlifecycle/cnichoices/#learn-more"
  },"103": {
    "doc": "CNI Updates",
    "title": "CNI Updates",
    "content": "On the detail page of the cluster you can see if an update of the CNI is available. If you see a green arrow on the CNI plugin box, you can update. Click the plugin box to start. It will show you the available versions. To start the upgrade, click the Change CNI Version button. The update process starts in the background. While updating, the worker nodes will experience some drop packages while each node is updated. The process updates one worker at a time, so deployments with more the 1 replica should not experience an outage. The update normally completes without issues and the cluster is working afterwards. In cases where there are network problems at the end of the update (all pods in the canal daemonset are recreated), possible solutions would be: . | Restart the pods one more time after the update. You can do this with the kubectl client: kubectl rollout restart daemonset --namespace kube-system canal | A rolling recreation of all worker nodes. You can do this in the UI. | . CNI update only supports updates to the next minor version. If you see more than one version in the dropdown, you need to update one by one. ",
    "url": "/gks/clusterlifecycle/upgradingcni/",
    
    "relUrl": "/gks/clusterlifecycle/upgradingcni/"
  },"104": {
    "doc": "Control Plane Connector",
    "title": "Control Plane Connector",
    "content": "Here you find an overview of how the worker nodes communicate with the control plane. ",
    "url": "/gks/clusterlifecycle/controlplaneconnector/",
    
    "relUrl": "/gks/clusterlifecycle/controlplaneconnector/"
  },"105": {
    "doc": "Control Plane Connector",
    "title": "Cluster Setup",
    "content": "The GKS platform provides a managed Kubernetes services to end users. For more reliability the control plane of these Kubernetes clusters is isolated from the workers. The end user has complete control of the worker nodes (right down to the operating system of the VM the worker runs on) in the end user’s own Openstack tenant. The control plane however is managed by the service provider and runs on isolated clusters apart from the worker nodes in different Openstack tenants. Still the worker nodes must communicate with the control plane on a regular bases. Communication Initiated from the Worker Nodes to the Control Plane . This direction is easy as the communication with the control plane is done by the Kubernetes API server exposing it’s service via an publicly accessible IPv4 address. That way components running on the worker nodes create a TCP connection from inside their openstack tenants private subnet, exiting the cluster via the router’s egress to the internet and entering the control planes’ dedicated openstack tenants subnet where the respective Kubernetes API server is exposed publicy and then forwarded inside it’s private openstack subnet again. But sometimes the Kubernetes API server needs to initiate the connection to the worker nodes (mostly their kubelet’s) either when doing “kubectl port-forward”, or “kubectl exec”, or “kubectl logs”. The worker nodes do not expose an endpoint to the internet for easy reachability (which is a good thing from a security perspective). Communication Initiated from the Control Plane to the Worker Nodes . The trick is to create a VPN from the worker nodes to the control plane in advance and let the control plane use this tunnel for communication. The configuration for this is done at cluster creation time. The way we have implemented this until recently was by managing an openVPN tunnel. This had some drawbacks and has now been replaced by a tool from the Kubernetes community dedicated for this special purpose: enter Konnectivity. This tool offers more reliability by being able to create multiple tunnels (instead of only one with the previous solution) much ore easily. ",
    "url": "/gks/clusterlifecycle/controlplaneconnector/#cluster-setup",
    
    "relUrl": "/gks/clusterlifecycle/controlplaneconnector/#cluster-setup"
  },"106": {
    "doc": "Control Plane Connector",
    "title": "Install/Migration",
    "content": "When creating a new cluster with the GKS platform, setup of Konnectivity is the default. Existing clusters can be migrated easily by editing the cluster in the GKS dashboard and checking the box right next to Konnectivity. This will initiate an automatic and seamless removal of openVPN and creation of Konnectivity agent and -server pairs on the worker- and control plane nodes respectively. ",
    "url": "/gks/clusterlifecycle/controlplaneconnector/#installmigration",
    
    "relUrl": "/gks/clusterlifecycle/controlplaneconnector/#installmigration"
  },"107": {
    "doc": "Control Plane Connector",
    "title": "Learn More",
    "content": ". | Konnectivity | GitHub repo for specification | . ",
    "url": "/gks/clusterlifecycle/controlplaneconnector/#learn-more",
    
    "relUrl": "/gks/clusterlifecycle/controlplaneconnector/#learn-more"
  },"108": {
    "doc": "Using Openstack Application Credentials",
    "title": "Using Openstack Application Credentials",
    "content": "In previous parts of the documentation the user needs to authenticate with the Openstack API to access resources (like compute instances for worker nodes and networks, etc.) which are used to build a Kubernetes Cluster. This has been shown with the users username and password. As the openstack-cloud-controller and machine-controller need access to the same Openstack tenants’ resources those credentials are persisted into the cluster as kubernetes secrets in the kube-system namespace. It follows that everyone with the cluster-admin role in that cluster will have access to that username and password as well. There is a way to prevent this. In Openstack exists a feature called Openstack Application Credentials. This Openstack feature allows to create an additional set of credentials with similar or reduced set of permissions to an Openstack project. The key aspect of this feature is that it is not possible to login to the Optimist dashboard with Application Credentials, thus increasing the security of your Openstack tenant. Another thing to note is that Application Credentials are bound to a project, not a tenant. This additional separation of concerns makes it easy to have one Openstack tenant with multiple projects like dev, test and prod, creating Application Credentials for each project separately thus being able to manage the resources of all projects while still retaining maximum isolation between your environments. ",
    "url": "/gks/clusterlifecycle/applicationcredentials/",
    
    "relUrl": "/gks/clusterlifecycle/applicationcredentials/"
  },"109": {
    "doc": "Using Openstack Application Credentials",
    "title": "Creating Openstack Application Credentials",
    "content": "The creation of Openstack Application Credentials is best described in the Openstack documentation here. ",
    "url": "/gks/clusterlifecycle/applicationcredentials/#creating-openstack-application-credentials",
    
    "relUrl": "/gks/clusterlifecycle/applicationcredentials/#creating-openstack-application-credentials"
  },"110": {
    "doc": "Using Openstack Application Credentials",
    "title": "Using the Application Credentials during GKS cluster creation",
    "content": "When creating a GKS cluster in the GKS-dashboard the user is prompted for Openstack credentials in step 3 (Settings) of the cluster creation wizard. Default is set to User Credentials, but there is another tab to it’s right named Application Credentials. Clicking that tab will change the number of fields in the form slightly: there is no need to specify the project nor the projectID as Application Credentials are automatically bound/scoped to a single project. After entering the Application Credentials the GKS cluster creation process continues as described in previous chapters. As confirmation the summary page in step 5 of the cluster creation wizard will show the applicationID of your Application Credentials instead of the domain, user- and project-name. ",
    "url": "/gks/clusterlifecycle/applicationcredentials/#using-the-application-credentials-during-gks-cluster-creation",
    
    "relUrl": "/gks/clusterlifecycle/applicationcredentials/#using-the-application-credentials-during-gks-cluster-creation"
  },"111": {
    "doc": "Using Openstack Application Credentials",
    "title": "Updating Application Credentials in a running GKS cluster",
    "content": "Another nice feature of Openstack Application Credentials is that they can have an expiry date. In that case the user will need to replace the current Application Credentials with newer ones. For that the user will need to create a new set of Application Credentials before the current set expires. After creation the user will need to enter the cluster-view in the GKS-dashboard. To change the Application Credentials click on the three-vertical-dots button in the upper right corner of the page and select the “Edit Provider” option. The next step will present a dialog with two tabs available: “User Credentials” and “Application Credentials”. The prior will be shown as default. The user needs to actively choose the Application Credentials tab. Switching tabs will reduce the fields in the form down to Application Credential ID and Application Credential Secret. After entering the correct values the settings need to be saved. Now the automation will start a process in the background to replace the old Application Credentials set with the one newly entered. ",
    "url": "/gks/clusterlifecycle/applicationcredentials/#updating-application-credentials-in-a-running-gks-cluster",
    
    "relUrl": "/gks/clusterlifecycle/applicationcredentials/#updating-application-credentials-in-a-running-gks-cluster"
  },"112": {
    "doc": "Changing Openstack Credentials",
    "title": "Changing Openstack Credentials",
    "content": "It is possible to change the credentials that GKS uses to create Openstack resources. This could be required after you changed the password, or when you need to use a different account. ",
    "url": "/gks/clusterlifecycle/openstackcredentials/",
    
    "relUrl": "/gks/clusterlifecycle/openstackcredentials/"
  },"113": {
    "doc": "Changing Openstack Credentials",
    "title": "Changing the Credentials",
    "content": "To change the credentials perform the following steps: . | Navigate to the desired cluster. | Click the three dots, to open the cluster submenu. | Select Edit Provider. | Change the credentials. | . Shortly after performing these steps, a confirmation message pops up, and the credentials have been changed successfully. ",
    "url": "/gks/clusterlifecycle/openstackcredentials/#changing-the-credentials",
    
    "relUrl": "/gks/clusterlifecycle/openstackcredentials/#changing-the-credentials"
  },"114": {
    "doc": "Deleting a Cluster",
    "title": "Deleting a Cluster",
    "content": "It’s quick and simple to delete clusters in the GKS dashboard. The only prerequisite is a running cluster in a GKS project. ",
    "url": "/gks/clusterlifecycle/deletingacluster/",
    
    "relUrl": "/gks/clusterlifecycle/deletingacluster/"
  },"115": {
    "doc": "Deleting a Cluster",
    "title": "Finding the Cluster",
    "content": "To delete a cluster, you need to go into the cluster’s detail view. For this, click on first-system. You need to use the cluster name later. To copy it into the clipboard, click on the name. ",
    "url": "/gks/clusterlifecycle/deletingacluster/#finding-the-cluster",
    
    "relUrl": "/gks/clusterlifecycle/deletingacluster/#finding-the-cluster"
  },"116": {
    "doc": "Deleting a Cluster",
    "title": "Deleting the Cluster",
    "content": "Now click Delete. This opens a window where you need to enter the cluster name to avoid sudden and unwanted deletions. Since you copied the name into your clipboard previously, you can simply paste it here. Since we also want to free up the resources, leave both check boxes marked. That way, volumes and load balancers provided by OpenStack will be removed as well. ",
    "url": "/gks/clusterlifecycle/deletingacluster/#deleting-the-cluster",
    
    "relUrl": "/gks/clusterlifecycle/deletingacluster/#deleting-the-cluster"
  },"117": {
    "doc": "Deleting a Cluster",
    "title": "Summary",
    "content": "Congratulations! You learned and achieved the following: . | How to delete a cluster | How to delete all resources in OpenStack as well | . ",
    "url": "/gks/clusterlifecycle/deletingacluster/#summary",
    
    "relUrl": "/gks/clusterlifecycle/deletingacluster/#summary"
  },"118": {
    "doc": "Deprecation Policy",
    "title": "Deprecation Policy",
    "content": "The upstream Kubernetes project releases approximately four Kubernetes versions a year and deprecates the same number of old versions. Up to v1.18, Kubernetes followed an N-2 support policy, meaning that the 3 most recent minor versions received security and bug fixes. From v1.19 onwards, Kubernetes follows an N-3 support policy. This means that the support window is extended to one year. A good visualization of the support period for each release is available below: . GKS aligns loosely to this lifecycle by continuously introducing new versions and deprecating older ones. After a given Kubernetes version has reached End-of-Life, it will not get any bugfixes or security updates. Hence, we cannot support it anymore either and have to deprecate it. ",
    "url": "/gks/clusterlifecycle/deprecationpolicy/",
    
    "relUrl": "/gks/clusterlifecycle/deprecationpolicy/"
  },"119": {
    "doc": "Deprecation Policy",
    "title": "Deprecation Process",
    "content": "If we deprecate a Kubernetes version, we inform the customers in advance about the deprecation (End-of-Life announcement) to give them enough time to plan, prepare, and perform the Kubernetes upgrade themselves. When this grace period is over, we remove the deprecated version from GKS. You can find the list of supported Kubernetes versions and their planned End-of-Life dates here. A detailed documentation about how to upgrade clusters is available here. What Does an End-of-Life Announcement Mean for Me? . If an End-of-Life announcement for a given Kubernetes version has been made, customers are asked to upgrade their clusters to a newer version (preferably to the latest one). What Happens If I Do Not Update Until the EOL Date? . If a customer cluster will not get updated before the removal of an EOL Kubernetes version, nothing happens at first. One cannot create new clusters with the deprecated version anymore, but existing clusters will continue to run. Can I Stay on an EOL Version Forever? . No, as this would possibly mean serious security issues in the future. ",
    "url": "/gks/clusterlifecycle/deprecationpolicy/#deprecation-process",
    
    "relUrl": "/gks/clusterlifecycle/deprecationpolicy/#deprecation-process"
  },"120": {
    "doc": "Deprecation Policy",
    "title": "GKS Force Upgrade Policy",
    "content": "If a Kubernetes version reaches End-of-Life, we have to remove its support from GKS since it will not receive any bugfixes or security updates anymore. From this point onward it is no longer possible to create new clusters with this version. It is important to emphasize the following technical limitations in Kubernetes: . | A (control plane of a) Kubernetes cluster can be upgraded by one version at a time, e.g. from v1.21 -&gt; v1.22. | It is not possible to upgrade more than one versions at once. | It is not possible to downgrade a cluster. | . This means that if customers do not update their clusters before the removal of the next EOL version, they risk not being able to upgrade after the removal of the next deprecated version. This would imply a serious problem as their only alternative would be to create a new cluster and migrate the workload from the old one as upgrading would not be possible (since it would require upgrading two versions at once). To overcome this issue, we need to actively force customers to upgrade clusters running on an EOL Kubernetes version, before we remove the next deprecated version. What Happens to My Clusters During Force Upgrade? . We will initiate an automated Kubernetes upgrade for the control plane and the Machine Deployment(s). While this should work, it cannot be guaranteed to work, given the diversity of applications and use cases. Breaking changes in the Kubernetes API can lead to broken/incompatible applications inside the Kubernetes cluster. We can not overtake responsibility for such problems. That’s why we strongly advise every customer to plan and perform the Kubernetes upgrade themselves. ",
    "url": "/gks/clusterlifecycle/deprecationpolicy/#gks-force-upgrade-policy",
    
    "relUrl": "/gks/clusterlifecycle/deprecationpolicy/#gks-force-upgrade-policy"
  },"121": {
    "doc": "Migration Container Runtime Engine",
    "title": "Migration Container Runtime Engine",
    "content": " ",
    "url": "/gks/clusterlifecycle/clustermigrations/containerruntimeengine/",
    
    "relUrl": "/gks/clusterlifecycle/clustermigrations/containerruntimeengine/"
  },"122": {
    "doc": "Migration Container Runtime Engine",
    "title": "General Container Runtime Information",
    "content": "For a long time now Docker (or rather the dockershim to be more precise) has been used as the default Container Runtime Engine in the underlying infrastructure of Kubernetes. But maintaining this dockershim on the development side has become a heavy burden on the Kubernetes maintainers. To reduce this burden the CRI standard has been implemented, but unfortunately Docker itself does not implement this standard (hence the dockershim!). With the Kubernetes v1.20 release the deprecation of the dockershim has been announced, scheduled for the release of v1.24. So to be able to upgrade to Kubernetes v1.24 we will need the Kubernetes nodes to run a different container runtime engine which is CRI-standard compatible. For GKS this will be containerd. ",
    "url": "/gks/clusterlifecycle/clustermigrations/containerruntimeengine/#general-container-runtime-information",
    
    "relUrl": "/gks/clusterlifecycle/clustermigrations/containerruntimeengine/#general-container-runtime-information"
  },"123": {
    "doc": "Migration Container Runtime Engine",
    "title": "Changing the Container Runtime Engine Config",
    "content": "To upgrade from Docker to containerd the following steps are needed: . | Edit the cluster configuration. | Change the value of the Container Runtime field from docker to containerd and save the changes. | . ",
    "url": "/gks/clusterlifecycle/clustermigrations/containerruntimeengine/#changing-the-container-runtime-engine-config",
    
    "relUrl": "/gks/clusterlifecycle/clustermigrations/containerruntimeengine/#changing-the-container-runtime-engine-config"
  },"124": {
    "doc": "Migration Container Runtime Engine",
    "title": "Activating the Config Change",
    "content": "For the change to take effect, you need to rotate your worker nodes once. This can be done by either upgrading to a new Kubernetes version or by doing a restart of the rollout of your MachineDeployment. While the prior can be done in the Web UI easily (and is already covered by here), the latter will be shown below. | Check which container runtime your workers are currently using: . $ kubectl describe node | grep \"Container Runtime Version\" Container Runtime Version: docker://19.3.15 Container Runtime Version: docker://19.3.15 Container Runtime Version: docker://19.3.15 . Here we see in the output that docker is still used as the container runtime. | Restart the Machine Deployment as follows: . | Click on the Machine Deployment of the cluster. | Click on the Restart button. | Confirm the restart of the Machine Deployment. | . | . Now one after another, a new machine will be added and an old machine will be removed after its workload has been transferred into the rest of the cluster. After the last old machine has been removed, the restart of the MachineDeployment is complete. | Now check again, which container runtime powers your worker nodes: . $ kubectl describe node | grep \"Container Runtime Version\" Container Runtime Version: containerd://1.5.4 Container Runtime Version: containerd://1.5.4 Container Runtime Version: containerd://1.5.4 . | . This concludes the migration of your Kubernetes clusters container runtime from docker to containerd. ",
    "url": "/gks/clusterlifecycle/clustermigrations/containerruntimeengine/#activating-the-config-change",
    
    "relUrl": "/gks/clusterlifecycle/clustermigrations/containerruntimeengine/#activating-the-config-change"
  },"125": {
    "doc": "Migration Container Runtime Engine",
    "title": "Learn More",
    "content": "More background information on the container runtime engines for Kubernetes is available here: . | https://kubernetes.io/blog/2020/12/02/dockershim-faq/ | https://kubernetes.io/blog/2020/12/02/dont-panic-kubernetes-and-docker/ | . ",
    "url": "/gks/clusterlifecycle/clustermigrations/containerruntimeengine/#learn-more",
    
    "relUrl": "/gks/clusterlifecycle/clustermigrations/containerruntimeengine/#learn-more"
  },"126": {
    "doc": "Cluster Backups",
    "title": "Cluster Backups",
    "content": " ",
    "url": "/gks/clusterbackups/",
    
    "relUrl": "/gks/clusterbackups/"
  },"127": {
    "doc": "Cluster Backups",
    "title": "Learn More",
    "content": ". | Etcd Backups and Restore | Restore a PVC from an Existing Openstack Volume | . ",
    "url": "/gks/clusterbackups/#learn-more",
    
    "relUrl": "/gks/clusterbackups/#learn-more"
  },"128": {
    "doc": "Etcd Backups and Restore",
    "title": "Etcd Backups and Restore",
    "content": "GKS supports a simple etcd backup and restore feature, which is enabled by default for all clusters. The default backup configuration schedule is created with a performed backup of @every 20m interval and a retention of 20 backups. However, it’s possible to create additional backup configuration if needed. Note: Before starting keep in mind that this is only an etcd backup and restore. The only thing that is restored is the etcd state, not PVC volumes with application data or similar. ",
    "url": "/gks/clusterbackups/etcdbackups/",
    
    "relUrl": "/gks/clusterbackups/etcdbackups/"
  },"129": {
    "doc": "Etcd Backups and Restore",
    "title": "Etcd Backups",
    "content": "Creating Etcd Backup Schedules . etcd backups and restores are resources bounded to a project. You can manage them in the Project view. To create a new backup schedule, you need to click on the Add Automatic Backup button. You have a choice of preset daily, weekly, or monthly backups, or you can create a backup with a custom interval and keep time: . To see all available backups, click on a backup you are interested in: . You will see the list of the completed backups: . Creating Etcd Backup Snapshots . You can also create one-time backup snapshots. They are set up similarly to the automatic ones, with the difference that they do not have a schedule or keep count set. ",
    "url": "/gks/clusterbackups/etcdbackups/#etcd-backups",
    
    "relUrl": "/gks/clusterbackups/etcdbackups/#etcd-backups"
  },"130": {
    "doc": "Etcd Backups and Restore",
    "title": "Restoring Etcd Backups",
    "content": "If you want to restore a backup, you need to click on the restore from backup icon in the UI. Restoring Etcd Backups from Schedule . Restoring Etcd Backups from Snapshot . After that the cluster gets paused, etcd gets deleted, and then it is recreated from the backup. When it’s done, the cluster is unpaused again. In the meantime, an EtcdRestore object is created in the project, and you can observe its progress in the EtcdRestore list. In the cluster view, you may notice that your cluster is in a Restoring state, and you can not interact with it until it is done. When it’s done, the cluster gets unpaused and unblocked, so you can use it. The Etcd Restore goes into a Completed state. ",
    "url": "/gks/clusterbackups/etcdbackups/#restoring-etcd-backups",
    
    "relUrl": "/gks/clusterbackups/etcdbackups/#restoring-etcd-backups"
  },"131": {
    "doc": "Restore a PVC from an Existing Openstack Volume",
    "title": "Restore a PVC from an Existing Openstack Volume",
    "content": "Normally, creating a PVC (PersistentVolumeClaim) in one of our Kubernetes clusters triggers the creation of a new PV (PersistentVolume) in Kubernetes and a new Volume in Openstack respectively. But it is also possible to use an existing Openstack volume for that purpose as described in this section. ",
    "url": "/gks/clusterbackups/restorepvcfromvolume/",
    
    "relUrl": "/gks/clusterbackups/restorepvcfromvolume/"
  },"132": {
    "doc": "Restore a PVC from an Existing Openstack Volume",
    "title": "Prerequisites",
    "content": "As a prerequisite, you need an existing, unused volume in Openstack. This could be the case, for example, if you have deleted a cluster without deleting all attached PVCs before, or if you want to move a volume from one cluster to another. To be able to use an existing Openstack volume in a Kubernetes cluster, you need to find out its ID. To do so, go to the Openstack/Optimist Dashboard: . Log in with your credentials. The credentials of the Kubernetes UI and Openstack dashboard are identical. Once you are logged in, navigate to Volumes and search for the volume which you want to use. That volume should not be used by any machine and should have the status “Available”. (Note: A volume can only be used by one instance at a time, so if it is still in use, you have to detach it from the old instance first.) . After you found the volume you want to use, click on its name. This brings you to the volume details page where you can find the ID of the volume. Note down this ID. ",
    "url": "/gks/clusterbackups/restorepvcfromvolume/#prerequisites",
    
    "relUrl": "/gks/clusterbackups/restorepvcfromvolume/#prerequisites"
  },"133": {
    "doc": "Restore a PVC from an Existing Openstack Volume",
    "title": "Adding the PV with the Existing Volume",
    "content": "To manually create a PV that references an existing volume, you have to specify the volume ID in the spec.csi.volumeHandle-key in the PersistentVolume-manifest: . apiVersion: v1 kind: PersistentVolume metadata: name: test-pv-restore spec: accessModes: - ReadWriteOnce capacity: storage: 3Gi csi: driver: cinder.csi.openstack.org volumeHandle: 6515d33b-287d-43e1-a3c5-e347d2fc8135 persistentVolumeReclaimPolicy: Delete storageClassName: cinder-csi volumeMode: Filesystem . Apply this manifest and check if the PV got created successfully: . # kubectl apply -f restore-pv.yaml persistentvolume/test-pv-restore created # kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE test-pv-restore 3Gi RWO Delete Available cinder-csi 3s . This example created a PV named test-pv-restore that referenced the existing Openstack volume. ",
    "url": "/gks/clusterbackups/restorepvcfromvolume/#adding-the-pv-with-the-existing-volume",
    
    "relUrl": "/gks/clusterbackups/restorepvcfromvolume/#adding-the-pv-with-the-existing-volume"
  },"134": {
    "doc": "Restore a PVC from an Existing Openstack Volume",
    "title": "Adding the PVC Referencing the Correct PV",
    "content": "Next, a PVC needs to be build which references the PV you just created. To do so, the PVC needs to have the spec.volumeName-key set to the PV name: . apiVersion: v1 kind: PersistentVolumeClaim metadata: name: test-pvc spec: accessModes: - ReadWriteOnce resources: requests: storage: 3Gi volumeName: test-pv-restore . Applying this manifest should result in a PVC in state “Bound”: . # kubectl apply -f restore-pvc.yaml persistentvolumeclaim/test-pvc created # kubectl get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE test-pvc Bound test-pv-restore 3Gi RWO cinder-csi 2s . This PVC named “test-pvc” is now ready to be used by a Pod. ",
    "url": "/gks/clusterbackups/restorepvcfromvolume/#adding-the-pvc-referencing-the-correct-pv",
    
    "relUrl": "/gks/clusterbackups/restorepvcfromvolume/#adding-the-pvc-referencing-the-correct-pv"
  },"135": {
    "doc": "Restore a PVC from an Existing Openstack Volume",
    "title": "Creating a Test-Pod to Inspect the Data",
    "content": "As you may want to inspect the PVC before using it, let’s create a test-pod to inspect the data on the volume: . apiVersion: v1 kind: Pod metadata: name: test-pod spec: containers: - name: test-pod image: ubuntu command: - \"sleep\" - \"604800\" volumeMounts: - mountPath: \"/restore\" name: test-pvc volumes: - name: test-pvc persistentVolumeClaim: claimName: test-pvc . The important part in the above example is that the claimName is set correctly to the PVC which we just created. After applying the manifest and creating the Pod, the volume from our example is mounted under /restore – which you can check by executing into the pod and opening a shell: . # kubectl apply -f pvc-example/test-pod.yaml pod/test-pod created # kubectl get pod -w NAME READY STATUS RESTARTS AGE test-pod 0/1 ContainerCreating 0 5s test-pod 0/1 ContainerCreating 0 17s test-pod 1/1 Running 0 22s ^C # kubectl exec -ti test-pod -- /bin/bash root@test-pod:/# ls /restore/ lost+found my_data.txt root@test-pod:/# exit exit . That’s basically it – you have used an existing Openstack volume and added to a Pod in your Kubernetes cluster. ",
    "url": "/gks/clusterbackups/restorepvcfromvolume/#creating-a-test-pod-to-inspect-the-data",
    
    "relUrl": "/gks/clusterbackups/restorepvcfromvolume/#creating-a-test-pod-to-inspect-the-data"
  },"136": {
    "doc": "Machine Deployments",
    "title": "Machine Deployments",
    "content": "A Kubernetes-Cluster typically consists of a controlplane (running the Kubernetes API, the etcd database, and other components) and worker nodes - (virtual) servers, which run the actual workloads (applications etc.). The controlplane of all GKS-Clusters is managed by the GKS platform itself and is not accessible for customers. The worker nodes of a cluster need to be configured with a Machine Deployment. The customer can control: . | Node Flavors are the virtual machine types which are used to create the worker nodes. The types usually differ in their size, i.e. in the available CPU and RAM. | Number of Nodes you can choose how many worker nodes you want to have in total. | SSH-Keys if enabled for your cluster and if your nodes have a public IP (Floating IP), you can connect to your worker nodes with SSH, i.e. for extended debugging. | Operating System of the Nodes although the OS of the worker-nodes should not matter for the Kubernetes workloads, it might matter for you and your debugging. | . ",
    "url": "/gks/machinedeployments/",
    
    "relUrl": "/gks/machinedeployments/"
  },"137": {
    "doc": "Machine Deployments",
    "title": "Learn More",
    "content": ". | Sizing: Choosing Node Flavors | Debugging: Cluster Nodes Usage Rate | Debugging: Adding SSH Keys to Worker Nodes | Upgrading the OS on Worker Nodes | Cluster Nodes in multiple Availability Zones | . ",
    "url": "/gks/machinedeployments/#learn-more",
    
    "relUrl": "/gks/machinedeployments/#learn-more"
  },"138": {
    "doc": "Machine Deployment",
    "title": "Machine Deployment",
    "content": " ",
    "url": "/gks/machinedeployments/machinedeployment/",
    
    "relUrl": "/gks/machinedeployments/machinedeployment/"
  },"139": {
    "doc": "Machine Deployment",
    "title": "Adding a Machine Deployment",
    "content": "To add a new Machine Deployment, use the Add Machine Deployment button in the upper right corner. This opens the Add Machine Deploymentwindow, which has the same options as at cluster creation time. Add your data, and click Add Machine Deployment to create the new nodes. You can look at the progress in the Machine Deployment details. Click on the new Machine Deployment. Wait until all nodes are green. ",
    "url": "/gks/machinedeployments/machinedeployment/#adding-a-machine-deployment",
    
    "relUrl": "/gks/machinedeployments/machinedeployment/#adding-a-machine-deployment"
  },"140": {
    "doc": "Machine Deployment",
    "title": "Deleting a Machine Deployment",
    "content": "To delete a Machine Deployment, use the trash symbol in the list. Alternatively, you can use the trash symbol on the details page. ",
    "url": "/gks/machinedeployments/machinedeployment/#deleting-a-machine-deployment",
    
    "relUrl": "/gks/machinedeployments/machinedeployment/#deleting-a-machine-deployment"
  },"141": {
    "doc": "Machine Deployment",
    "title": "Renaming a Machine Deployment",
    "content": "Machine Deployment cannot be renamed. Therefore, you need to create a second one and delete the old one. Deleting a Machine Deployment deletes all nodes at the same time. Depending on your replicas and number of nodes this can lead to a downtime. To mitigate this, you should reduce the replica of the Machine Deployment step-by-step until it is 0 and then delete the Machine Deployment. The pods will be most likely rescheduled to the new host directly, but it might be that some pod will end up on old nodes. This will lead to many rescheduled ones. If this is a problem, you can first cordon all old nodes with kubectl cordon &lt;node name&gt; and then slowly reduce the replicas. ",
    "url": "/gks/machinedeployments/machinedeployment/#renaming-a-machine-deployment",
    
    "relUrl": "/gks/machinedeployments/machinedeployment/#renaming-a-machine-deployment"
  },"142": {
    "doc": "Choosing Node Flavors/Sizes",
    "title": "Choosing Node Flavors/Sizes",
    "content": "During the setup of new nodes in a cluster you can select a respective “flavor” for that node. Flavors determine the amount of processing cores (“CPUs”), Memory (“GB RAM”), and disk space (“GB Disk”) of the respective node. A good practice for choosing the right flavor for your project is going through the following steps: . | Start by selecting a small flavor. | Evaluate the performance of the desired use case. | Pick a larger flavor in case of any issues. | . ",
    "url": "/gks/machinedeployments/nodeflavors/",
    
    "relUrl": "/gks/machinedeployments/nodeflavors/"
  },"143": {
    "doc": "Choosing Node Flavors/Sizes",
    "title": "Changing Flavors",
    "content": "Changing the flavor of a node at any point in time is as easy as following these steps: . | Navigate to the desired cluster. | Click the Edit icon on the node for which you would like to select another flavor. The icon becomes visible when hovering over the node with your mouse. | Select the desired flavor and click the Save Changesbutton. | Soon after performing these steps, a confirmation message pops up, and your node flavor has been changed successfully. | . ",
    "url": "/gks/machinedeployments/nodeflavors/#changing-flavors",
    
    "relUrl": "/gks/machinedeployments/nodeflavors/#changing-flavors"
  },"144": {
    "doc": "Cluster Nodes Usage Rate",
    "title": "Cluster Nodes Usage Rate",
    "content": "When checking the cluster, we noticed an unusually high memory usage. We see that one node is fully utilized while the other shows little load. This often happens when a cluster with less than three nodes is upgraded at the nodes. Here we explain the reason for it. ",
    "url": "/gks/machinedeployments/clusternodesusagerate/",
    
    "relUrl": "/gks/machinedeployments/clusternodesusagerate/"
  },"145": {
    "doc": "Cluster Nodes Usage Rate",
    "title": "Finding Nodes Workload",
    "content": "First, we check how many nodes are in the cluster and the current load of CPU/memory. The command kubectl top node shows the current node usage. In the example, there are two running nodes. First the node is cordoned, so that no new pods are scheduled to this node. Then the node is drained so that it is made completely empty, and the pods that were previously running on the native node are distributed to all other nodes in the cluster. With only two nodes, everything is placed on exactly the other node and then we have a high load on this node. When the second node is back after the update, the pods are not automatically distributed to both nodes. This leads to the initially observed imbalance. ",
    "url": "/gks/machinedeployments/clusternodesusagerate/#finding-nodes-workload",
    
    "relUrl": "/gks/machinedeployments/clusternodesusagerate/#finding-nodes-workload"
  },"146": {
    "doc": "Cluster Nodes Usage Rate",
    "title": "Some Tips",
    "content": ". | We recommend using at least three nodes, as this allows loads to be better distributed during upgrades. | The tool ‘popeye’ analyzes clusters and makes suggestions for improvement based on best practices. | Upgrade the nodes to the latest version and perform the final step in section Kubernetes Updates. | . ",
    "url": "/gks/machinedeployments/clusternodesusagerate/#some-tips",
    
    "relUrl": "/gks/machinedeployments/clusternodesusagerate/#some-tips"
  },"147": {
    "doc": "Adding an SSH Key to an Existing Cluster",
    "title": "Adding an SSH Key to an Existing Cluster",
    "content": "With the GKS platform you can add SSH keys to worker nodes. This might be useful if you want to debug your Kubernetes clusters and your application directly from the worker nodes. To achieve this, you need to: . | Create an SSH key | Have a cluster with User SSH Key Agent enabled | Add the key to the project | Enable it in the cluster | . In most cases you also need to assign a Floating IP to your worker nodes to be able to access them. ",
    "url": "/gks/machinedeployments/add_ssh_key/",
    
    "relUrl": "/gks/machinedeployments/add_ssh_key/"
  },"148": {
    "doc": "Adding an SSH Key to an Existing Cluster",
    "title": "User SSH Key Agent",
    "content": "To manage SSH keys on the worker nodes, you must enable the User SSH Key Agent during cluster creation: . If you do not activate this setting during cluster creation, you cannot add/modify SSH keys later on. You can only add the User SSH Key Agent during cluster creation. If you did not enable the User SSH Key Agent during cluster creation, you cannot enable it later on. Checking the Status of the User SSH Key Agent . To check if the User SSH Key Agent is enabled for a certain cluster, check the cluster status page. To do so, open the additional cluster information view. You can now see the status in the lower right corner. If the User SSH Key Agent is activated, you can add SSH keys any time as described below. Other Ways of Managing User SSH Keys . You can create a cluster without enabling the User SSH Key Agent. In this case all worker nodes are created without any SSH keys added - and SSH keys cannot be changed when using our platform. This would allow other methods/agents of managing SSH keys like saltstack, puppet, or chef if the worker nodes are created from a custom image supporting this. It is not possible to add the User SSH Key Agent after cluster creation in order to not interfere and accidentially break such setups. ",
    "url": "/gks/machinedeployments/add_ssh_key/#user-ssh-key-agent",
    
    "relUrl": "/gks/machinedeployments/add_ssh_key/#user-ssh-key-agent"
  },"149": {
    "doc": "Adding an SSH Key to an Existing Cluster",
    "title": "Adding an SSH Key to an Existing Cluster",
    "content": "If you want to add an SSH Key to an existing cluster which has the User SSH Key Agent is enabled, follow the steps below. Creating an SSH Key . The simplest way to generate a key pair is to run ssh-keygen without arguments: . ssh-keygen . An SSH key will be created. The default path for the SSH key is: ~/.ssh/id_rsa.pub. Adding the SSH Key to the Project . | Select the project. | . | Go to the SSH Key page. | . | Use the Add SSH Key button. | . | Name the key and paste the public SSH key that was created by ssh-keygen (not the private key!). | . Now you can use the key in any cluster in this project. Adding the SSH Key to the Cluster . | Select a cluster where you want to add the key. | Click the three dots, to open the cluster submenu. | Select Manage SSH keys. | Now the newly created SSH key can be selected from a drop-down list. | After the selection, the key is displayed in the list and can also be deleted from it if required. | . Your key will now be added to all worker nodes in all machine deployments. ",
    "url": "/gks/machinedeployments/add_ssh_key/",
    
    "relUrl": "/gks/machinedeployments/add_ssh_key/"
  },"150": {
    "doc": "Adding an SSH Key to an Existing Cluster",
    "title": "Adding an SSH Key During Cluster Creation",
    "content": "You can also add SSH keys during cluster creation. For more information, see section Creating a Cluster. ",
    "url": "/gks/machinedeployments/add_ssh_key/#adding-an-ssh-key-during-cluster-creation",
    
    "relUrl": "/gks/machinedeployments/add_ssh_key/#adding-an-ssh-key-during-cluster-creation"
  },"151": {
    "doc": "Adding an SSH Key to an Existing Cluster",
    "title": "Access to the Node",
    "content": "Once you added the SSH key to the cluster, you need to attach a Floating IP to the Machine Deployment to be able to access the worker nodes with SSH. To achieve this, you have to edit the Machine Deployment: . Make sure Allocate Floating IP is selected: . Once the node is fully created and has an external IP, you can access the node with the key. The default user for Flatcar is core. ssh -A core@PUBLIC_IP . ",
    "url": "/gks/machinedeployments/add_ssh_key/#access-to-the-node",
    
    "relUrl": "/gks/machinedeployments/add_ssh_key/#access-to-the-node"
  },"152": {
    "doc": "Updating the OS of Worker Nodes",
    "title": "Updating the OS of Worker Nodes",
    "content": " ",
    "url": "/gks/machinedeployments/updatingnodeos/",
    
    "relUrl": "/gks/machinedeployments/updatingnodeos/"
  },"153": {
    "doc": "Updating the OS of Worker Nodes",
    "title": "Flatcar",
    "content": "Automatic Updates of Flatcar Worker Nodes . GKS provides the functionality to keep the operating system of Flatcar-based worker nodes up-to-date. This feature automatically installs any updates released by the upstream vendor (Kinvolk) for Flatcar on the worker nodes. The auto-update feature uses FLUO, the Flatcar Linux Update Operator, in the background. When a reboot is needed after updating the system, it will drain the node before rebooting it. It coordinates the reboots of multiple nodes in the cluster, ensuring that only one node is rebooting at once. Using the auto-update functionality is enabled by default. The following screenshot shows the creation of a machine deployment with auto-updater enabled: . If you would like to take care of OS updates (and the reboots) yourself, you can disable the automatic updates of the worker nodes by selecting the Disable auto-update checkbox: . We highly encourage our users to use the auto-update feature to keep your infrastructure safe. Checking the State of the Auto-Updater . To check if your nodes receive automatic OS-updates, click on the machine deployment: . Check if the Disable auto-update option has a green checkmark in front of it (auto-updater is off): . Or check if it’s greyed out (auto-updater is on): . Enabling/Disabling Auto-Updater on an Existing Machine Deployment . To change the status of the auto-updater, click on the “edit” button of the machine deployment. (De)-select the checkbox accordingly. After clicking on Save Changes, all worker nodes will perform a rolling update and reboot. Updating Flatcar Worker Nodes Manually . To update a Flatcar worker node manually, you require access with SSH. You can find out the actual installed OS version from /etc/os-release. $ grep VERSION_ID /etc/os-release VERSION_ID=2765.2.2 . In the next step you’ll unmask and start the systemd unit update-engine. $ sudo systemctl unmask update-engine.service Removed /etc/systemd/system/update-engine.service. $ sudo systemctl start update-engine.service . Now you can check for available updates and install them. sudo update_engine_client -check_for_update sudo update_engine_client -status . The Update-Engine client now downloads the latest available release of Flatcar and adjusts the boot order automatically so that the new release will be activated at the next boot. As soon as the status has changed from UPDATE_STATUS_UPDATE_AVAILABLE to UPDATE_STATUS_DOWNLOADING, and then to UPDATE_STATUS_UPDATED_NEED_REBOOT, you can reboot the worker node and repeat the steps for all nodes in the machine deployment. sudo systemctl reboot . We highly recommend using the auto-updater feature, in order to ensure the security and integrity of your infrastructure. ",
    "url": "/gks/machinedeployments/updatingnodeos/#flatcar",
    
    "relUrl": "/gks/machinedeployments/updatingnodeos/#flatcar"
  },"154": {
    "doc": "Updating the OS of Worker Nodes",
    "title": "Ubuntu",
    "content": "Ubuntu support in GKS was removed in July 2021. Read here how to update to supported Flatcar OS for existing machine deployments. Update to Flatcar . To update from Ubuntu to Flatcar, click on the edit button of the machine deployment. Then click on the flatcar logo. It changed the Image and enabled the auto-update feature. The node is recreated, and your cluster is up-to-date. ",
    "url": "/gks/machinedeployments/updatingnodeos/#ubuntu",
    
    "relUrl": "/gks/machinedeployments/updatingnodeos/#ubuntu"
  },"155": {
    "doc": "Updating the OS of Worker Nodes",
    "title": "Summary",
    "content": "In this section you’ve learned the following: . | What the auto-update feature is | How to enable and disable the auto-update feature for a Flatcar machine deployment | How to apply the updates on a Flatcar worker node manually | How to enable the automatic reboots of Ubuntu-based worker nodes | . ",
    "url": "/gks/machinedeployments/updatingnodeos/#summary",
    
    "relUrl": "/gks/machinedeployments/updatingnodeos/#summary"
  },"156": {
    "doc": "Updating the OS of Worker Nodes",
    "title": "Learn More",
    "content": ". | Auto-Updating Flatcar Container Linux | FLUO on Github | The Flatcar partitioning scheme | . ",
    "url": "/gks/machinedeployments/updatingnodeos/#learn-more",
    
    "relUrl": "/gks/machinedeployments/updatingnodeos/#learn-more"
  },"157": {
    "doc": "Multiple Availability Zones",
    "title": "Multiple Availability Zones",
    "content": " ",
    "url": "/gks/machinedeployments/multipleavailabilityzones/",
    
    "relUrl": "/gks/machinedeployments/multipleavailabilityzones/"
  },"158": {
    "doc": "Multiple Availability Zones",
    "title": "Introduction",
    "content": "A machine deployment is always bound to one availability zone (in short AZ) as it’s machines cannot exist in more than one (and machines from different AZs are not mixed in one machine deployment). This restricts the storage that can used by the pods running on these machines. So the storage that pods running on these machines want to use need to be available in the same availability zone the machine resides in. ",
    "url": "/gks/machinedeployments/multipleavailabilityzones/#introduction",
    
    "relUrl": "/gks/machinedeployments/multipleavailabilityzones/#introduction"
  },"159": {
    "doc": "Multiple Availability Zones",
    "title": "Single-AZ Setups",
    "content": "Having only one machine-deployment running (which is the default) poses no problems as the storage for a pod is requested for the same AZ. If a pod gets evicted to a different machine (for whatever reason) it will still be in the same AZ and thus will be able to access that storage in the usual way. ",
    "url": "/gks/machinedeployments/multipleavailabilityzones/#single-az-setups",
    
    "relUrl": "/gks/machinedeployments/multipleavailabilityzones/#single-az-setups"
  },"160": {
    "doc": "Multiple Availability Zones",
    "title": "Multi-AZ Setups",
    "content": "Stay in the same AZ . If you create more than one machine deployment (like adding an additional one to the default one), you can choose from different availability zones. You are not required to use the same one you used for the first (default) machine deployment (although you could if it suites your use-case). When adding a second machine deployment in another AZ you need to be aware that the kubernetes scheduler could decide to schedule a pod which was previously running on a machine in one AZ to now run on a machine in a different AZ. This will leave the pod in pending state unable to be run correctly as it is missing it’s storage which is anchored to the old AZ and cannot be moved or migrated automatically. A solution to this scenario would be to pin the pod to a specific AZ. This can be done with a kubernetes feature called affinity, which can be specified on either a deployment, statefulset or daemonset. With affinity you can specify that a pod should only run in a specific AZ. This would make sure that if a pod gets evicted from a machine it will only ever be scheduled on a machine in that specific AZ. Here is an example for a deployment with one pod in AZ ES1: . apiVersion: apps/v1 kind: Deployment metadata: labels: app: myapp name: myapp namespace: default spec: replicas: 1 ... template: ... spec: affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: topology.kubernetes.io/zone: operator: In values: - es1 ... (The three consecutive dots meaning we are omitting lines which are not vital to the argument we are trying to make … the example will not work without the omitted lines though!) . spread across multiple AZs for high-availability . There is a way to achieve higher availability by creating more pods and use (anti-)affinity to make sure two pods of the same application ido not run in the same AZ. As this whole topic is about storage we need to choose a StatefulSet for this, otherwise it will not be possible for the kubernetes scheduler to connect the pods with it’s respective storages (persistent volumes and their claims). Here is an example for affinity rules of a StatefulSet with two replicas, and demanding the two replicas not to be deployed in the same AZ: . apiVersion: apps/v1 kind: StatefulSet metadata: labels: app: myapp name: myapp namespace: default spec: replicas: 2 ... template: ... spec: affinity: podAntiAffinity: requiredDuringSchedulingIgnoredDuringExecution: - labelSelector: matchExpressions: - key: app operator: In values: - myapp topologyKey: topology.kubernetes.io/zone ... (Again the three consecutive dots meaning we are omitting lines which are not vital to the argument we are trying to make … the example will not work without the omitted lines though!) . For more complicated setup please consult the original kubernetes documentation. ",
    "url": "/gks/machinedeployments/multipleavailabilityzones/#multi-az-setups",
    
    "relUrl": "/gks/machinedeployments/multipleavailabilityzones/#multi-az-setups"
  },"161": {
    "doc": "Multiple Availability Zones",
    "title": "Learn More",
    "content": ". | simple Affinity | more complicated Affinity | . ",
    "url": "/gks/machinedeployments/multipleavailabilityzones/#learn-more",
    
    "relUrl": "/gks/machinedeployments/multipleavailabilityzones/#learn-more"
  },"162": {
    "doc": "Access Management",
    "title": "Access Management",
    "content": "There are two ways how to grant other users access to an GKS cluster: . | Granting users access to a complete GKS project (which grants access to all clusters in that project as well) | Using role-based access control (RBAC) to define more fine-grained access control directly to the cluster | . ",
    "url": "/gks/accessmanagement/",
    
    "relUrl": "/gks/accessmanagement/"
  },"163": {
    "doc": "Access Management",
    "title": "Learn More",
    "content": ". | Project-Based Access: Connecting to a Cluster | Role-Based Access Control (RBAC) | Revoking Tokens | . ",
    "url": "/gks/accessmanagement/#learn-more",
    
    "relUrl": "/gks/accessmanagement/#learn-more"
  },"164": {
    "doc": "Project-Based Access",
    "title": "Project-Based Access",
    "content": "Note: This is the recommended method of granting users access to a cluster. Giving users access on project level provides them access to all clusters in this project. Users with this level of access can log in to the GKS dashboard, view, edit (dependent on the level of access), or create clusters. All users with the same level of project-access share the same kubeconfig file. This kubeconfig uses a token-based authentication, and the token is bound to the level of access (read-only/admin access). ",
    "url": "/gks/accessmanagement/connectingtoacluster/",
    
    "relUrl": "/gks/accessmanagement/connectingtoacluster/"
  },"165": {
    "doc": "Project-Based Access",
    "title": "Connnecting to a Cluster",
    "content": "After you created a cluster in GKS, you need to connect to it to deploy and manage your applications. To find your cluster, go to the detail view of the cluster by clicking on the entry first-system: . Click on Get Kubeconfic in the top right corner. This way you download the kubeconfig file. All users with the same level of project-access share the same kubeconfigfile. The file contains all end points, certificates, and other information about the cluster. The kubectl command uses this file to connect to the cluster. To use kubeconfig, you need to register it on the console. There are two ways to do this: . | kubectl by default tries to use the file .kube/config in your home directory. | You can temporarily use the kubeconfig by exporting it to an environment variable. | . To keep things straightforward and to avoid changing standards on your system, choose the second method in the example. For this you need to open a Terminal. In the screenshots we use iTerm2 on macOS, but the examples work the same way when using bash on Linux or Windows. First, you need to find the downloaded kubeconfig file. Browsers like Chrome or Firefox usually store it in the Downloads folder. The name is composed of two parts: . | kubeconfig-admin- | The cluster id | . To register the kubeconfig, use the following command: . cd Downloads export KUBECONFIG=$(pwd)/kubeconfig-admin-CLUSTERID . Now you can interact with the cluster. The simplest command is: “show all the nodes that comprise my cluster”: . kubectl get nodes NAME STATUS ROLES AGE VERSION musing-kalam-XXXXXXXXX-ks4xz Ready &lt;none&gt; 10m v1.20.7 musing-kalam-XXXXXXXXX-txc4w Ready &lt;none&gt; 10m v1.20.7 musing-kalam-XXXXXXXXX-vc4g2 Ready &lt;none&gt; 10m v1.20.7 . ",
    "url": "/gks/accessmanagement/connectingtoacluster/#connnecting-to-a-cluster",
    
    "relUrl": "/gks/accessmanagement/connectingtoacluster/#connnecting-to-a-cluster"
  },"166": {
    "doc": "Project-Based Access",
    "title": "Kubernetes Dashboard",
    "content": "In GKS you can access the Kubernetes dashboard with one click. You only need to click on the Open Dashboard button on the top right of the cluster view: . Now you see the Kubernetes dashboard and can explore your cluster graphically: . ",
    "url": "/gks/accessmanagement/connectingtoacluster/#kubernetes-dashboard",
    
    "relUrl": "/gks/accessmanagement/connectingtoacluster/#kubernetes-dashboard"
  },"167": {
    "doc": "Project-Based Access",
    "title": "Learn More",
    "content": ". | Managing GKS Projects | Revoking Tokens | . ",
    "url": "/gks/accessmanagement/connectingtoacluster/#learn-more",
    
    "relUrl": "/gks/accessmanagement/connectingtoacluster/#learn-more"
  },"168": {
    "doc": "Role-Based Access Control (RBAC)",
    "title": "Role-Based Access Control (RBAC)",
    "content": "Using role-based access control allows a project admin to provide more fine-grained access based on predefined ClusterRoles and Roles. With the GKS dashboard, the admin can easily create (cluster-wide) ClusterRoleBindings and (namespace-scoped) RoleBindings: . A user with this level of access can download a specific kubeconfig, which can be directly downloaded with a direct link (see below). Such a user does not need access to the GKS dashboard at all. More information on Kubernetes RBAC is available here. ",
    "url": "/gks/accessmanagement/usingrbac/",
    
    "relUrl": "/gks/accessmanagement/usingrbac/"
  },"169": {
    "doc": "Role-Based Access Control (RBAC)",
    "title": "Granting User Access with RBAC",
    "content": "To grant a user access with RBAC, expand the RBAC-widget, and click Add Binding: . ",
    "url": "/gks/accessmanagement/usingrbac/#granting-user-access-with-rbac",
    
    "relUrl": "/gks/accessmanagement/usingrbac/#granting-user-access-with-rbac"
  },"170": {
    "doc": "Role-Based Access Control (RBAC)",
    "title": "Cluster-Wide Permissions",
    "content": "To grant users cluster-wide permissions, leave the switch on Cluster, add the email of the user, and select the role for the user: . Note that the user must exist in GKS, or otherwise he or she will not be able to log in to download kubeconfig later on. The selectable roles are predefined ClusterRoles which can be viewed by running kubectl: . kubectl get clusterrole $NAME_OF_CLUSTERROLE -o yaml . ",
    "url": "/gks/accessmanagement/usingrbac/#cluster-wide-permissions",
    
    "relUrl": "/gks/accessmanagement/usingrbac/#cluster-wide-permissions"
  },"171": {
    "doc": "Role-Based Access Control (RBAC)",
    "title": "Namespace-Wide Permissions",
    "content": "When access shall be granted on a namespace-level, switch to Namespace and add the user email there. First, you have to select the role which should be assigned to the user: . Finally, you need to select the namespace where this should be valid: . In case you want to see and understand the level of access granted here, you can view the mentioned roles with kubectl as well. Unlike ClusterRoles, Roles are scoped to a namespace, so you have to specify the namespace as well: . kubectl get role $NAME_OF_ROLE -n $NAMESPACE -o yaml . After you completed these steps, the rights should be visible in the RBAC widget of the Dashboard: . ",
    "url": "/gks/accessmanagement/usingrbac/#namespace-wide-permissions",
    
    "relUrl": "/gks/accessmanagement/usingrbac/#namespace-wide-permissions"
  },"172": {
    "doc": "Role-Based Access Control (RBAC)",
    "title": "Providing Users with Their Kubeconfig",
    "content": "Once you assigned the user a cluster- or namespace-wide role, you can provide the user a link to download kubeconfig. To do so, click the Share button on the top of the dashboard: . Next, copy the link and send it to the user: . After the user has logged in, the download of kubeconfig will start directly: . Once a user has downloaded kubeconfig, any further changes made on the RBAC will have immediate effect. Especially there is no need to revoke cluster tokens to remove access for a user. Just remove the RoleBindings and access is no longer possible. ",
    "url": "/gks/accessmanagement/usingrbac/#providing-users-with-their-kubeconfig",
    
    "relUrl": "/gks/accessmanagement/usingrbac/#providing-users-with-their-kubeconfig"
  },"173": {
    "doc": "Revoking Tokens",
    "title": "Revoking Tokens",
    "content": "All users with the same level of project-access effectively share the same kubeconfig file. This kubeconfig uses a token-based authentication, and the token is bound to the level of access (read-only/admin access). In case access needs to be removed for such a user, the tokens needs to be revoked, and all users need to download their kubeconfig again. ",
    "url": "/gks/accessmanagement/revokingtoken/",
    
    "relUrl": "/gks/accessmanagement/revokingtoken/"
  },"174": {
    "doc": "Revoking Tokens",
    "title": "Revoke Tokens",
    "content": "If you need to rotate the Kubeconfig login token, proceed as follows: . | Select your cluster. | On the cluster detail page, click the three dots to open the cluster submenu. Then select Revoke Token. | Select the token and press Revoke Token. | Finally, download the new Kubeconfig file as the old one is now invalid. | . ",
    "url": "/gks/accessmanagement/revokingtoken/#revoke-tokens",
    
    "relUrl": "/gks/accessmanagement/revokingtoken/#revoke-tokens"
  },"175": {
    "doc": "Kubernetes Applications",
    "title": "Kubernetes Applications",
    "content": "Running applications on Kubernetes is a complex topic. Here we collected some information which might be of interest with regard to the GKS platform. ",
    "url": "/gks/k8sapplications/",
    
    "relUrl": "/gks/k8sapplications/"
  },"176": {
    "doc": "Kubernetes Applications",
    "title": "Learn More",
    "content": ". | Running Applications in Kubernetes | Managing Service Accounts | ExternalDNS with Designate | Time Zone Management | StorageClass Setup | . ",
    "url": "/gks/k8sapplications/#learn-more",
    
    "relUrl": "/gks/k8sapplications/#learn-more"
  },"177": {
    "doc": "Running Applications in Kubernetes",
    "title": "Running Applications in Kubernetes",
    "content": "You have a running cluster, and now you want to run an application on it. In this example, you run NGINX with a Load Balancer in front of the cluster. ",
    "url": "/gks/k8sapplications/runningapplications/",
    
    "relUrl": "/gks/k8sapplications/runningapplications/"
  },"178": {
    "doc": "Running Applications in Kubernetes",
    "title": "Prerequisites",
    "content": "To successfully finish the steps below, you need the following: . | kubectl latest version | A running Kubernetes cluster created with GKS, with a ready Machine Deployment . | For more information, see Creating a Cluster. | . | A valid kubeconfig for your cluster . | For more information, see Connecting to a Cluster. | . | . ",
    "url": "/gks/k8sapplications/runningapplications/#prerequisites",
    
    "relUrl": "/gks/k8sapplications/runningapplications/#prerequisites"
  },"179": {
    "doc": "Running Applications in Kubernetes",
    "title": "Data Types",
    "content": "Everything running in Kubernetes (inside the cluster) is tracked by the API server. This is how you run and manage applications in Kubernetes. Deployment . The data type deployment ensures that an application runs in Kubernetes and can be updated with a chosen method, for example Rolling. You need to create a deployment for your NGINX example, so Kubernetes knows it needs to download a docker image that contains NGINX to the cluster. Service . A service in Kubernetes is a collection of various containers running in the cluster. The containers are matched to the collection with labels which you apply to deployments. A service can have several types. In our example, you choose the type LoadBalancer to make your service accessible from outside the cluster with a public IP address. ",
    "url": "/gks/k8sapplications/runningapplications/#data-types",
    
    "relUrl": "/gks/k8sapplications/runningapplications/#data-types"
  },"180": {
    "doc": "Running Applications in Kubernetes",
    "title": "Manifests",
    "content": "To run NGINX on Kubernetes, you need an object of type deployment. You can easily create it with kubectl: . kubectl create deployment --dry-run -o yaml --image nginx nginx apiVersion: apps/v1 kind: Deployment metadata: creationTimestamp: null labels: app: nginx name: nginx spec: replicas: 1 selector: matchLabels: app: nginx strategy: {} template: metadata: creationTimestamp: null labels: app: nginx spec: containers: - image: nginx name: nginx resources: {} status: {} . You store this in a file called deployment.yaml. kubectl create deployment --dry-run -o yaml --image nginx nginx &gt; deployment.yaml . Next, you need the service which makes the application publicly accessible. As type you choose LoadBalancer. This automatically creates a fully configured LoadBalancer in OpenStack, and allows you to access the cluster. kubectl create service loadbalancer --dry-run --tcp=80 -o yaml nginx apiVersion: v1 kind: Service metadata: creationTimestamp: null labels: app: nginx name: nginx spec: ports: - name: \"80\" port: 80 protocol: TCP targetPort: 80 selector: app: nginx type: LoadBalancer status: loadBalancer: {} . Again, you save this into a file. This time you name it service.yaml. kubectl create service loadbalancer --dry-run --tcp=80 -o yaml nginx &gt; service.yaml . These two files are the basis for a publicly accessible NGINX running on Kubernetes. There is no connection between the two manifests, except for the label app: nginx, which you can find in the deployment’s metadata and in the service as a selector. ",
    "url": "/gks/k8sapplications/runningapplications/#manifests",
    
    "relUrl": "/gks/k8sapplications/runningapplications/#manifests"
  },"181": {
    "doc": "Running Applications in Kubernetes",
    "title": "Creating the Application",
    "content": "To create the application, you send the two previously created files to the Kubernetes API: . kubectl apply -f deployment.yaml deployment.apps/nginx created kubectl apply -f service.yaml service/nginx created . Now you can inspect the two objects you created before as follows: . kubectl get deployment,service NAME READY UP-TO-DATE AVAILABLE AGE deployment.extensions/nginx 1/1 1 1 55s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/kubernetes NodePort 10.10.10.1 &lt;none&gt; 443:31630/TCP 2d23h service/nginx LoadBalancer 10.10.10.86 &lt;pending&gt; 80:31762/TCP 46s . As shown in the output, a deployment was created and is in the READY state. The service NGINX was created as well, but EXTERNAL-IP is still pending. You need to wait a minute until the LoadBalancer has started up completely. A couple of minutes later, you can run the command again and now you see an external IP address: . kubectl get deployment,svc NAME READY UP-TO-DATE AVAILABLE AGE deployment.extensions/nginx 1/1 1 1 2m8s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/kubernetes NodePort 10.10.10.1 &lt;none&gt; 443:31630/TCP 2d23h service/nginx LoadBalancer 10.10.10.86 185.116.245.169 80:31762/TCP 119s . The external IP 185.116.245.169 from this example is now reachable from the public internet and shows the NGINX default page. ",
    "url": "/gks/k8sapplications/runningapplications/#creating-the-application",
    
    "relUrl": "/gks/k8sapplications/runningapplications/#creating-the-application"
  },"182": {
    "doc": "Running Applications in Kubernetes",
    "title": "Cleanup",
    "content": "It’s easy to clean up an application you do not want to run anymore. kubectl delete -f service.yaml service \"nginx\" deleted kubectl delete -f deployment.yaml deployment.apps \"nginx\" deleted kubectl get deployment,svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/kubernetes NodePort 10.10.10.1 &lt;none&gt; 443:31630/TCP 2d23h . As you can see, both deployment and service are deleted. If you try to open the website via IP again, you will get an error: Our application has stopped running. ",
    "url": "/gks/k8sapplications/runningapplications/#cleanup",
    
    "relUrl": "/gks/k8sapplications/runningapplications/#cleanup"
  },"183": {
    "doc": "Running Applications in Kubernetes",
    "title": "Summary",
    "content": "You learned and achieved the following: . | How to talk to Kubernetes | What is a deployment | How to create a deployment | What is a service | How to create a service | How to delete an application | . Congratulations! You now know all required steps to start and delete applications in Kubernetes. ",
    "url": "/gks/k8sapplications/runningapplications/#summary",
    
    "relUrl": "/gks/k8sapplications/runningapplications/#summary"
  },"184": {
    "doc": "Managing Service Accounts",
    "title": "Managing Service Accounts",
    "content": "Limited access to a Kubernetes cluster can be achieved using Kubernetes service accounts, and the RBAC feature within Kubernetes. To achieve this, you need to create: . | A Kubernetes service account | An access role | Role binding for the Kubernetes service account to use the access role | . All authentication with Kubernetes clusters created in GKS is done with bearer tokens. When you create a new Kubernetes service account a secret will be created for it in the same namespace, which will be automatically removed when you delete the Kubernetes service account. ",
    "url": "/gks/k8sapplications/serviceaccounts/",
    
    "relUrl": "/gks/k8sapplications/serviceaccounts/"
  },"185": {
    "doc": "Managing Service Accounts",
    "title": "Creating a Kubernetes Service Account",
    "content": "To create a Kubernetes service account, run the following command and replace my-serviceaccount with the name you want to use for the Kubernetes service account: . kubectl apply -f - &lt;&lt;EOF apiVersion: v1 kind: ServiceAccount metadata: name: my-serviceaccount namespace: my-namespace EOF . The cluster then automatically creates a new access token with a name like my-serviceaccount-token-##### where the #s are alphanumeric characters. To get a list of the tokens in a specific namespace, run: . kubectl get secrets --namespace=my-namespace . Then you can print the token you want to use with the following command and replace $SECRETNAME with the one that has been created for your service account: . kubectl get secret $SECRETNAME -o jsonpath='{.data.token}' --namespace=my-namespace . Provide the token that was printed with the name of the service account to a developer or third party to allow them to interact with the cluster. At this point the service account can authenticate with the Kubernetes cluster, but is unable to use it. You need to create a role and a role binding to provide permissions to the service account. ",
    "url": "/gks/k8sapplications/serviceaccounts/#creating-a-kubernetes-service-account",
    
    "relUrl": "/gks/k8sapplications/serviceaccounts/#creating-a-kubernetes-service-account"
  },"186": {
    "doc": "Managing Service Accounts",
    "title": "Creating Authorization Permissions",
    "content": "Kubernetes has two ways of granting access to service accounts, roles, and cluster roles. Since cluster roles provide access to all namespaces, it is recommended not to use them unless you have to. We provide examples on how to use the namespace-restricted roles here. Roles explicitly whitelist permissions for users (both human and Kubernetes service accounts). When users have multiple roles they can do anything that is granted by any of the roles. To create a role, which allows a user to read information about pods in the namespace my-namespace, use the following command: . kubectl apply -f - &lt;&lt;EOF apiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata: namespace: my-namespace name: pod-reader rules: - apiGroups: [\"\"] # \"\" indicates the core API group resources: [\"pods\"] verbs: [\"get\", \"watch\", \"list\"] EOF . To grant the service account you created earlier to use this role, use the following command to create a role binding: . kubectl create rolebinding read-pods \\ --role=pod-reader \\ --serviceaccount=my-namespace:my-serviceaccount \\ --namespace=my-namespace . For a full list of resources run: . kubectl api-resources . For most resources, the available verbs are: . | get | list | watch | create | edit | update | delete | exec | . More Information . More details are available in the official Kubernetes documentation on controlling access and using roles and role bindings in RBAC. ",
    "url": "/gks/k8sapplications/serviceaccounts/#creating-authorization-permissions",
    
    "relUrl": "/gks/k8sapplications/serviceaccounts/#creating-authorization-permissions"
  },"187": {
    "doc": "Managing Service Accounts",
    "title": "Summary",
    "content": "In this section you learned how to use the Kubernetes CLI to: . | Create a Kubernetes service account | Retrieve the automatically generated bearer token for a service account | Create a new role in Kubernetes RBAC | Create a role binding to allow the Kubernetes service account to use that role | . ",
    "url": "/gks/k8sapplications/serviceaccounts/#summary",
    
    "relUrl": "/gks/k8sapplications/serviceaccounts/#summary"
  },"188": {
    "doc": "ExternalDNS with Designate",
    "title": "ExternalDNS with Designate",
    "content": "To reduce manual effort and automate the configuration of DNS zones, you may want to use ExternalDNS. In summary, ExternalDNS allows you to control DNS records dynamically with Kubernetes resources in a DNS provider-agnostic way. ExternalDNS is not a DNS server by itself, but merely configures other DNS providers (for example, OpenStack Designate, Amazon Route53, Google Cloud DNS, and so on.) . ",
    "url": "/gks/k8sapplications/externaldnsanddesignate/",
    
    "relUrl": "/gks/k8sapplications/externaldnsanddesignate/"
  },"189": {
    "doc": "ExternalDNS with Designate",
    "title": "Prerequisites",
    "content": "To successfully complete the following steps, you need the following: . | kubectl latest version | A running Kubernetes cluster created with GKS, with a ready Machine Deployment . | For more information, see Creating a Cluster. | . | A valid kubeconfig for your cluster . | For more information, see Connecting to a Cluster. | . | Installed OpenStack CLI tools | OpenStack API access | A valid domain | . ",
    "url": "/gks/k8sapplications/externaldnsanddesignate/#prerequisites",
    
    "relUrl": "/gks/k8sapplications/externaldnsanddesignate/#prerequisites"
  },"190": {
    "doc": "ExternalDNS with Designate",
    "title": "Configuring Your Domain to Use Designate",
    "content": "Delegate your domains from your DNS provider to the following GEC DNS name servers so that Designate can control the DNS resources of your domain. dns1.ddns.innovo.cloud dns2.ddns.innovo.cloud . ",
    "url": "/gks/k8sapplications/externaldnsanddesignate/#configuring-your-domain-to-use-designate",
    
    "relUrl": "/gks/k8sapplications/externaldnsanddesignate/#configuring-your-domain-to-use-designate"
  },"191": {
    "doc": "ExternalDNS with Designate",
    "title": "Creating your DNS Zone",
    "content": "Before you use ExternalDNS, you need to add your DNS zones to your DNS provider, in this case, Designate DNS. In our example we use the test domain name foobar.cloud. It is important to create the zones before starting to control the DNS resources with Kubernetes. Note: You must include the final . at the end of the zone/domain to be created. $ openstack zone create --email webmaster@foobar.cloud foobar.cloud. +----------------+--------------------------------------+ | Field | Value | +----------------+--------------------------------------+ | action | CREATE | attributes | | created_at | 2018-08-15T06:45:24.000000 | description | None | email | webmaster@foobar.cloud | id | 036ae6e6-6318-47e1-920f-be518d845fb5 | masters | | name | foobar.cloud. | pool_id | bb031d0d-b8ca-455a-8963-50ec70fe57cf | project_id | 2b62bc8ff48445f394d0318dbd058967 | serial | 1534315524 | status | PENDING | transferred_at | None | ttl | 3600 | type | PRIMARY | updated_at | None | version | 1 | +----------------+--------------------------------------+ . Next, make sure that the zone was created successfully. $ openstack zone list +--------------------------------------+-----------------------+---------+------------+--------+--------+ | id | name | type | serial | status | action | +--------------------------------------+-----------------------+---------+------------+--------+--------+ | 036ae6e6-6318-47e1-920f-be518d845fb5 | foobar.cloud. | PRIMARY | 1534315524 | ACTIVE | NONE | +--------------------------------------+-----------------------+---------+------------+--------+--------+ $ openstack zone show foobar.cloud. +----------------+--------------------------------------+ | Field | Value | +----------------+--------------------------------------+ | action | NONE | attributes | | created_at | 2018-08-15T06:45:24.000000 | description | None | email | webmaster@foobar.cloud | id | 036ae6e6-6318-47e1-920f-be518d845fb5 | masters | | name | foobar.cloud. | pool_id | bb031d0d-b8ca-455a-8963-50ec70fe57cf | project_id | 2b62bc8ff48445f394d0318dbd058967 | serial | 1534315524 | status | ACTIVE | transferred_at | None | ttl | 3600 | type | PRIMARY | updated_at | 2018-08-15T06:45:30.000000 | version | 2 | +----------------+--------------------------------------+ . ",
    "url": "/gks/k8sapplications/externaldnsanddesignate/#creating-your-dns-zone",
    
    "relUrl": "/gks/k8sapplications/externaldnsanddesignate/#creating-your-dns-zone"
  },"192": {
    "doc": "ExternalDNS with Designate",
    "title": "Installing ExternalDNS with Helm",
    "content": "Install ExternalDNS to your cluster. In our example we will use Helm as follows: . | Install Helm | $ helm repo add stable https://kubernetes-charts.storage.googleapis.com/ | $ helm repo update | Create this values.yaml file and configure it. For more information, see values-external-dns. | . ## K8s resources type to be observed for new DNS entries by ExternalDNS ## sources: - service - ingress ## DNS provider where the DNS records will be created. Available providers are: ## - aws, azure, cloudflare, coredns, designate, digitalocoean, google, infoblox, rfc2136, transip ## provider: designate ## Adjust the interval for DNS updates ## interval: \"1m\" ## Registry Type. Available types are: txt, noop ## ref: https://github.com/kubernetes-incubator/external-dns/blob/master/docs/proposal/registry.md ## registry: \"txt\" ## TXT Registry Identifier, a name that identifies this instance of External-DNS ## This value should not change, while the cluster exists ## txtOwnerId: \"external-dns\" ## Modify how DNS records are sychronized between sources and providers (options: sync, upsert-only ) ## policy: sync ## Configure resource requests and limits ## ref: http://kubernetes.io/docs/user-guide/compute-resources/ ## resources: limits: memory: 50Mi cpu: 10m requests: memory: 50Mi cpu: 10m ## Configure your OS Access ## extraEnv: - name: OS_AUTH_URL value: https://identity.optimist.gec.io/v3 - name: OS_REGION_NAME value: fra - name: OS_USERNAME value: \"%YOUR_OPENSTACK_USERNAME%\" - name: OS_PASSWORD value: \"%YOUR_OPENSTACK_PASSWORD%\" - name: OS_PROJECT_NAME value: \"%YOUR_OPENSTACK_PROJECT_NAME%\" - name: OS_USER_DOMAIN_NAME value: Default . | $ kubectl create namespace external-dns | $ helm upgrade --install external-dns -f values.yaml stable/external-dns --namespace=external-dns | . ",
    "url": "/gks/k8sapplications/externaldnsanddesignate/#installing-externaldns-with-helm",
    
    "relUrl": "/gks/k8sapplications/externaldnsanddesignate/#installing-externaldns-with-helm"
  },"193": {
    "doc": "ExternalDNS with Designate",
    "title": "Running Your Deployment",
    "content": "To test the fully-qualified domain name (FQDN) of the domain, create this example deployment as nginx.yaml: . apiVersion: apps/v1 kind: Deployment metadata: name: nginx spec: selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - image: nginx name: nginx ports: - containerPort: 80 --- apiVersion: v1 kind: Service metadata: name: nginx annotations: external-dns.alpha.kubernetes.io/hostname: nginx.foobar.cloud external-dns.alpha.kubernetes.io/ttl: \"60\" spec: selector: app: nginx type: LoadBalancer ports: - protocol: TCP port: 80 targetPort: 80 . Apply the configuration to your cluster with: . kubectl apply -f nginx.yaml . Confirming the Results . Check your DNS resources in OpenStack. You should see a similar list with the corresponding DNS records. openstack recordset list foobar.cloud. Wait a few minutes, and then test the availability over the internet. For example, browse to your website. You should see the following in your browser. ",
    "url": "/gks/k8sapplications/externaldnsanddesignate/#running-your-deployment",
    
    "relUrl": "/gks/k8sapplications/externaldnsanddesignate/#running-your-deployment"
  },"194": {
    "doc": "ExternalDNS with Designate",
    "title": "Summary",
    "content": "By completing these steps, you’ve accomplished the following: . | Learned what ExternalDNS is and how to install it | Configured ExternalDNS with Helm to use Designate | Created a deployment with NGINX and tested the connectivity | . Congratulations! You now know all required steps to control your DNS resources in Kubernetes. ",
    "url": "/gks/k8sapplications/externaldnsanddesignate/#summary",
    
    "relUrl": "/gks/k8sapplications/externaldnsanddesignate/#summary"
  },"195": {
    "doc": "Time Zone Management",
    "title": "Time Zone Management",
    "content": "In case your application requires time zone switching you can follow this guide. ",
    "url": "/gks/k8sapplications/timezones/",
    
    "relUrl": "/gks/k8sapplications/timezones/"
  },"196": {
    "doc": "Time Zone Management",
    "title": "Prerequisites",
    "content": "To successfully finish this guide, you need the following items. | kubectl latest version | A running Kubernetes Cluster, created with GKS, with a ready Machine Deployment. | See also Creating a Cluster | . | A valid kubeconfig for your cluster . | See also Connecting to a Cluster | . | . ",
    "url": "/gks/k8sapplications/timezones/#prerequisites",
    
    "relUrl": "/gks/k8sapplications/timezones/#prerequisites"
  },"197": {
    "doc": "Time Zone Management",
    "title": "Time zones in Kubernetes environment",
    "content": "Default behaviour in Kubernetes cluster . Kubernetes clusters inherit the time zone configuration from worker node, so that the default time zone in the Kubernetes cluster is the same as the one in your worker node - it’s controlled by the kernel. Pods &amp; Containers . While it is not trivial to change the time zone at the cluster level, there is an easy way to achieve this at pod and container level. To achieve this goal, you can follow the scenario below: . Scenario . Start a test container . Start a minimal container e.g. a one with busybox. For example: . apiVersion: v1 kind: Pod metadata: name: busybox-sleep spec: containers: - name: busybox image: busybox args: - sleep - \"100000\" . Save this definition file as busybox.yaml and apply it to the cluster: . kubectl apply -f busybox.yaml . Wait a moment until the pod deployment has been finished and query the time from the container: . kubectl exec busybox-sleep -it -- date . You would get a similar output, in our example the time zone is UTC: . Wed Feb 26 10:57:53 UTC 2020 . Change the time zone in the test container . To change the time zone at pod or container level, do the following: . Share your desired ‘zoneinfo’ file with the pod or container by mounting it from the worker node into the container. The following example shows the change to Berlin local time: . apiVersion: v1 kind: Pod metadata: name: busybox-sleep-2 spec: containers: - name: busybox image: busybox args: - sleep - \"100000\" volumeMounts: - name: timezone-config mountPath: /etc/localtime volumes: - name: timezone-config hostPath: path: /usr/share/zoneinfo/Europe/Berlin . Basically, it’s the same definition, file but with a volume named timezone-config which set the container’s localtime to your time zone. Save this definition file as busybox-2.yaml and apply it to the cluster: . kubectl apply -f busybox-2.yaml . After waiting a moment until the deployment has been finished query the time from the container: . kubectl exec busybox-sleep-2 -it -- date . You should get an output that shows the Berlin local time: . Wed Feb 26 11:01:04 CET 2020 . ",
    "url": "/gks/k8sapplications/timezones/#time-zones-in-kubernetes-environment",
    
    "relUrl": "/gks/k8sapplications/timezones/#time-zones-in-kubernetes-environment"
  },"198": {
    "doc": "Time Zone Management",
    "title": "Summary",
    "content": "We learned the following: . | How to query time from inside a container | How to mount zoneinfo / localtime to the container, so that the time zone changes. | . Congratulations! You now know all required steps to change time zone in Kubernetes pods and containers. ",
    "url": "/gks/k8sapplications/timezones/#summary",
    
    "relUrl": "/gks/k8sapplications/timezones/#summary"
  },"199": {
    "doc": "StorageClass Setup",
    "title": "StorageClass Setup",
    "content": "We provide one default storage class per Cluster. Caution: This is managed by GKS and can be overwritten at any time. Please create a separate storage class for your changes. kubectl get storageclasses.storage.k8s.io NAME PROVISIONER AGE standard (default) kubernetes.io/cinder 268d . kubectl get storageclasses.storage.k8s.io NAME PROVISIONER AGE cinder-csi (default) cinder.csi.openstack.org 6h45m . The provisioner is version and creation time dependent. | kubernetes.io/cinder all Kubernetes Cluster prior 1.16 and created before 29.10 . | cinder.csi.openstack.org all Kubernetes Cluster 1.16+ and created after 19.10 . | . ",
    "url": "/gks/k8sapplications/storageclasses/",
    
    "relUrl": "/gks/k8sapplications/storageclasses/"
  },"200": {
    "doc": "StorageClass Setup",
    "title": "Openstack Volume Types",
    "content": "The Openstack volume types sorted by maximum possible IOPS: . | low-iops | default &lt;- used in the default class | high-iops | . Adding Your Own Classes . If you need use one of the other types, you can add your own definitions. Example: . apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: my-high-iops-class provisioner: cinder.csi.openstack.org parameters: type: high-iops . Apply with kubectl apply -f storage-class.yaml. | name: Choose a unique one, as we don’t want to interfere with the default names. | provisioner: Use the one of your cluster. You can always have a look in the default class to verify the right provider. | type: Use one of the official provided types from the Optimist platform (at the time of writing low-iops and high-iops). | . To use the new storage class you need to change your volumes definitions and add the new StorageClass name. ",
    "url": "/gks/k8sapplications/storageclasses/#openstack-volume-types",
    
    "relUrl": "/gks/k8sapplications/storageclasses/#openstack-volume-types"
  },"201": {
    "doc": "Add-On",
    "title": "Add-On",
    "content": "Inside of GKS you can run a list of curated applications as add-ons. In this section we show how to install and work with those add-ons on the GKS platform. ",
    "url": "/gks/addons/",
    
    "relUrl": "/gks/addons/"
  },"202": {
    "doc": "Add-On",
    "title": "Learn More",
    "content": ". | Cluster Autoscaler | . ",
    "url": "/gks/addons/#learn-more",
    
    "relUrl": "/gks/addons/#learn-more"
  },"203": {
    "doc": "Cluster Autoscaler",
    "title": "Cluster Autoscaler",
    "content": " ",
    "url": "/gks/addons/cluster-autoscaler/",
    
    "relUrl": "/gks/addons/cluster-autoscaler/"
  },"204": {
    "doc": "Cluster Autoscaler",
    "title": "What is a Kubernetes Cluster Autoscaler?",
    "content": "Kubernetes Cluster Autoscaler is a tool that automatically adjusts the number of worker nodes in a cluster up or down depending on the consumption. The Autoscaler, for example, scales up a cluster by increasing the amount of nodes automatically when there are not enough node resources for cluster workload scheduling. It scales down when the node resources have continuously stayed idle or more than enough node resources were available for cluster workload scheduling. In a nutshell, it is a component that automatically adjusts the size of a Kubernetes cluster so that all pods have a place to run and there are no unneeded nodes. ",
    "url": "/gks/addons/cluster-autoscaler/#what-is-a-kubernetes-cluster-autoscaler",
    
    "relUrl": "/gks/addons/cluster-autoscaler/#what-is-a-kubernetes-cluster-autoscaler"
  },"205": {
    "doc": "Cluster Autoscaler",
    "title": "Cluster Autoscaler Usage",
    "content": "The Kubernetes Autoscaler in the GKS cluster automatically scaled up/down when one of the following conditions is fulfilled: . | Some pods failed to run in the cluster due to insufficient resources | There are nodes in the cluster that have been underutilized for an extended period (10 minutes by default) and can place their Pods on other existing nodes | . ",
    "url": "/gks/addons/cluster-autoscaler/#cluster-autoscaler-usage",
    
    "relUrl": "/gks/addons/cluster-autoscaler/#cluster-autoscaler-usage"
  },"206": {
    "doc": "Cluster Autoscaler",
    "title": "Requirements",
    "content": "Using a Kubernetes cluster Autoscaler in the GKS cluster must meet specific minimum requirement: . | Kubernetes cluster running Kubernetes v1.18 or newer is required | . ",
    "url": "/gks/addons/cluster-autoscaler/#requirements",
    
    "relUrl": "/gks/addons/cluster-autoscaler/#requirements"
  },"207": {
    "doc": "Cluster Autoscaler",
    "title": "Installing Kubernetes Autoscaler on GKS Clusters",
    "content": "You can install Kubernetes Autoscaler on a running GKS cluster using the GKS addon mechanism, which is already built into the GKS cluster dashboard. Step 1 . Create a GKS cluster by selecting your project on the dashboard and click on “Create Cluster”. More details is available here. Step 2 . When the cluster is ready, check the pods in the kube-system namespace to know if any Autoscaler is running. $ kubectl get deployment -n kube-system NAME READY UP-TO-DATE AVAILABLE AGE coredns 2/2 2 2 1d flatcar-linux-update-operator 1/1 1 1 1d openvpn-client 1/1 1 1 1d . As shown above, the Autoscaler is not part of the running Kubernetes components within the namespace. Step 3 . Add the Autoscaler to the cluster under the addon section on the dashboard by clicking on Addons and then Install Addon. Select cluster-autoscaler. Select Install. Step 4 . Go to the cluster and check the pods in the kube-system namespace using the kubectl command. $ kubectl get deployment -n kube-system NAME READY UP-TO-DATE AVAILABLE AGE cluster-autoscaler 1/1 1 1 6m27s coredns 2/2 2 2 1d flatcar-linux-update-operator 1/1 1 1 1d openvpn-client 1/1 1 1 1d . As shown above, the Autoscaler has been provisioned and is running. ",
    "url": "/gks/addons/cluster-autoscaler/#installing-kubernetes-autoscaler-on-gks-clusters",
    
    "relUrl": "/gks/addons/cluster-autoscaler/#installing-kubernetes-autoscaler-on-gks-clusters"
  },"208": {
    "doc": "Cluster Autoscaler",
    "title": "Annotating Machine Deployments for Autoscaling",
    "content": "The Cluster Autoscaler only considers Machine Deployments with valid annotations. The annotations are used to control the minimum and the maximum number of replicas per Machine Deployment. You don’t need to apply those annotations to all Machine Deployment objects, but only on Machine Deployments that Cluster Autoscaler should consider. cluster.k8s.io/cluster-api-autoscaler-node-group-min-size - the minimum number of replicas (must be greater than zero) cluster.k8s.io/cluster-api-autoscaler-node-group-max-size - the maximum number of replicas . You can apply the annotations to Machine Deployments once the cluster is provisioned and the Machine Deployments are created and running by following the steps below. Step 1 . Run the following kubectl command to check the available Machine Deployments: . $ kubectl get machinedeployments -n kube-system NAME AGE DELETED REPLICAS AVAILABLEREPLICAS PROVIDER OS VERSION epic-goldwasser-worker-289mgt 1d 2 2 openstack flatcar 1.21.5 . Step 2 . The annotation command is used with one of the Machine Deployments above to annotate the desired Machine Deployments. In this case the test-cluster-worker-v5drmq is annotated and the minimum and maximum is set. Minimum annotation: . $ kubectl annotate machinedeployment -n kube-system epic-goldwasser-worker-289mgt cluster.k8s.io/cluster-api-autoscaler-node-group-min-size=\"1\" machinedeployment.cluster.k8s.io/epic-goldwasser-worker-289mgt annotated . Maximum annotation: . $ kubectl annotate machinedeployment -n kube-system epic-goldwasser-worker-289mgt cluster.k8s.io/cluster-api-autoscaler-node-group-max-size=\"5\" machinedeployment.cluster.k8s.io/epic-goldwasser-worker-289mgt annotated . Step 3 . Check the Machine Deployment description: . $ kubectl describe machinedeployments -n kube-system epic-goldwasser-worker-289mgt Name: epic-goldwasser-worker-289mgt Namespace: kube-system Labels: &lt;none&gt; Annotations: cluster.k8s.io/cluster-api-autoscaler-node-group-max-size: 5 cluster.k8s.io/cluster-api-autoscaler-node-group-min-size: 1 machinedeployment.clusters.k8s.io/revision: 1 API Version: cluster.k8s.io/v1alpha1 Kind: MachineDeployment Metadata: Creation Timestamp: 2021-10-04T09:44:48Z Finalizers: foregroundDeletion Generation: 1 Managed Fields: API Version: cluster.k8s.io/v1alpha1 Fields Type: FieldsV1 [...] . As shown above, the Machine Deployment has been annotated with a minimum of 1 and a maximum of 5. Therefore, the Autoscaler considers only the annotated Machine Deployment on the cluster. ",
    "url": "/gks/addons/cluster-autoscaler/#annotating-machine-deployments-for-autoscaling",
    
    "relUrl": "/gks/addons/cluster-autoscaler/#annotating-machine-deployments-for-autoscaling"
  },"209": {
    "doc": "Cluster Autoscaler",
    "title": "Deleting the Autoscaler",
    "content": "To delete the Autoscaler, click on the three dots in front of the Cluster Autoscaler in the Addons section of the cluster dashboard and select Delete. Once it has been deleted, you can check the cluster to ensure that the Autoscaler has been deleted with the kubectl get deployment -n kube-system command. ",
    "url": "/gks/addons/cluster-autoscaler/#deleting-the-autoscaler",
    
    "relUrl": "/gks/addons/cluster-autoscaler/#deleting-the-autoscaler"
  },"210": {
    "doc": "Cluster Autoscaler",
    "title": "Summary",
    "content": "You have successfully deployed a Kubernetes Autoscaler on a GKS cluster and annotated the desired Machine Deployment, which Autoscaler should consider. Check the Learn More section below for more resources on Kubernetes Autoscaler and how to provision a GKS cluster. ",
    "url": "/gks/addons/cluster-autoscaler/#summary",
    
    "relUrl": "/gks/addons/cluster-autoscaler/#summary"
  },"211": {
    "doc": "Cluster Autoscaler",
    "title": "Learn More",
    "content": ". | Read more on Kubernetes Autoscaler here. | . ",
    "url": "/gks/addons/cluster-autoscaler/#learn-more",
    
    "relUrl": "/gks/addons/cluster-autoscaler/#learn-more"
  },"212": {
    "doc": "Product Overview",
    "title": "Product Overview",
    "content": "The ONCITE Open Edition provides an easy-to-use VM and a container platform based on Open Source. It is scalable from the size of an Edge to the size of a data center and offers full data sovereignty and real-time capabilities. ",
    "url": "/edge/productoverview/",
    
    "relUrl": "/edge/productoverview/"
  },"213": {
    "doc": "Product Overview",
    "title": "Architecture",
    "content": "For providing Virtual Machines on the Edge we use Openstack. The provided Storage is handled by CEPH. To create and manage the VMs the Customer can use the OperationsCenter, which is connected via LDAP for SingleSignOn. Administrators can use the Openstack Horizon, which is not connected to LDAP. ",
    "url": "/edge/productoverview/#architecture",
    
    "relUrl": "/edge/productoverview/#architecture"
  },"214": {
    "doc": "Operations Center",
    "title": "Operations Center",
    "content": " ",
    "url": "/edge/operationscenter/",
    
    "relUrl": "/edge/operationscenter/"
  },"215": {
    "doc": "VPN as a Service",
    "title": "VPN as a Service",
    "content": " ",
    "url": "/edge/operationscenter/vpnaas/",
    
    "relUrl": "/edge/operationscenter/vpnaas/"
  },"216": {
    "doc": "VPN as a Service",
    "title": "Overview",
    "content": "Here you see how the customer OpenStack projects and the Operations Center communicate via the OpenStack Project VPN Gateway. The OpenStack Project VPN Gateway has one public IP address. It autenticates and leads the traffic by the different ports to the respective VPN server. Usually, each project has one VPN server. In this configuration, each VPN Gateway has one public IP adress that is valid for all VPN servers. ",
    "url": "/edge/operationscenter/vpnaas/#overview",
    
    "relUrl": "/edge/operationscenter/vpnaas/#overview"
  },"217": {
    "doc": "VPN as a Service",
    "title": "Purpose",
    "content": "GEC provides a VPNaaS solution which allows the customer to integrate their applications and systems with project partner systems. An external partner is either a person accessing the project with his or her computer, or a communication system. ",
    "url": "/edge/operationscenter/vpnaas/#purpose",
    
    "relUrl": "/edge/operationscenter/vpnaas/#purpose"
  },"218": {
    "doc": "VPN as a Service",
    "title": "Requirements",
    "content": "The VPNaaS solution must meet the following requirement to the system and the customer: . | The Gateway needs a publicly reachable floating IP. | . ",
    "url": "/edge/operationscenter/vpnaas/#requirements",
    
    "relUrl": "/edge/operationscenter/vpnaas/#requirements"
  },"219": {
    "doc": "VPN as a Service",
    "title": "Limitations",
    "content": "The VPNaaS solution has the following limitations: . | The amount of VPN server counts is limited by the preconfigured port range in the gateway. | The amount of users in a network is limited to 250. | The gateway and server configuration cannot be changed after creation and have to be recreated. | After recreation, all OVPN configurations are invalid. | . ",
    "url": "/edge/operationscenter/vpnaas/#limitations",
    
    "relUrl": "/edge/operationscenter/vpnaas/#limitations"
  },"220": {
    "doc": "VPN as a Service",
    "title": "Components and Communication Flow",
    "content": "Here you see the communication flow between the Operations Center and the Customer VPN Servers. The customer sends a request, and the VPN gateway authenticates and distributes the requests to the respective VPN server. VPN Gateway . The VPN gateway is the GEC central management hub. It is used to manage VPN servers and allows external connections. Since only one public IP address is available, IPtable rules are used to make sure that the connections arrive at the correct VPN server. This requires a Wide Area Network (WAN) network and a VPN Transfer Network connected to the VPN Gateway. The VPN Gateway only manages vpn servers and proxy connections to and from the VPN servers, so traffic only ends up where it is needed. VPN Server . The VPN Server uses OpenVPN for the VPN connection to the customer network. That network connection is in a different Virtual Routing and Forwarding (VRF) to provide additional isolation without opening any other ports. Outside connections are routed via the VPN Gateway. The API Server for the VPN Server manages the OpenVPN Server that is running on the machine. ",
    "url": "/edge/operationscenter/vpnaas/#components-and-communication-flow",
    
    "relUrl": "/edge/operationscenter/vpnaas/#components-and-communication-flow"
  },"221": {
    "doc": "VPN as a Service",
    "title": "Security",
    "content": "WAN . | Only ports that are required for the existing VPN Servers are open | SSH is not used | Virtual Routing and Forwarding (VRF) separation is available | Security Groups only allow the VPN Server port ranges | FW rules are in place and adjusted as needed | . VPNaaS . | Separate VRF to WAN | Firewall rules are in place and adjusted as needed | SSH and API are available | SSH-Key is used | . ",
    "url": "/edge/operationscenter/vpnaas/#security",
    
    "relUrl": "/edge/operationscenter/vpnaas/#security"
  },"222": {
    "doc": "VPN as a Service",
    "title": "Server and User Management",
    "content": "Server Status . | Click on the VPN resource. | Under Properties, check the status. To be able to edit the configuration, the status has to be active. | . Add a User . Note: The project owner is automatically added for remote access and cannot be removed. Prerequisites: You can only select users who have previously been added to the project. | Go to the VPN configuration. | Select the user you want to add as VPN User. | Click Save. The user is added immediately. | . Get the OVPN Configuration and Passphrase . Note: You can only download the OVPN configuration and reveal a passphrase for yourself or as project owner for external users. You cannot download the OVPN configuration or reveal passphrases for other users. | Go to the VPN configuration. | Click on the world icon link and choose between Download and Passphrase. | . Delete a User . | Click on the Delete icon next to the user you want to delete. | Click Save. | . ",
    "url": "/edge/operationscenter/vpnaas/#server-and-user-management",
    
    "relUrl": "/edge/operationscenter/vpnaas/#server-and-user-management"
  },"223": {
    "doc": "Openstack",
    "title": "Openstack",
    "content": "OpenStack is the software used for the Infrastructure as a Service layer. All VMs are created in OpenStack, which is directly connected to CEPH for providing storage. ",
    "url": "/edge/openstack/",
    
    "relUrl": "/edge/openstack/"
  },"224": {
    "doc": "Openstack",
    "title": "Openstack - Horizon",
    "content": "The Dashboard for Openstack is called Horizon. Horizon Login . All Admins gain login credentials to the Openstack Dashboard after Handover . Horizon Instances . In the instance overview, you can view the VMs running in the selected project. Horizon Network . In the “Network” tab, you can see the networks in the current project, including the provider networks. The provider networks are the networks that connect the VMs to the customer network. Furthermore, you can view the network topology, showing how the VMs are connected and how the networks are interconnected. Horizon New Network . To create a new network, click on “Create Network” in the “Network” tab. Then, you can choose the name. And you need to create a subnet. To work with the network or connect the network externally, a router must be created. The router must then be connected to a network. This is done by creating an interface on the router, which then connects the router to a network. To connect two networks, the router needs an interface in both networks. ",
    "url": "/edge/openstack/#openstack---horizon",
    
    "relUrl": "/edge/openstack/#openstack---horizon"
  },"225": {
    "doc": "Create VM",
    "title": "Creation of a VM",
    "content": "To create a new Virtual Machine in OpenStack, you need to log in to the OpenStack Dashboard. This access is only available to administrators. After logging in, new VMs can be created in the “Instances” tab. Click on “Launch Instance” to be guided through the creation of the VM using a wizard. ",
    "url": "/edge/openstack/create_vm/#creation-of-a-vm",
    
    "relUrl": "/edge/openstack/create_vm/#creation-of-a-vm"
  },"226": {
    "doc": "Create VM",
    "title": "Details",
    "content": "Here, the name for the VM can be chosen. Additionally, the number of VMs can be changed; the default is one. ",
    "url": "/edge/openstack/create_vm/#details",
    
    "relUrl": "/edge/openstack/create_vm/#details"
  },"227": {
    "doc": "Create VM",
    "title": "Source",
    "content": "Here, you can choose whether the instance should be created from an existing volume. Or whether the VM should be created from an image. Furthermore, you can set whether a new volume should be created for the VM and whether this volume should be deleted along with the VM. ",
    "url": "/edge/openstack/create_vm/#source",
    
    "relUrl": "/edge/openstack/create_vm/#source"
  },"228": {
    "doc": "Create VM",
    "title": "Flavor",
    "content": "Here, you can specify the size of the VM and whether it should be equipped with a graphics card or TSN card if necessary. ",
    "url": "/edge/openstack/create_vm/#flavor",
    
    "relUrl": "/edge/openstack/create_vm/#flavor"
  },"229": {
    "doc": "Create VM",
    "title": "Network",
    "content": "Here, you can select the network to which the VM will be connected during creation. ",
    "url": "/edge/openstack/create_vm/#network",
    
    "relUrl": "/edge/openstack/create_vm/#network"
  },"230": {
    "doc": "Create VM",
    "title": "Key Pair",
    "content": "Here, the SSH key can be selected for connecting via SSH. Additionally, new SSH keys can be imported here. ",
    "url": "/edge/openstack/create_vm/#key-pair",
    
    "relUrl": "/edge/openstack/create_vm/#key-pair"
  },"231": {
    "doc": "Create VM",
    "title": "Create VM",
    "content": " ",
    "url": "/edge/openstack/create_vm/",
    
    "relUrl": "/edge/openstack/create_vm/"
  },"232": {
    "doc": "FAQ",
    "title": "FAQ",
    "content": " ",
    "url": "/edge/faq/",
    
    "relUrl": "/edge/faq/"
  },"233": {
    "doc": "FAQ",
    "title": "Openstack",
    "content": "How are users added in Openstack? . Currently, users are added only upon request via Jira Helpdesk Ticket since every OpenStack user is also an admin. How can VMs from other users/projects be deleted? . In the Horizon dashboard, you can access all VMs via the admin section and delete them. How can the disk size of an existing VM be increased? . This is not possible in the current version of OpenStack. What subnets are available? . There are always the following subnets: . | public | shared | private/internal | . Upon request, a fourth network can be created: Lab. This is intended for direct connection without the Barracuda Firewall. How are subnet separations managed? . The networks are purely virtualized via OpenVSwitch. Compliance with zones is managed in the OperationsCenter via Security Rules and Security Groups, which are IPFilter rules in OpenStack Neutron. ",
    "url": "/edge/faq/#openstack",
    
    "relUrl": "/edge/faq/#openstack"
  },"234": {
    "doc": "FAQ",
    "title": "Operations Center",
    "content": "How are users added in the OperationsCenter? . Access to the Operations Center is realized via SSO. After users have logged in, they can be added to a project by project owners. Groups vs Roles: Difference between Roles and Groups . The Operations Center currently has 4 user groups: . | External Users | Users (SSO) | Admins (SSO) | Global Admins (SSO) | . Can admin rights be restricted to one project only? How? . Users automatically have full access to projects they create. Other project owners can pass on the right. How can I change the owner of a project? . Any project owner and admin with access to the project can change the permissions. What happens when a user leaves the company? . Through SSO integration, the user automatically loses access. It may be necessary to reassign their projects. See the previous point. VPN accesses must be deleted manually. Can specifications be made on how a project should be named? . During creation. There is no provision for changes. How is a VPN created? . Global administrators can activate the function for individual projects. Project owners can further enable users added to the project. How are project shares created? . On the home page of each project via the member list. How are flavors managed? . Flavors are managed in OpenStack. Each “public” flavor is available for configuration in Operations Center. If a “public” flavor should no longer be selectable in Operations Center when creating a VM, the metadata of the flavor can be changed accordingly. To do this, select the flavor in Horizon, click on “Update Metadata”, enter visibility in the “Custom” field and add it with “+”. The field now appears under “Existing Metadata”. Enter the value “false” here and save the change. The flavor can now no longer be selected when creating a new VM. Existing VMs that use this flavor are not affected and can still be managed in Operations Center. Flavors cannot be restricted to individual users. Are flavors allowed to be deleted? . Yes, but only if it’s ensured that no VM is using the flavor. Deletion is done via Horizon. If a flavor is deleted that is still in use, each project that has a VM using that flavor cannot be managed in Operations Center any more. Where can logs be viewed if a VM has been created but doesn’t appear? . It’s possible to observe and repeat the VM startup process via Horizon. How can VMs be deleted from other users/projects? . Only users and global administrators with project access can delete VMs. Project list in Operations Center vs Horizon . Projects from the Operations Center are also visible in Horizon. Projects created in Horizon are not visible in the Operations Center. How can VMs be started? . VMs can be started via the “Start” button in the Operations Center. How can the disk size of an existing VM be increased in Operations Center? . It’s not possible. How can external/internal users be deleted? . Internal users are authenticated via SSO. If they are disabled there, they are automatically blocked. External users can be deleted by administrators in the overview of external users in the Operations Center. How can the router be edited and configured? . The Operations Center automatically configures routers according to project specifications. How can routes be edited in the Operations Center? . Additional routes cannot be set up in the Operations Center. How can an SSH key be added to an existing VM? . It’s not possible to add SSH keys via the Operations Center after VM creation. They must be added directly to the VM. Volume Sizes . The Operations Center allows the creation of volumes in different sizes. The list can be expanded upon request. Can the Operations Center be configured via API? . No, that is not intended. How can I log in to a Windows VM? The user ‘operation’ does not work with the entered password. Instead of the ‘operation’ user, for Windows VMs, the user of the selected image must be used. Often Administrator. ",
    "url": "/edge/faq/#operations-center",
    
    "relUrl": "/edge/faq/#operations-center"
  },"235": {
    "doc": "FAQ",
    "title": "General",
    "content": "Zones . Security zones are structured in hierarchically descending order, starting with the Private/Internal Zone, followed by the Shared Zone and Public Zone. Based on this, virtual machines (VMs) can communicate hierarchically descending by assigning floating IPs. If VMs are located in the Private/Internal Zones, communication with floating IPs in the Shared and Public Zones is possible. However, the reverse, from the Shared Zone to the Private/Internal Zones, is not allowed. Similarly, communication from the Public Zone to the Shared Zone to the Private/Internal Zones is not allowed. Communication always follows the principle from secure to insecure. This hierarchical structure allows orderly and security-conscious communication between the different zones. Furthermore, floating IPs from the Private/Internal Zones are only reachable from the institute network, services with a floating IP from the Public IP can be provided to the internet through the institute IT. On request, a Lab Zone can be set up as a special zone with a direct connection to the switches. The zones are managed by the Operations Center. For projects created in OpenStack, the user is responsible. How can the hardware servers be shut down/restarted? For example, during a temporary power outage. Currently only through a request in the Jira Helpdesk. Do servers start automatically after a power outage? . The servers start automatically, but the edge is not automatically usable, as CEPH replication is disabled during a controlled shutdown. This must be re-enabled upon restart. If the attached Windows image is selected, does it already include a license? What kind of license is it? . It’s a trial license from Microsoft. For Windows VMs, the customer is responsible for acquiring and adding a license. NVLink and NVSwitches . NVLink and NVSwitches connect NVidia graphics cards for direct data exchange between the graphics cards. Currently, we use Supermicro GPU servers, which do not have NVLink or NVSwitches between the individual GPUs. At present, NVLink and NVSwitches are only installed in NVidia’s own servers of the DGX or HGX brand. ",
    "url": "/edge/faq/#general",
    
    "relUrl": "/edge/faq/#general"
  },"236": {
    "doc": "Guided Tour",
    "title": "Guided Tour",
    "content": "From the browser to a self written heat template . ",
    "url": "/optimist/guided_tour/",
    
    "relUrl": "/optimist/guided_tour/"
  },"237": {
    "doc": "01: The Horizon (Dashboard)",
    "title": "Step 1: The Horizon (Dashboard)",
    "content": " ",
    "url": "/optimist/guided_tour/step01/#step-1-the-horizon-dashboard",
    
    "relUrl": "/optimist/guided_tour/step01/#step-1-the-horizon-dashboard"
  },"238": {
    "doc": "01: The Horizon (Dashboard)",
    "title": "Start",
    "content": "In this step-by-step tutorial, you will learn how to use OpenStack. You will start with the Horizon dashboard, then switch to the console and finish with writing your own Heat template. ",
    "url": "/optimist/guided_tour/step01/#start",
    
    "relUrl": "/optimist/guided_tour/step01/#start"
  },"239": {
    "doc": "01: The Horizon (Dashboard)",
    "title": "Login",
    "content": "After you have received your credentials, you can log on to the dashboard. IMPORTANT: You cannot automatically reset the password. If you need a new password, write an e-mail to support@gec.io . The URL for the dashboard is https://optimist.gec.io . The login form appears. Use default for the Domain field, and enter your user name and password in the relevant fields. To log in, click Connect. ",
    "url": "/optimist/guided_tour/step01/#login",
    
    "relUrl": "/optimist/guided_tour/step01/#login"
  },"240": {
    "doc": "01: The Horizon (Dashboard)",
    "title": "Change Password",
    "content": "For security reasons we recommend changing your password after receiving it from GEC. To change the password, click your User name (1) in right corner of Horizon and then Settings (2). The Settings window appears, where you can change various settings. To change the password, click Change Password (1) in the left-side navigation menu. Enter your old password (2), then enter the new one (3), and confirm the new one (4). To save it, click Change (5). ",
    "url": "/optimist/guided_tour/step01/#change-password",
    
    "relUrl": "/optimist/guided_tour/step01/#change-password"
  },"241": {
    "doc": "01: The Horizon (Dashboard)",
    "title": "Conclusion",
    "content": "You have taken your first steps in the dashboard and successfully changed your password. ",
    "url": "/optimist/guided_tour/step01/#conclusion",
    
    "relUrl": "/optimist/guided_tour/step01/#conclusion"
  },"242": {
    "doc": "01: The Horizon (Dashboard)",
    "title": "01: The Horizon (Dashboard)",
    "content": " ",
    "url": "/optimist/guided_tour/step01/",
    
    "relUrl": "/optimist/guided_tour/step01/"
  },"243": {
    "doc": "02: Create an SSH-Key with the Horizon (Dashboard)",
    "title": "Step 2: Create an SSH-Key with the Horizon (Dashboard)",
    "content": " ",
    "url": "/optimist/guided_tour/step02/#step-2-create-an-ssh-key-with-the-horizon-dashboard",
    
    "relUrl": "/optimist/guided_tour/step02/#step-2-create-an-ssh-key-with-the-horizon-dashboard"
  },"244": {
    "doc": "02: Create an SSH-Key with the Horizon (Dashboard)",
    "title": "Start",
    "content": "To continue, you need an SSH keypair. If you already have a keypair and know how to use it, you can skip this section and continue with Step 3. ",
    "url": "/optimist/guided_tour/step02/#start",
    
    "relUrl": "/optimist/guided_tour/step02/#start"
  },"245": {
    "doc": "02: Create an SSH-Key with the Horizon (Dashboard)",
    "title": "Installation",
    "content": "There are several ways to generate an SSH keypair. Later in this guided tour, we will show you how to create a keypair manually. In this step, however, you will learn how to create it from the dashboard. To create the SSH keypair, go to Compute -&gt; Key Pairs and click Create Key Pair. A window appears where you can name the key. In this example, the name BeispielKey is used. After you have entered the name, click Create Keypair. ",
    "url": "/optimist/guided_tour/step02/#installation",
    
    "relUrl": "/optimist/guided_tour/step02/#installation"
  },"246": {
    "doc": "02: Create an SSH-Key with the Horizon (Dashboard)",
    "title": "Conclusion",
    "content": "You have now created your SSH keypair. ",
    "url": "/optimist/guided_tour/step02/#conclusion",
    
    "relUrl": "/optimist/guided_tour/step02/#conclusion"
  },"247": {
    "doc": "02: Create an SSH-Key with the Horizon (Dashboard)",
    "title": "02: Create an SSH-Key with the Horizon (Dashboard)",
    "content": " ",
    "url": "/optimist/guided_tour/step02/",
    
    "relUrl": "/optimist/guided_tour/step02/"
  },"248": {
    "doc": "03: Spawn a new Stack",
    "title": "Step 3: Spawn a new Stack",
    "content": " ",
    "url": "/optimist/guided_tour/step03/#step-3-spawn-a-new-stack",
    
    "relUrl": "/optimist/guided_tour/step03/#step-3-spawn-a-new-stack"
  },"249": {
    "doc": "03: Spawn a new Stack",
    "title": "Start",
    "content": "In this step you will use the dashboard to spawn a stack that includes a VM. You will also get better acquainted with the dashboard. For this step you need the SSH keypair created in Step 2. To spawn a new stack, you need a template that starts a VM. We recommend using SingleServer.yaml from the GECio Github Repository. Once you have acquired the template, log in to the dashboad with your new password (see Step 1). Go to Orchestration → Stacks and click LAUNCH STACK. In the following dialog window, for Template Source select File, and for Template File, use the downloaded SingleServer.yaml file. Then click Next. In the next dialog window, provide the following data: . | Stack Name: BeispielServer | Creation Timeout: 60 | Password for User: Please use your own password | availability_zone: ix1 | flavor_name: m1.micro | key_name: BeispielKey | machine_name: singleserver | public_network_id: provider | . After you have completed all fields, click Launch to spawn the stack. The stack will spawn and looks like this. You can verify if the stack has correctly started the instance. Navigate to Compute -&gt; Instances. The overview should look like this: . You have spawned the stack. Now we will show you how to delete it, including the VM. It is also possible to delete the instance itself, but this may cause problems if you want to delete the stack afterwards. To delete a stack, navigate to Orchestration → Stack, and click the down arrow symbol behind the Example Stack. Then choose Delete Stack. ",
    "url": "/optimist/guided_tour/step03/#start",
    
    "relUrl": "/optimist/guided_tour/step03/#start"
  },"250": {
    "doc": "03: Spawn a new Stack",
    "title": "Conclusion",
    "content": "You have created and deleted your first stack. ",
    "url": "/optimist/guided_tour/step03/#conclusion",
    
    "relUrl": "/optimist/guided_tour/step03/#conclusion"
  },"251": {
    "doc": "03: Spawn a new Stack",
    "title": "03: Spawn a new Stack",
    "content": " ",
    "url": "/optimist/guided_tour/step03/",
    
    "relUrl": "/optimist/guided_tour/step03/"
  },"252": {
    "doc": "04: Your way to the console",
    "title": "Step 4: Your way to the console",
    "content": " ",
    "url": "/optimist/guided_tour/step04/#step-4-your-way-to-the-console",
    
    "relUrl": "/optimist/guided_tour/step04/#step-4-your-way-to-the-console"
  },"253": {
    "doc": "04: Your way to the console",
    "title": "Start",
    "content": "To make the administration of OpenStack as simple as possible, we recommend using the OpenStackClient. For simple, non-recurring tasks, it may be easier to use the Horizon dashboard. With recurring tasks, or when you want to manage a complex stack, it is better to use the OpenStack client and Heat. This may be unfamiliar at first, but with some practice, you can quickly and efficiently manage stacks. The client is helpful in the administration of the OpenStack environment. It contains Nova, Glance, Cinder, and Neutron. As we use the client heavily in our Guided Tour, we will install it in the next step. ",
    "url": "/optimist/guided_tour/step04/#start",
    
    "relUrl": "/optimist/guided_tour/step04/#start"
  },"254": {
    "doc": "04: Your way to the console",
    "title": "Installation",
    "content": "To install the OpenStackClient, you need at least Python 2.7 and Python Setuptools (which are included in macOS). There are several ways to install the OpenStackClient. In our example, we use pip, and we recommend that you do the same. “pip” is easy to use and you can also use it as an update manager for pip. You can install the client as root (the administrative user), but that may cause problems. Therefore, you can install it in a virtual environment. macOS . To install the OpenStackClient, you need to install pip. Start the console (Launchpad → Console) and type the following command: . $ easy_install pip Searching for pip Best match: pip 9.0.1 Adding pip 9.0.1 to easy-install.pth file Installing pip script to /usr/local/bin Installing pip2.7 script to /usr/local/bin Installing pip2 script to /usr/local/bin Using /usr/local/lib/python2.7/site-packages Processing dependencies for pip Finished processing dependencies for pip . Now you can install virtualenv. $ pip install virtualenv Collecting virtualenv Downloading virtualenv-15.1.0-py2.py3-none-any.whl (1.8MB) 100% |????????????????????????????????| 1.8MB 619kB/s Installing collected packages: virtualenv Successfully installed virtualenv-15.1.0 . After you have installed virtualenv, you can create the virtual environment. $ virtualenv ~/.virtualenvs/openstack New python executable in /Users/iNNOVO/.virtualenvs/openstack/bin/python Installing setuptools, pip, wheel...done. Now you may activate the virtual environment. $ source ~/.virtualenvs/openstack/bin/activate (openstack) $ . Now you can install the OpenStack client. (openstack) $ pip install python-openstackclient . Since we use other services in our documentation, you can install these clients as well. (openstack) $ pip install python-heatclient python-designateclient python-octaviaclient . Now that you are done, you can deactivate your environment. (openstack) $ deactivate . Finally, ensure that you can use the client outside of your virtual environment. export PATH=\"$HOME/.virtualenvs/openstack/bin/:$PATH\" . Now you can check that everything works. It should look like this: . $ type -a openstack openstack is /home/iNNOVO/.virtualenvs/openstack/bin/openstack . Windows . If Python is already installed, you need to navigate to installation folder (standard installation folder C:\\Python27\\Scripts). To install pip, use the command easy_install pip: . Once pip is installed, you can install the OpenStack client: . Linux (in our example Ubuntu) . First, install pip. $ sudo apt-get install python3-pip Reading package lists... Done Building dependency tree Reading state information... Done . Next, install virtualenv, which is required to set up your virtual environment. $ sudo apt-get install python3-virtualenv Reading package lists... Done Building dependency tree Reading state information... Done . Now you can create a virtual environment where you install the OpenStack client. $ virtualenv ~/.virtualenvs/openstack New python executable in /Users/iNNOVO/.virtualenvs/openstack/bin/python Installing setuptools, pip, wheel...done. Then you can activate your newly created environment. $ source ~/.virtualenvs/openstack/bin/activate (openstack) $ . Once activated, you can install the OpenStackClient: . (openstack) $ pip install python-openstackclient . As we use Heat in our documentation, you will also install the Heat client. (openstack) $ pip install python-heatclient . Once you are done, you can deactivate your virtual environment. (openstack) $ deactivate . Finally, ensure that you can use your newly installed software. export PATH=\"$HOME/.virtualenvs/openstack/bin/:$PATH\" . Now you can check that everything works. It should look like this: . $ type -a openstack openstack is /home/iNNOVO/.virtualenvs/openstack/bin/openstack . ",
    "url": "/optimist/guided_tour/step04/#installation",
    
    "relUrl": "/optimist/guided_tour/step04/#installation"
  },"255": {
    "doc": "04: Your way to the console",
    "title": "Credentials",
    "content": "For the OpenStack client to work, you need to supply it with the credentials. You can download the credentials directly from the Horizon dashboard. After the login, click on your mail address in the right corner. Then click  Download OpenStack RC File v3. macOS | Linux . You need to source the credentials, which can be easily done with this command (IMPORTANT: The command can only be used in the folder where the RC file was downloaded):   . source EXAMPLE.sh . Windows . To source the credentials on Windows, you must use PowerShell, Git for Windows or Linux on Windows. If you use Git for Windows or Linux on Windows, you can use the same commands described in the macOS | Linux section. source EXAMPLE.sh . If you use PowerShell, you need to set each variable individually. All required variables are in the previously downloaded Beispiel.sh. To set the variables, use the following command: . set-item env:OS_AUTH_URL -value \"https://identity.optimist.gec.io/v3\" set-item env:OS_PROJECT_ID -value \"Projekt ID eintragen\" set-item env:OS_PROJECT_NAME -value \"Namen eintrage\" set-item env:OS_USER_DOMAIN_NAME -value \"Default\" set-item env:OS_USERNAME -value \"Usernamen eintragen\" set-item env:OS_PASSWORD -value \"Passwort eingeben\" set-item env:OS_USER_DOMAIN_NAME -value \"Default\" set-item env:OS_REGION_NAME -value \"fra\" set-item env:OS_INTERFACE -value \"public\" set-item env:OS_IDENTITY_API_VERSION -value \"3\" . ",
    "url": "/optimist/guided_tour/step04/#credentials",
    
    "relUrl": "/optimist/guided_tour/step04/#credentials"
  },"256": {
    "doc": "04: Your way to the console",
    "title": "Conclusion",
    "content": "You now have an OpenStack client with working credentials, and can test the commands. To get an overview of all OpenStack commands, run the following command: . openstack --help . ",
    "url": "/optimist/guided_tour/step04/#conclusion",
    
    "relUrl": "/optimist/guided_tour/step04/#conclusion"
  },"257": {
    "doc": "04: Your way to the console",
    "title": "04: Your way to the console",
    "content": " ",
    "url": "/optimist/guided_tour/step04/",
    
    "relUrl": "/optimist/guided_tour/step04/"
  },"258": {
    "doc": "05: An overview of the most important OpenStackClient commands",
    "title": "Step 5: An overview of the most important OpenStackClient commands",
    "content": " ",
    "url": "/optimist/guided_tour/step05/#step-5-an-overview-of-the-most-important-openstackclient-commands",
    
    "relUrl": "/optimist/guided_tour/step05/#step-5-an-overview-of-the-most-important-openstackclient-commands"
  },"259": {
    "doc": "05: An overview of the most important OpenStackClient commands",
    "title": "Start",
    "content": "Now that you have installed the OpenStack client in Step 4, you will learn some important commands. To get more information about a specific subcommand, append the --help flag to it. To list all commands, you can use the --help flag: . openstack --help . ",
    "url": "/optimist/guided_tour/step05/#start",
    
    "relUrl": "/optimist/guided_tour/step05/#start"
  },"260": {
    "doc": "05: An overview of the most important OpenStackClient commands",
    "title": "Server",
    "content": "With the command openstack server you can create, administrate, or delete a VM. Here is a list of some common commands: . | openstack server add Adds parameters (Fixed IP, Floating IP, Security group, Volume) to a VM | openstack server create Creates a VM | openstack server delete Deletes a VM | openstack server list Shows a list of all VMs | openstack server remove Removes parameters (Fixed IP, Floating IP, Security group, Volume) from a VM | openstack server show Shows all important information about the specified VM | . ",
    "url": "/optimist/guided_tour/step05/#server",
    
    "relUrl": "/optimist/guided_tour/step05/#server"
  },"261": {
    "doc": "05: An overview of the most important OpenStackClient commands",
    "title": "Stack",
    "content": "With the command openstack stack you are able to administrate complete stacks, like openstack server for instances. Here is a list for some common commands: . | openstack stack create Creates a new stack | openstack stack list Shows a list of all stacks | openstack stack show Shows all important information about the specified stack | openstack stack delete Deletes the specified stack | . ",
    "url": "/optimist/guided_tour/step05/#stack",
    
    "relUrl": "/optimist/guided_tour/step05/#stack"
  },"262": {
    "doc": "05: An overview of the most important OpenStackClient commands",
    "title": "Security Group",
    "content": "Security Groups are used to allow or deny incoming and outgoing network traffic based on IP adresses and ports for VMs. You can also manage security groups in the OpenStackClient. Here are some common commands: . | openstack security group create Creates a new security group. | openstack security group delete Deletes a security group | openstack security group list Shows a list of all security groups | openstack security group show Shows all important information about a security group | openstack security group rule create Adds a rule for a security group | openstack security group rule delete Deletes a rule in a security group | . ",
    "url": "/optimist/guided_tour/step05/#security-group",
    
    "relUrl": "/optimist/guided_tour/step05/#security-group"
  },"263": {
    "doc": "05: An overview of the most important OpenStackClient commands",
    "title": "Network",
    "content": "To create VMs, a network is required. Here are some common network commands: . | openstack network create Creates a new network | openstack network list Shows a list of all networks | openstack network show Shows all important information about a network | openstack network delete Deletes a network | . ",
    "url": "/optimist/guided_tour/step05/#network",
    
    "relUrl": "/optimist/guided_tour/step05/#network"
  },"264": {
    "doc": "05: An overview of the most important OpenStackClient commands",
    "title": "Router",
    "content": "For the VMs on your network to reach the internet, you need a router. Here are some common router commands. | openstack router create Creates a new router | openstack router delete Deletes a router | openstack router add port Adds a port to a router | openstack router add subnet Adds a subnet to a router | . ",
    "url": "/optimist/guided_tour/step05/#router",
    
    "relUrl": "/optimist/guided_tour/step05/#router"
  },"265": {
    "doc": "05: An overview of the most important OpenStackClient commands",
    "title": "Subnet",
    "content": "To use a virtual router correctly, you need a subnet that can be administrated with openstack subnet. Here are some common commands: . | openstack subnet create Creates a new subnet | openstack subnet delete Deletes a subnet | openstack subnet show Shows all infomation about a subnet | . ",
    "url": "/optimist/guided_tour/step05/#subnet",
    
    "relUrl": "/optimist/guided_tour/step05/#subnet"
  },"266": {
    "doc": "05: An overview of the most important OpenStackClient commands",
    "title": "Port",
    "content": "Ports connect your VMs to your network. Here are some common commands: . | openstack port create Create a new port | openstack port delete Deletes a port | openstack port show Shows all infomation about a port | . ",
    "url": "/optimist/guided_tour/step05/#port",
    
    "relUrl": "/optimist/guided_tour/step05/#port"
  },"267": {
    "doc": "05: An overview of the most important OpenStackClient commands",
    "title": "Volume",
    "content": "Volumes are storage locations that persist across the existence of individual instances. Here are some common commands: . | openstack volume create Creates a new Volume | openstack volume delete Deletes a volume | openstack volume show Shows all infomation about a volume | . ",
    "url": "/optimist/guided_tour/step05/#volume",
    
    "relUrl": "/optimist/guided_tour/step05/#volume"
  },"268": {
    "doc": "05: An overview of the most important OpenStackClient commands",
    "title": "Conclusion",
    "content": "Now you know some common OpenStack commands, and have a better overview of the system. These commands are required in the next steps and form the basis for the rest of the guided tour. In Step 6, you will create and use your own SSH key pairs. ",
    "url": "/optimist/guided_tour/step05/#conclusion",
    
    "relUrl": "/optimist/guided_tour/step05/#conclusion"
  },"269": {
    "doc": "05: An overview of the most important OpenStackClient commands",
    "title": "05: An overview of the most important OpenStackClient commands",
    "content": " ",
    "url": "/optimist/guided_tour/step05/",
    
    "relUrl": "/optimist/guided_tour/step05/"
  },"270": {
    "doc": "06: Create and use your own SSH-Key",
    "title": "Step 6: Create and use your own SSH-Key",
    "content": " ",
    "url": "/optimist/guided_tour/step06/#step-6-create-and-use-your-own-ssh-key",
    
    "relUrl": "/optimist/guided_tour/step06/#step-6-create-and-use-your-own-ssh-key"
  },"271": {
    "doc": "06: Create and use your own SSH-Key",
    "title": "Start",
    "content": "To access your VMs with SSH you need to create an SSH keypair. If you already have a keypair, you do not need to create a new one. ",
    "url": "/optimist/guided_tour/step06/#start",
    
    "relUrl": "/optimist/guided_tour/step06/#start"
  },"272": {
    "doc": "06: Create and use your own SSH-Key",
    "title": "Creation",
    "content": "As mentioned in Step 2, there are several ways to create an SSH keypair. You may create one from the console using the following command: . $ ssh-keygen -t rsa -f Beispiel.key Generating public/private rsa key pair. Enter passphrase (empty for no passphrase): Enter same passphrase again: Your identification has been saved in Beispiel.key. Your public key has been saved in Beispiel.key.pub. The key fingerprint is: SHA256:UKSodmr6MFCO1fSqNYAoyM7uX8n/O5a43cPEV5vJXW8 The key's randomart image is: +---[RSA 2048]----+ | .o |+. o o o |=.+ o + |+= o ..|oo+ = S . o B|o. =... o . =E|o.+ + . + . |.= ...+.o |.oo. o++o.. | +----[SHA256]-----+ . The command above generates two files, i.e. the aforementioned key pair. The two generated files are Beispiel.key (private key) and Beispiel,key.pub (public key). You should always keep your private key in a secure location, while distributing the public key to places you require access to. ",
    "url": "/optimist/guided_tour/step06/#creation",
    
    "relUrl": "/optimist/guided_tour/step06/#creation"
  },"273": {
    "doc": "06: Create and use your own SSH-Key",
    "title": "Installation",
    "content": "To start using your new keypair, you need to add it to your OpenStack environment. You can do this with the OpenStack client. Use the command below (in our example, the created keypair is stored in ~/.ssh/. If your keys are saved in a different location, you need to copy the keypair to ~/.ssh/). $ openstack keypair create --public-key ~/.ssh/Beispiel.key.pub Beispiel +-------------+-------------------------------------------------+ | Field | Value | +-------------+-------------------------------------------------+ | fingerprint | ec:a6:75:f9:33:4b:e0:ba:e7:bb:b6:8a:a1:5d:48:ff | name | Beispiel | user_id | 9bf501f4c3d14b7eb0f1443efe80f656 | +-------------+-------------------------------------------------+ . You can check that this worked by listing the keys. The one you just uploaded should be also visible. $ openstack keypair list +----------+-------------------------------------------------+ | Name | Fingerprint | +----------+-------------------------------------------------+ | Beispiel | ec:a6:75:f9:33:4b:e0:ba:e7:bb:b6:8a:a1:5d:48:ff | +----------+-------------------------------------------------+ . ",
    "url": "/optimist/guided_tour/step06/#installation",
    
    "relUrl": "/optimist/guided_tour/step06/#installation"
  },"274": {
    "doc": "06: Create and use your own SSH-Key",
    "title": "Conclusion",
    "content": "You have now generated a keypair and uploaded the public key. You can use it to log in to your new Instances. We will explain this in Step 7. ",
    "url": "/optimist/guided_tour/step06/#conclusion",
    
    "relUrl": "/optimist/guided_tour/step06/#conclusion"
  },"275": {
    "doc": "06: Create and use your own SSH-Key",
    "title": "06: Create and use your own SSH-Key",
    "content": " ",
    "url": "/optimist/guided_tour/step06/",
    
    "relUrl": "/optimist/guided_tour/step06/"
  },"276": {
    "doc": "07: The first VM",
    "title": "Step 7: The first VM",
    "content": " ",
    "url": "/optimist/guided_tour/step07/#step-7-the-first-vm",
    
    "relUrl": "/optimist/guided_tour/step07/#step-7-the-first-vm"
  },"277": {
    "doc": "07: The first VM",
    "title": "Start",
    "content": "In the previous steps, you learned everything you need to create a VM. In general, it is best to create VMs as part of a stack, and to create these stacks with Heat, or other automation tools like Terraform. To ensure that you know the basics, this step deals with creating a single VM manually. ",
    "url": "/optimist/guided_tour/step07/#start",
    
    "relUrl": "/optimist/guided_tour/step07/#start"
  },"278": {
    "doc": "07: The first VM",
    "title": "Installation",
    "content": "The basic command to create a single VM is: . openstack server create test . If you execute the above command, the following error is returned: . usage: openstack server create [-h] [-f {json,shell,table,value,yaml}] [-c COLUMN] [--max-width &lt;integer&gt;] [--print-empty] [--noindent] [--prefix PREFIX] (--image &lt;image&gt; | --volume &lt;volume&gt;) --flavor &lt;flavor&gt; [--security-group &lt;security-group-name&gt;] [--key-name &lt;key-name&gt;] [--property &lt;key=value&gt;] [--file &lt;dest-filename=source-filename&gt;] [--user-data &lt;user-data&gt;] [--availability-zone &lt;zone-name&gt;] [--block-device-mapping &lt;dev-name=mapping&gt;] [--nic &lt;net-id=net-uuid,v4-fixed-ip=ip-addr,v6-fixed-ip=ip-addr,port-id=port-uuid,auto,none&gt;] [--hint &lt;key=value&gt;] [--config-drive &lt;config-drive-volume&gt;|True] [--min &lt;count&gt;] [--max &lt;count&gt;] [--wait] &lt;server-name&gt; openstack server create: error: argument --flavor is required . It tells us that you have not specified a flavor for your VM. To specify a flavor, you need to add the flag --flavor with a flavor argument. With the following command you can see the available flavors: . $ openstack flavor list +--------------------------------------+------------+-------+------+-----------+-------+-----------+ | ID | Name | RAM | Disk | Ephemeral | VCPUs | Is Public | +--------------------------------------+------------+-------+------+-----------+-------+-----------+ | 090bcc91-6207-465d-aff0-bfcc10a9e063 | m1.medium | 8192 | 20 | 0 | 4 | True | 4ade7a50-f829-4bf6-af15-266798ea8d6f | win.large | 32768 | 80 | 0 | 8 | True | 5dd72380-088e-48cd-9a18-112cb5a9cab5 | win.small | 8192 | 80 | 0 | 2 | True | 884d5b93-1467-4bc1-a445-ff7c74271cbd | m1.micro | 1024 | 20 | 0 | 1 | True | b7c4fa0b-7960-4311-a86b-507dbf58e8ac | m1.small | 4096 | 20 | 0 | 2 | True | d45e3029-8364-4e4c-beab-242e8b4622a3 | win.medium | 16384 | 80 | 0 | 4 | True | dfead62e-96a8-46e9-bdae-342ecce32d41 | win.micro | 2048 | 80 | 0 | 1 | True | ed18c320-324a-487f-88e1-3e9eb9244509 | m1.large | 16384 | 20 | 0 | 8 | True | +--------------------------------------+------------+-------+------+-----------+-------+-----------+ . If you add --flavor m1.micro to the command and execute it, it will still return an error as OpenStack needs more data to start a new VM. In addition to flavors, you need to supply the key (--key-name) and the operating system image to be installed (--image), the network the VM will run on (--network), and the security group to be applied to it (--security-group). You already created a security group in a previous step, so you need to acquire an image and a network to create your first VM. With the following command you can see the available images: . $ openstack image list +--------------------------------------+---------------------------------------+--------+ | ID | Name | Status | +--------------------------------------+---------------------------------------+--------+ | fd8ad5aa-6b33-4198-a05d-8be42fc0f20e | CentOS 7 - Latest | active | 82242d21-d990-4fc2-92a5-c7bd7820e790 | Ubuntu 16.04 Xenial Xerus - Latest | active | 8e82fd42-3d6f-44a7-9f20-92f5661823cf | Windows Server 2012 R2 Std - Latest | active | 536c086c-d2a4-43dd-80ea-a9d05ee2b97f | Windows Server 2016 Std - Latest | active | c94ced87-a03e-4eec-89f7-48f2c0ec6cd2 | debian-9.1.5-20170910-openstack-amd64 | active | b1195ddf-9336-42a7-a134-4f2e7ea57710 | iNNOVO-OPNsense-17.7.8 | active | 9134b6ed-8c5a-4a9a-907e-733dc2b5f0ef | iNNOVO_pfSense 2.3.4 | active | +--------------------------------------+---------------------------------------+--------+ . Next, you have to select a nework. You can create a simple network with the following command: . $ openstack network create BeispielNetzwerk +---------------------------+--------------------------------------+ | Field | Value | +---------------------------+--------------------------------------+ | admin_state_up | UP | availability_zone_hints | | availability_zones | | created_at | 2017-12-08T08:32:44Z | description | | dns_domain | None | id | a783d691-7efe-4f67-9226-99a014fa8926 | ipv4_address_scope | None | ipv6_address_scope | None | is_default | False | mtu | 1500 | name | BeispielNetzwerk | port_security_enabled | True | project_id | b15cde70d85749689e08106f973bb002 | provider:network_type | None | provider:physical_network | None | provider:segmentation_id | None | qos_policy_id | None | revision_number | 2 | router:external | Internal | segments | None | shared | False | status | ACTIVE | subnets | | updated_at | 2017-12-08T08:32:44Z | +---------------------------+--------------------------------------+ . Note that this network has no internet connection, and no additional configuration. You will learn how to create a functional network in Step 10. Now we will put everything together, and create a VM. In this example, you will use the default security group, the Ubuntu 16.04 image (you use the ID in the command line), and the previously created network and key: . $ openstack server create BeispielServer --flavor m1.small --key-name Beispiel --image 82242d21-d990-4fc2-92a5-c7bd7820e790 --network=BeispielNetzwerk --security-group default +-----------------------------+--------------------------------------------------------+ | Field | Value | +-----------------------------+--------------------------------------------------------+ | OS-DCF:diskConfig | MANUAL | OS-EXT-AZ:availability_zone | es1 | OS-EXT-STS:power_state | NOSTATE | OS-EXT-STS:task_state | scheduling | OS-EXT-STS:vm_state | building | OS-SRV-USG:launched_at | None | OS-SRV-USG:terminated_at | None | accessIPv4 | | accessIPv6 | | addresses | | config_drive | | created | 2017-12-06T14:15:02Z | flavor | m1.small (676d2587-b5aa-49eb-998d-d91c1bd6c056) | hostId | | id | 44ff2688-4ce5-417d-962b-3a80199bf1bc | image | cirros-tempest1 (2fbe66ef-adc8-44d0-b2e2-03d95dc36936) | key_name | cg | name | BeispielServer | progress | 0 | project_id | 1e775e2cc71a461991be42d4fad8a5cb | properties | | security_groups | name='3265503b-ac24-4f60-a8d0-466b7c812916' | status | BUILD | updated | 2017-12-06T14:15:02Z | user_id | b54fda3f4d1a484797b3ad4de9b3f4f9 | volumes_attached | +-----------------------------+--------------------------------------------------------+ . To see all possible parameters during the creation of a VM, you can use --help: . $ openstack server create --help usage: openstack server create [-h] [-f {json,shell,table,value,yaml}] [-c COLUMN] [--max-width &lt;integer&gt;] [--print-empty] [--noindent] [--prefix PREFIX] (--image &lt;image&gt; | --volume &lt;volume&gt;) --flavor &lt;flavor&gt; [--security-group &lt;security-group-name&gt;] [--key-name &lt;key-name&gt;] [--property &lt;key=value&gt;] [--file &lt;dest-filename=source-filename&gt;] [--user-data &lt;user-data&gt;] [--availability-zone &lt;zone-name&gt;] [--block-device-mapping &lt;dev-name=mapping&gt;] [--nic &lt;net-id=net-uuid,v4-fixed-ip=ip-addr,v6-fixed-ip=ip-addr,port-id=port-uuid,auto,none&gt;] [--hint &lt;key=value&gt;] [--config-drive &lt;config-drive-volume&gt;|True] [--min &lt;count&gt;] [--max &lt;count&gt;] [--wait] &lt;server-name&gt; Create a new server positional arguments: &lt;server-name&gt; New server name optional arguments: -h, --help show this help message and exit --image &lt;image&gt; Create server boot disk from this image (name or ID) --volume &lt;volume&gt; Create server using this volume as the boot disk (name or ID) --flavor &lt;flavor&gt; Create server with this flavor (name or ID) --security-group &lt;security-group-name&gt; Security group to assign to this server (name or ID) (repeat option to set multiple groups) --key-name &lt;key-name&gt; Keypair to inject into this server (optional extension) --property &lt;key=value&gt; Set a property on this server (repeat option to set multiple values) --file &lt;dest-filename=source-filename&gt; File to inject into image before boot (repeat option to set multiple files) --user-data &lt;user-data&gt; User data file to serve from the metadata server --availability-zone &lt;zone-name&gt; Select an availability zone for the server --block-device-mapping &lt;dev-name=mapping&gt; Map block devices; map is &lt;id&gt;:&lt;type&gt;:&lt;size(GB)&gt;:&lt;delete_on_terminate&gt; (optional extension) --nic &lt;net-id=net-uuid,v4-fixed-ip=ip-addr,v6-fixed-ip=ip-addr,port-id=port-uuid,auto,none&gt; Create a NIC on the server. Specify option multiple times to create multiple NICs. Either net-id or port- id must be provided, but not both. net-id: attach NIC to network with this UUID, port-id: attach NIC to port with this UUID, v4-fixed-ip: IPv4 fixed address for NIC (optional), v6-fixed-ip: IPv6 fixed address for NIC (optional), none: (v2.37+) no network is attached, auto: (v2.37+) the compute service will automatically allocate a network. Specifying a --nic of auto or none cannot be used with any other --nic value. --hint &lt;key=value&gt; Hints for the scheduler (optional extension) --config-drive &lt;config-drive-volume&gt;|True Use specified volume as the config drive, or 'True' to use an ephemeral drive --min &lt;count&gt; Minimum number of servers to launch (default=1) --max &lt;count&gt; Maximum number of servers to launch (default=1) --wait Wait for build to complete output formatters: output formatter options -f {json,shell,table,value,yaml}, --format {json,shell,table,value,yaml} the output format, defaults to table -c COLUMN, --column COLUMN specify the column(s) to include, can be repeated table formatter: --max-width &lt;integer&gt; Maximum display width, &lt;1 to disable. You can also use the CLIFF_MAX_TERM_WIDTH environment variable, but the parameter takes precedence. --print-empty Print empty table if there is no data to show. json formatter: --noindent whether to disable indenting the JSON shell formatter: a format a UNIX shell can parse (variable=\"value\") --prefix PREFIX add a prefix to all variable names . ",
    "url": "/optimist/guided_tour/step07/#installation",
    
    "relUrl": "/optimist/guided_tour/step07/#installation"
  },"279": {
    "doc": "07: The first VM",
    "title": "Conclusion",
    "content": "You have now created your first Instance using basic OpenStack commands. In the next step, you will delete this instance. ",
    "url": "/optimist/guided_tour/step07/#conclusion",
    
    "relUrl": "/optimist/guided_tour/step07/#conclusion"
  },"280": {
    "doc": "07: The first VM",
    "title": "07: The first VM",
    "content": " ",
    "url": "/optimist/guided_tour/step07/",
    
    "relUrl": "/optimist/guided_tour/step07/"
  },"281": {
    "doc": "08: Delete the first VM",
    "title": "Step 8: Delete the first VM",
    "content": " ",
    "url": "/optimist/guided_tour/step08/#step-8-delete-the-first-vm",
    
    "relUrl": "/optimist/guided_tour/step08/#step-8-delete-the-first-vm"
  },"282": {
    "doc": "08: Delete the first VM",
    "title": "Start",
    "content": "In the previous step, you created a VM. In this step, you will learn how to delete it so that you can reuse its resources. First of all, you need to acquire the name or the ID of the VM. If you only have few VMs, you can use the name. Since names are not unique, we strongly recommend using the ID. Let’s get a list of all your VMs: . $ openstack server list +--------------------------------------+--------------+--------+---------------------------------------------------+------------------------------------+ | ID | Name | Status | Networks | Image Name | +--------------------------------------+--------------+--------+---------------------------------------------------+------------------------------------+ | 801b3021-0c00-4566-881e-b50d47152e63 | singleserver | ACTIVE | single_internal_network=10.0.0.12, 185.116.245.39 | Ubuntu 16.04 Xenial Xerus - Latest | +--------------------------------------+--------------+--------+---------------------------------------------------+------------------------------------+ . This returns a list of all your VMs. You can find the ID in column ID, and the name in column Name. Now that you have this information, you can delete it: . openstack server delete 801b3021-0c00-4566-881e-b50d47152e63 . If you re-run the command, it should return nothing at all: . $ openstack server list $ . ",
    "url": "/optimist/guided_tour/step08/#start",
    
    "relUrl": "/optimist/guided_tour/step08/#start"
  },"283": {
    "doc": "08: Delete the first VM",
    "title": "Conclusion",
    "content": "You have now learned how to delete instances. Additionally, with the command openstack server list you can get an overview of all instances. In Step 9, we will focus on security groups. ",
    "url": "/optimist/guided_tour/step08/#conclusion",
    
    "relUrl": "/optimist/guided_tour/step08/#conclusion"
  },"284": {
    "doc": "08: Delete the first VM",
    "title": "08: Delete the first VM",
    "content": " ",
    "url": "/optimist/guided_tour/step08/",
    
    "relUrl": "/optimist/guided_tour/step08/"
  },"285": {
    "doc": "09: Security Groups",
    "title": "Step 9: Security Groups",
    "content": " ",
    "url": "/optimist/guided_tour/step09/#step-9-security-groups",
    
    "relUrl": "/optimist/guided_tour/step09/#step-9-security-groups"
  },"286": {
    "doc": "09: Security Groups",
    "title": "Start",
    "content": "By default, any incoming traffic to a VM is denied. To allow access to an instance, at least one security group must be created and assigned to the instance. While you can add all access rules to a single security group, we recommend using a separate security group for each service. ",
    "url": "/optimist/guided_tour/step09/#start",
    
    "relUrl": "/optimist/guided_tour/step09/#start"
  },"287": {
    "doc": "09: Security Groups",
    "title": "Create a Security Group",
    "content": "The base command for creating a security group is openstack security group create, for example: . openstack security group create allow-ssh-from-anywhere --description Beispiel +-----------------+-----------------------------------------------------------------------------------------------------------------------------------------------------+ | Field | Value | +-----------------+-----------------------------------------------------------------------------------------------------------------------------------------------------+ | created_at | 2017-12-08T12:01:42Z | description | Beispiel | id | 1cab4a62-0fda-40d9-bac8-fd73275b472d | name | allow-ssh-from-anywhere | project_id | b15cde70d85749689e08106f973bb002 | revision_number | 2 | rules | created_at='2017-12-08T12:01:42Z', direction='egress', ethertype='IPv6', id='5a852e4b-1d79-4fe9-b359-64ca54c98501', | | updated_at='2017-12-08T12:01:42Z' | | created_at='2017-12-08T12:01:42Z', direction='egress', ethertype='IPv4', id='fa90a1ee-d3b9-40d4-9bb5-89fdd5005c02', | | updated_at='2017-12-08T12:01:42Z' | updated_at | 2017-12-08T12:01:42Z | +-----------------+-----------------------------------------------------------------------------------------------------------------------------------------------------+ . Now that you have created an empty security group, you need to add some rules. Some commonly used options are: . | --protocol: The protocol that this rule matches (example arguments: tcp, udp, icmp) | --dst-port: Destination port range to give access to (example arguments: 22:22 for port 22 100:200 for ports 100 through 200) | --remote-ip: Remote IP to allow access from (example arguments: 0.0.0.0/0 for all IP addresses, 1.2.3.0/24 for all IP addresses starting with 1.2.3.) | --ingress or --egress: ingress is incoming traffic and egress is outgoing traffic (no arguments possible) | . You can use these options to create a rule for your new security group to allow SSH from anywhere: . $ openstack security group rule create allow-ssh-from-anywhere --protocol tcp --dst-port 22:22 --remote-ip 0.0.0.0/0 +-------------------+--------------------------------------+ | Field | Value | +-------------------+--------------------------------------+ | created_at | 2017-12-08T12:02:15Z | description | | direction | ingress | ether_type | IPv4 | id | 694a0573-b4c3-423c-847d-550f79e83f2b | name | None | port_range_max | 22 | port_range_min | 22 | project_id | b15cde70d85749689e08106f973bb002 | protocol | tcp | remote_group_id | None | remote_ip_prefix | 0.0.0.0/0 | revision_number | 0 | security_group_id | 1cab4a62-0fda-40d9-bac8-fd73275b472d | updated_at | 2017-12-08T12:02:15Z | +-------------------+--------------------------------------+ . Next, verify if your security group was created correctly: . $ openstack security group show allow-ssh-from-anywhere +-----------------+-----------------------------------------------------------------------------------------------------------------------------------------------------+ | Field | Value | +-----------------+-----------------------------------------------------------------------------------------------------------------------------------------------------+ | created_at | 2017-12-08T12:01:42Z | description | Beispiel | id | 1cab4a62-0fda-40d9-bac8-fd73275b472d | name | allow-ssh-from-anywhere | project_id | b15cde70d85749689e08106f973bb002 | revision_number | 3 | rules | created_at='2017-12-08T12:01:42Z', direction='egress', ethertype='IPv6', id='5a852e4b-1d79-4fe9-b359-64ca54c98501', | | updated_at='2017-12-08T12:01:42Z' | | created_at='2017-12-08T12:02:15Z', direction='ingress', ethertype='IPv4', id='694a0573-b4c3-423c-847d-550f79e83f2b', port_range_max='22', | | port_range_min='22', protocol='tcp', remote_ip_prefix='0.0.0.0/0', updated_at='2017-12-08T12:02:15Z' | | created_at='2017-12-08T12:01:42Z', direction='egress', ethertype='IPv4', id='fa90a1ee-d3b9-40d4-9bb5-89fdd5005c02', | | updated_at='2017-12-08T12:01:42Z' | updated_at | 2017-12-08T12:02:15Z | +-----------------+-----------------------------------------------------------------------------------------------------------------------------------------------------+ . ",
    "url": "/optimist/guided_tour/step09/#create-a-security-group",
    
    "relUrl": "/optimist/guided_tour/step09/#create-a-security-group"
  },"288": {
    "doc": "09: Security Groups",
    "title": "Conclusion",
    "content": "You have successfully created a security group. In the next step, you learn how to add a network. ",
    "url": "/optimist/guided_tour/step09/#conclusion",
    
    "relUrl": "/optimist/guided_tour/step09/#conclusion"
  },"289": {
    "doc": "09: Security Groups",
    "title": "09: Security Groups",
    "content": " ",
    "url": "/optimist/guided_tour/step09/",
    
    "relUrl": "/optimist/guided_tour/step09/"
  },"290": {
    "doc": "10: Get access to the Internet; Create a network",
    "title": "Step 10: Get access to the Internet: Create a network",
    "content": " ",
    "url": "/optimist/guided_tour/step10/#step-10-get-access-to-the-internet-create-a-network",
    
    "relUrl": "/optimist/guided_tour/step10/#step-10-get-access-to-the-internet-create-a-network"
  },"291": {
    "doc": "10: Get access to the Internet; Create a network",
    "title": "Start",
    "content": "So far, you have created a VM and a security group. The next step is to create a network. ",
    "url": "/optimist/guided_tour/step10/#start",
    
    "relUrl": "/optimist/guided_tour/step10/#start"
  },"292": {
    "doc": "10: Get access to the Internet; Create a network",
    "title": "The network",
    "content": "You will start with the network. As with previous commands, you have additional options you can list with --help. To create your network use the following command: . $ openstack network create BeispielNetzwerk +---------------------------+--------------------------------------+ | Field | Value | +---------------------------+--------------------------------------+ | admin_state_up | UP | availability_zone_hints | | availability_zones | | created_at | 2017-12-08T12:06:38Z | description | | dns_domain | None | id | ff6d8654-66d6-4881-9528-2686bddcb6dc | ipv4_address_scope | None | ipv6_address_scope | None | is_default | False | mtu | 1500 | name | BeispielNetzwerk | port_security_enabled | True | project_id | b15cde70d85749689e08106f973bb002 | provider:network_type | None | provider:physical_network | None | provider:segmentation_id | None | qos_policy_id | None | revision_number | 2 | router:external | Internal | segments | None | shared | False | status | ACTIVE | subnets | | updated_at | 2017-12-08T12:06:38Z | +---------------------------+--------------------------------------+ . ",
    "url": "/optimist/guided_tour/step10/#the-network",
    
    "relUrl": "/optimist/guided_tour/step10/#the-network"
  },"293": {
    "doc": "10: Get access to the Internet; Create a network",
    "title": "Subnet",
    "content": "Now that you have a network, you need to create a subnet for it. The subnet creation command also has a few options. In our example, we use: . | --network: Specifies the network where the subnet will be created | --subnet-range: The CIDR range for the subnet. In our example it is 192.168.2.0/24 | . To create a subnet in your existing network, run the following command: . $ openstack subnet create BeispielSubnet --network BeispielNetzwerk --subnet-range 192.168.2.0/24 +-------------------------+--------------------------------------+ | Field | Value | +-------------------------+--------------------------------------+ | allocation_pools | 192.168.2.2-192.0.2.254 | cidr | 192.168.2.0/24 | created_at | 2017-12-08T12:09:07Z | description | | dns_nameservers | | enable_dhcp | True | gateway_ip | 192.168.2.1 | host_routes | | id | 984b24bf-db60-46a9-83c3-d68f6f1062e4 | ip_version | 4 | ipv6_address_mode | None | ipv6_ra_mode | None | name | BeispielSubnet | network_id | ff6d8654-66d6-4881-9528-2686bddcb6dc | project_id | b15cde70d85749689e08106f973bb002 | revision_number | 0 | segment_id | None | service_types | | subnetpool_id | None | updated_at | 2017-12-08T12:09:07Z | use_default_subnet_pool | None | +-------------------------+--------------------------------------+ . ",
    "url": "/optimist/guided_tour/step10/#subnet",
    
    "relUrl": "/optimist/guided_tour/step10/#subnet"
  },"294": {
    "doc": "10: Get access to the Internet; Create a network",
    "title": "Router",
    "content": "For your virtual network to be able to reach the internet, you need to create a router: . $ openstack router create BeispielRouter +-------------------------+--------------------------------------+ | Field | Value | +-------------------------+--------------------------------------+ | admin_state_up | UP | availability_zone_hints | | availability_zones | | created_at | 2017-12-08T12:09:49Z | description | | distributed | False | external_gateway_info | None | flavor_id | None | ha | False | id | bfb91c7f-acca-450a-aae0-c519ab563d38 | name | BeispielRouter | project_id | b15cde70d85749689e08106f973bb002 | revision_number | None | routes | | status | ACTIVE | updated_at | 2017-12-08T12:09:49Z | +-------------------------+--------------------------------------+ . To be able to access the internet, you need to define the external gateway: . openstack router set BeispielRouter --external-gateway provider . Next, add the subnet to the router: . openstack router add subnet BeispielRouter BeispielSubnet . ",
    "url": "/optimist/guided_tour/step10/#router",
    
    "relUrl": "/optimist/guided_tour/step10/#router"
  },"295": {
    "doc": "10: Get access to the Internet; Create a network",
    "title": "Port",
    "content": "Now that you have your subnet and router, you need to create a port for the network. You can link the port using the --network option: . $ openstack port create BeispielPort --network BeispielNetzwerk +-----------------------+----------------------------------------------------------------------------+ | Field | Value | +-----------------------+----------------------------------------------------------------------------+ | admin_state_up | UP | allowed_address_pairs | | binding_host_id | None | binding_profile | None | binding_vif_details | None | binding_vif_type | None | binding_vnic_type | normal | created_at | 2017-12-08T12:12:13Z | description | | device_id | | device_owner | | dns_assignment | None | dns_name | None | extra_dhcp_opts | | fixed_ips | ip_address='192.168.2.8', subnet_id='984b24bf-db60-46a9-83c3-d68f6f1062e4' | id | 31777c0a-a952-43ca-bb7f-11ad33926dae | ip_address | None | mac_address | fa:16:3e:09:88:c8 | name | BeispielPort | network_id | ff6d8654-66d6-4881-9528-2686bddcb6dc | option_name | None | option_value | None | port_security_enabled | True | project_id | b15cde70d85749689e08106f973bb002 | qos_policy_id | None | revision_number | 3 | security_group_ids | 3d3e3074-3087-4965-9a64-34a6d56193b9 | status | DOWN | subnet_id | None | updated_at | 2017-12-08T12:12:13Z | +-----------------------+----------------------------------------------------------------------------+ . ",
    "url": "/optimist/guided_tour/step10/#port",
    
    "relUrl": "/optimist/guided_tour/step10/#port"
  },"296": {
    "doc": "10: Get access to the Internet; Create a network",
    "title": "Conclusion",
    "content": "After the router, subnet, and port have been created and linked together, the setup of the sample network is complete. In the next step, you will add IPv6 access. ",
    "url": "/optimist/guided_tour/step10/#conclusion",
    
    "relUrl": "/optimist/guided_tour/step10/#conclusion"
  },"297": {
    "doc": "10: Get access to the Internet; Create a network",
    "title": "10: Get access to the Internet; Create a network",
    "content": " ",
    "url": "/optimist/guided_tour/step10/",
    
    "relUrl": "/optimist/guided_tour/step10/"
  },"298": {
    "doc": "11: Prepare access to the Internet; Add IPv6 to your network",
    "title": "Step 11: Prepare access to the Internet: Add IPv6 to your network",
    "content": " ",
    "url": "/optimist/guided_tour/step11/#step-11-prepare-access-to-the-internet-add-ipv6-to-your-network",
    
    "relUrl": "/optimist/guided_tour/step11/#step-11-prepare-access-to-the-internet-add-ipv6-to-your-network"
  },"299": {
    "doc": "11: Prepare access to the Internet; Add IPv6 to your network",
    "title": "Start",
    "content": "Now that you have a working network, the next step is to expand it by enabling IPv6 on your setup. You do not have to create a new router, as the existing one will be used. The cloud images we supply have a predefined primary network interface with DHCP enabled. Once you have completed this step, IPv6 will work as well. ",
    "url": "/optimist/guided_tour/step11/#start",
    
    "relUrl": "/optimist/guided_tour/step11/#start"
  },"300": {
    "doc": "11: Prepare access to the Internet; Add IPv6 to your network",
    "title": "Subnet",
    "content": "We have already defined an IPv6 pool. It will be used to create a new subnet. Let’s list all existing pools: . $ openstack subnet pool list +--------------------------------------+---------------+---------------------+ | ID | Name | Prefixes | +--------------------------------------+---------------+---------------------+ | f541f3b6-af22-435a-9cbb-b233d12e74f4 | customer-ipv6 | 2a00:c320:1000::/48 | +--------------------------------------+---------------+---------------------+ . You can now use the pool to generate a subnet. The 64 bit prefix length is fixed for each generated subnet. You can use the subnet in the creation process, or you can accept the default from OpenStack. Let’s create your subnet now: . $ openstack subnet create --network BeispielNetzwerk --ip-version 6 --use-default-subnet-pool --ipv6-address-mode dhcpv6-stateful --ipv6-ra-mode dhcpv6-stateful BeispielSubnetIPv6 +-------------------------+----------------------------------------------------------+ | Field | Value | +-------------------------+----------------------------------------------------------+ | allocation_pools | 2a00:c320:1000:2::2-2a00:c320:1000:2:ffff:ffff:ffff:ffff | cidr | 2a00:c320:1000:2::/64 | created_at | 2017-12-08T12:41:42Z | description | | dns_nameservers | | enable_dhcp | True | gateway_ip | 2a00:c320:1000:2::1 | host_routes | | id | 0046c29b-a9b0-47c3-b5dd-704aa801704d | ip_version | 6 | ipv6_address_mode | dhcpv6-stateful | ipv6_ra_mode | dhcpv6-stateful | name | BeispielSubnetIPv6 | network_id | ff6d8654-66d6-4881-9528-2686bddcb6dc | project_id | b15cde70d85749689e08106f973bb002 | revision_number | 0 | segment_id | None | service_types | | subnetpool_id | f541f3b6-af22-435a-9cbb-b233d12e74f4 | updated_at | 2017-12-08T12:41:42Z | use_default_subnet_pool | True | +-------------------------+----------------------------------------------------------+ . ",
    "url": "/optimist/guided_tour/step11/#subnet",
    
    "relUrl": "/optimist/guided_tour/step11/#subnet"
  },"301": {
    "doc": "11: Prepare access to the Internet; Add IPv6 to your network",
    "title": "Router",
    "content": "Now that the subnet has been created, it can be added to the router. To do so, execute the following command: . openstack router add subnet BeispielRouter BeispielSubnetIPv6 . ",
    "url": "/optimist/guided_tour/step11/#router",
    
    "relUrl": "/optimist/guided_tour/step11/#router"
  },"302": {
    "doc": "11: Prepare access to the Internet; Add IPv6 to your network",
    "title": "Security Group",
    "content": "The security group rules that you created in Step 9 were IPv4 rules. Now you need to add two more rules for IPv6. First, allow SSH access using IPv6 (::/0 is the equivalent of 0.0.0.0/0 but for IPv6): . $ openstack security group rule create --remote-ip \"::/0\" --protocol tcp --dst-port 22:22 --ethertype IPv6 --ingress allow-ssh-from-anywhere +-------------------+--------------------------------------+ | Field | Value | +-------------------+--------------------------------------+ | created_at | 2017-12-08T12:44:04Z | description | | direction | ingress | ether_type | IPv6 | id | 7d871e85-05fa-4620-b558-c6fc64076cde | name | None | port_range_max | 22 | port_range_min | 22 | project_id | b15cde70d85749689e08106f973bb002 | protocol | tcp | remote_group_id | None | remote_ip_prefix | ::/0 | revision_number | 0 | security_group_id | 1cab4a62-0fda-40d9-bac8-fd73275b472d | updated_at | 2017-12-08T12:44:04Z | +-------------------+--------------------------------------+ . For completion’s sake, we will allow ICMP access so that you can ping your VM with IPv6: . $ openstack security group rule create --remote-ip \"::/0\" --protocol ipv6-icmp --ingress allow-ssh-from-anywhere +-------------------+--------------------------------------+ | Field | Value | +-------------------+--------------------------------------+ | created_at | 2017-12-08T12:44:44Z | description | | direction | ingress | ether_type | IPv6 | id | f63e4787-9965-4732-b9d2-20ce0fedc974 | name | None | port_range_max | None | port_range_min | None | project_id | b15cde70d85749689e08106f973bb002 | protocol | ipv6-icmp | remote_group_id | None | remote_ip_prefix | ::/0 | revision_number | 0 | security_group_id | 1cab4a62-0fda-40d9-bac8-fd73275b472d | updated_at | 2017-12-08T12:44:44Z | +-------------------+--------------------------------------+ . ",
    "url": "/optimist/guided_tour/step11/#security-group",
    
    "relUrl": "/optimist/guided_tour/step11/#security-group"
  },"303": {
    "doc": "11: Prepare access to the Internet; Add IPv6 to your network",
    "title": "Adjustments to the operating system",
    "content": "Any new VM based on our images will now have both IPv4 and IPv6 configured, and our provided heat templates will also enable IPv6. Many standard vendor images do not have IPv6 configured and only have IPv4 enabled by default. If you want to enable IPv6 on a VM where it is not already enabled, you can follow the instructions below. Ubuntu 16.04 . To use IPv6 correctly, the following files must be created with the specified content. | /etc/dhcp/dhclient6.conf . timeout 30; . | /etc/cloud/cloud.cfg.d/99-disable-network-config.cfg . network: {config: disabled} . | /etc/network/interfaces.d/lo.cfg . auto lo iface lo inet loopback . | /etc/network/interfaces.d/ens3.cfg . iface ens3 inet6 auto up sleep 5 up dhclient -1 -6 -cf /etc/dhcp/dhclient6.conf -lf /var/lib/dhcp/dhclient6.ens3.leases -v ens3 || true . | . Now that you have created the files, you can reenable the interface: . sudo ifdown ens3 &amp;&amp; sudo ifup ens3 . Once complete, you will have working IPv4 and IPv6 addresses. If you want to automate the actions above, you can add this to the cloud-init part of our heat template (we will go over cloud-init in Step 19: . #cloud-config write_files: - path: /etc/dhcp/dhclient6.conf content: \"timeout 30;\" - path: /etc/cloud/cloud.cfg.d/99-disable-network-config.cfg content: \"network: {config: disabled}\" - path: /etc/network/interfaces.d/lo.cfg content: | auto lo iface lo inet loopback - path: /etc/network/interfaces.d/ens3.cfg content: | iface ens3 inet6 auto up sleep 5 up dhclient -1 -6 -cf /etc/dhcp/dhclient6.conf -lf /var/lib/dhcp/dhclient6.ens3.leases -v ens3 || true runcmd: - [ ifdown, ens3] - [ ifup, ens3] . CentOS 7 . To use IPv6 correctly, the following files must be created with the specified content. | /etc/sysconfig/network . NETWORKING_IPV6=yes . | /etc/sysconfig/network-scripts/ifcfg-eth0 . IPV6INIT=yes DHCPV6C=yes . | . Now that you have created the files, you can reenable the interface: . sudo ifdown eth0 &amp;&amp; sudo ifup eth0 . Once complete, you will have working IPv4 and IPv6 addresses. If you want to automate the actions above, you can add this to the cloud-init part of our heat template (we will go over cloud-init in Step 19: . #cloud-config write_files: - path: /etc/sysconfig/network owner: root:root permissions: '0644' content: | NETWORKING=yes NOZEROCONF=yes NETWORKING_IPV6=yes - path: /etc/sysconfig/network-scripts/ifcfg-eth0 owner: root:root permissions: '0644' content: | DEVICE=\"eth0\" BOOTPROTO=\"dhcp\" ONBOOT=\"yes\" TYPE=\"Ethernet\" USERCTL=\"yes\" PEERDNS=\"yes\" PERSISTENT_DHCLIENT=\"1\" IPV6INIT=yes DHCPV6C=yes runcmd: - [ ifdown, eth0] - [ ifup, eth0] . ",
    "url": "/optimist/guided_tour/step11/#adjustments-to-the-operating-system",
    
    "relUrl": "/optimist/guided_tour/step11/#adjustments-to-the-operating-system"
  },"304": {
    "doc": "11: Prepare access to the Internet; Add IPv6 to your network",
    "title": "External access",
    "content": "Important: This VM can now be reached from anywhere in the world via its IPv6 address (only on the ports that you allowed in the security group). Unlike IPv4, you do not need to assign a floating IP address to be able to reach the VM. If you want to reach the VM with IPv4, you must assign a floating IP address. If you want to test the IPv6 reachability but do not have access to a machine with IPv6, you can use certain web-based tools, for example: https://www.subnetonline.com/pages/ipv6-network-tools/online-ipv6-ping.php . ",
    "url": "/optimist/guided_tour/step11/#external-access",
    
    "relUrl": "/optimist/guided_tour/step11/#external-access"
  },"305": {
    "doc": "11: Prepare access to the Internet; Add IPv6 to your network",
    "title": "Conclusion",
    "content": "In the previous step, you established a connection with IPv4. Access via IPv6 has now also been added. In the next step, the instance from Step 7 will be used as a template and made accessible from outside. ",
    "url": "/optimist/guided_tour/step11/#conclusion",
    
    "relUrl": "/optimist/guided_tour/step11/#conclusion"
  },"306": {
    "doc": "11: Prepare access to the Internet; Add IPv6 to your network",
    "title": "11: Prepare access to the Internet; Add IPv6 to your network",
    "content": " ",
    "url": "/optimist/guided_tour/step11/",
    
    "relUrl": "/optimist/guided_tour/step11/"
  },"307": {
    "doc": "12: A usable VM",
    "title": "Step 12: A usable VM",
    "content": " ",
    "url": "/optimist/guided_tour/step12/#step-12-a-usable-vm",
    
    "relUrl": "/optimist/guided_tour/step12/#step-12-a-usable-vm"
  },"308": {
    "doc": "12: A usable VM",
    "title": "Start",
    "content": "Although you already created a VM in Step 7, the VM was not usable as it was not connected to a network, let alone the internet. Let’s create one that you can actually log on to. ",
    "url": "/optimist/guided_tour/step12/#start",
    
    "relUrl": "/optimist/guided_tour/step12/#start"
  },"309": {
    "doc": "12: A usable VM",
    "title": "Installation",
    "content": "To create this VM, add some parameters to the command we used in Step 7: . $ openstack server create BeispielInstanz --flavor m1.small --key-name Beispiel --image \"Ubuntu 16.04 Xenial Xerus - Latest\" --security-group allow-ssh-from-anywhere --network=BeispielNetzwerk +-----------------------------+---------------------------------------------------------------------------+ | Field | Value | +-----------------------------+---------------------------------------------------------------------------+ | OS-DCF:diskConfig | MANUAL | OS-EXT-AZ:availability_zone | es1 | OS-EXT-STS:power_state | NOSTATE | OS-EXT-STS:task_state | scheduling | OS-EXT-STS:vm_state | building | OS-SRV-USG:launched_at | None | OS-SRV-USG:terminated_at | None | accessIPv4 | | accessIPv6 | | addresses | | config_drive | | created | 2017-12-08T12:52:37Z | flavor | m1.small (b7c4fa0b-7960-4311-a86b-507dbf58e8ac) | hostId | | id | 1de98aa4-7d2b-4427-a8a5-d369ea8bdaf5 | image | Ubuntu 16.04 Xenial Xerus - Latest (82242d21-d990-4fc2-92a5-c7bd7820e790) | key_name | Beispiel | name | BeispielInstanz | progress | 0 | project_id | b15cde70d85749689e08106f973bb002 | properties | | security_groups | name='allow-ssh-from-anywhere' | status | BUILD | updated | 2017-12-08T12:52:37Z | user_id | 9bf501f4c3d14b7eb0f1443efe80f656 | volumes_attached | +-----------------------------+---------------------------------------------------------------------------+ . The following parameters are included: . | --flavor: The flavor of the the VM. You get all available flavors with openstack flavor list. | --key-name: The key to install on the VM. | --image: The operating system image to install on the VM. You can get all available images with openstack image list. | --security-group: Specifies the security group. | --network: Specifies the network to attach the VM to. | . If you want to reach your VM from the internet, you need a floating IP address. You can create one as follows: . $ openstack floating ip create provider +---------------------+--------------------------------------+ | Field | Value | +---------------------+--------------------------------------+ | created_at | 2017-12-08T12:53:37Z | description | | fixed_ip_address | None | floating_ip_address | 185.116.245.65 | floating_network_id | 54258498-a513-47da-9369-1a644e4be692 | id | 84eca140-9ac1-42c3-baf6-860ba920a23c | name | None | port_id | None | project_id | b15cde70d85749689e08106f973bb002 | revision_number | 0 | router_id | None | status | DOWN | updated_at | 2017-12-08T12:53:37Z | +---------------------+--------------------------------------+ . The created IP must be associated with your VM: . openstack server add floating ip BeispielInstanz 185.116.245.145 . ",
    "url": "/optimist/guided_tour/step12/#installation",
    
    "relUrl": "/optimist/guided_tour/step12/#installation"
  },"310": {
    "doc": "12: A usable VM",
    "title": "Usage",
    "content": "You should now have a reachable VM. To check if all worked correctly, log in to your VM with SSH. IMPORTANT: You can only log in if the specified ssh key exists and is accessible (if it does not work, follow the instructions in Step 6). $ ssh ubuntu@185.116.245.145 The authenticity of host '185.116.245.145 (185.116.245.145)' can't be established. ECDSA key fingerprint is SHA256:kbSkm8eJA0748911RkbWK2/pBVQOjJBASD1oOOXalk. Are you sure you want to continue connecting (yes/no)? yes Warning: Permanently added '185.116.245.145' (ECDSA) to the list of known hosts. Enter passphrase for key '/Users/ubuntu/.ssh/id_rsa': . ",
    "url": "/optimist/guided_tour/step12/#usage",
    
    "relUrl": "/optimist/guided_tour/step12/#usage"
  },"311": {
    "doc": "12: A usable VM",
    "title": "Clean-Up",
    "content": "If you want to delete all components you just created, you must delete them in the following order. If you do not delete them in the correct order, you will be unable to delete components that other resources depend on. | Instance . | openstack server delete BeispielInstanz | . | Floating-IP . | openstack floating ip delete 185.116.245.145 | . | Router Port . | openstack port delete BeispielPort | . | Router . | openstack router delete BeispielRouter | . | Subnet . | openstack subnet delete BeispielSubnet | . | Network . | openstack network delete BeispielNetzwerk | . | . ",
    "url": "/optimist/guided_tour/step12/#clean-up",
    
    "relUrl": "/optimist/guided_tour/step12/#clean-up"
  },"312": {
    "doc": "12: A usable VM",
    "title": "Conclusion",
    "content": "You have now created a VM based on your knowledge from steps 7 to 11, you can reach it from the internet, and have logged in with SSH. In the next step, you will break away from individual instances and will create a stack. ",
    "url": "/optimist/guided_tour/step12/#conclusion",
    
    "relUrl": "/optimist/guided_tour/step12/#conclusion"
  },"313": {
    "doc": "12: A usable VM",
    "title": "12: A usable VM",
    "content": " ",
    "url": "/optimist/guided_tour/step12/",
    
    "relUrl": "/optimist/guided_tour/step12/"
  },"314": {
    "doc": "13: The structured way to create an instance (with stacks)",
    "title": "Step 13: The structured way to create an instance (with stacks)",
    "content": " ",
    "url": "/optimist/guided_tour/step13/#step-13-the-structured-way-to-create-an-instance-with-stacks",
    
    "relUrl": "/optimist/guided_tour/step13/#step-13-the-structured-way-to-create-an-instance-with-stacks"
  },"315": {
    "doc": "13: The structured way to create an instance (with stacks)",
    "title": "Start",
    "content": "Previously, you created a VM, a security group, and a virtual network separately. Now you will learn how to create all of them at once in an integrated way. This requires a pre-installed python-heatclient, which was already installed in Step 4: Our way to the console. ",
    "url": "/optimist/guided_tour/step13/#start",
    
    "relUrl": "/optimist/guided_tour/step13/#start"
  },"316": {
    "doc": "13: The structured way to create an instance (with stacks)",
    "title": "Installation",
    "content": "Instead of creating individual manually, any OpenStack resources (e.g. instances, networks, routers, security groups) can also be operated in a defined network; a stack (or heat stack). This makes it easy to compose an entire setup, which you can then easily create and delete at will. In this step, you will use a pre-made heat template and later, will learn how to write one yourself. Everything you created in steps 9 through 11 are easily expressed in a single template. Let’s start with an example template. This template creates a stack that includes a VM, two security groups, a virtual network (including router, port, and subnet), and a floating-IP. When you create the stack, make sure that you are in the same directory as the template: . $ openstack stack create -t SingleServer.yaml --parameter key_name=Beispiel SingleServer --wait 2017-12-08 13:13:43Z [SingleServer]: CREATE_IN_PROGRESS Stack CREATE started 2017-12-08 13:13:44Z [SingleServer.router]: CREATE_IN_PROGRESS state changed 2017-12-08 13:13:45Z [SingleServer.enable_traffic]: CREATE_IN_PROGRESS state changed 2017-12-08 13:13:45Z [SingleServer.enable_traffic]: CREATE_COMPLETE state changed 2017-12-08 13:13:46Z [SingleServer.internal_network_id]: CREATE_IN_PROGRESS state changed 2017-12-08 13:13:46Z [SingleServer.router]: CREATE_COMPLETE state changed 2017-12-08 13:13:46Z [SingleServer.internal_network_id]: CREATE_COMPLETE state changed 2017-12-08 13:13:47Z [SingleServer.enable_ssh]: CREATE_IN_PROGRESS state changed 2017-12-08 13:13:47Z [SingleServer.subnet]: CREATE_IN_PROGRESS state changed 2017-12-08 13:13:47Z [SingleServer.enable_ssh]: CREATE_COMPLETE state changed 2017-12-08 13:13:48Z [SingleServer.start-config]: CREATE_IN_PROGRESS state changed 2017-12-08 13:13:48Z [SingleServer.subnet]: CREATE_COMPLETE state changed 2017-12-08 13:13:49Z [SingleServer.router_subnet_bridge]: CREATE_IN_PROGRESS state changed 2017-12-08 13:13:49Z [SingleServer.port]: CREATE_IN_PROGRESS state changed 2017-12-08 13:13:50Z [SingleServer.start-config]: CREATE_COMPLETE state changed 2017-12-08 13:13:50Z [SingleServer.port]: CREATE_COMPLETE state changed 2017-12-08 13:13:50Z [SingleServer.host]: CREATE_IN_PROGRESS state changed 2017-12-08 13:13:52Z [SingleServer.router_subnet_bridge]: CREATE_COMPLETE state changed 2017-12-08 13:13:53Z [SingleServer.floating_ip]: CREATE_IN_PROGRESS state changed 2017-12-08 13:13:55Z [SingleServer.floating_ip]: CREATE_COMPLETE state changed 2017-12-08 13:14:05Z [SingleServer.host]: CREATE_COMPLETE state changed 2017-12-08 13:14:06Z [SingleServer]: CREATE_COMPLETE Stack CREATE completed successfully +---------------------+-------------------------------------------------+ | Field | Value | +---------------------+-------------------------------------------------+ | id | 0f5cdf0e-24cc-4292-a0bc-adf2e9f8618a | stack_name | SingleServer | description | A simple template to deploy your first instance | creation_time | 2017-12-08T13:13:42Z | updated_time | None | stack_status | CREATE_COMPLETE | stack_status_reason | Stack CREATE completed successfully | +---------------------+-------------------------------------------------+ . Here is a short explanation of the executed command: . The command openstack stack create creates the stack, according to the template defined with -t SingleServer.yaml . We set the parameter key_name with --parameter key_name=BEISPIEL to fill the key_name parameter with BEISPIEL (in this template that installs our BEISPIEL key into your VM). We also named our stack SingleServer. Finally, we used the --wait option to wait and observe the creation process. If you do not add this option, the command completes immediately while the creation process continues in the background. Once the command has completed, you should be able to connect to your VM. First, we acquire the floating IP of the VM: . $ openstack stack output show 0f5cdf0e-24cc-4292-a0bc-adf2e9f8618a instance_fip +--------------+---------------------------------+ | Field | Value | +--------------+---------------------------------+ | description | External IP address of instance | output_key | instance_fip | output_value | 185.116.245.70 | +--------------+---------------------------------+ . Now you can log in to your VM: . $ ssh ubuntu@185.116.245.70 The authenticity of host '185.116.245.70 (185.116.245.70)' can't be established. ECDSA key fingerprint is SHA256:kbSkm8eJA0748911RkbWK2/pBVQOjJBASD1oOOXalk. Are you sure you want to continue connecting (yes/no)? yes Warning: Permanently added '185.116.245.70' (ECDSA) to the list of known hosts. Enter passphrase for key '/Users/ubuntu/.ssh/id_rsa': . ",
    "url": "/optimist/guided_tour/step13/#installation",
    
    "relUrl": "/optimist/guided_tour/step13/#installation"
  },"317": {
    "doc": "13: The structured way to create an instance (with stacks)",
    "title": "Conclusion",
    "content": "Using a heat stack, you combined steps 9 through 11 in a single command. In the following steps we will look into Heat in more detail. ",
    "url": "/optimist/guided_tour/step13/#conclusion",
    
    "relUrl": "/optimist/guided_tour/step13/#conclusion"
  },"318": {
    "doc": "13: The structured way to create an instance (with stacks)",
    "title": "13: The structured way to create an instance (with stacks)",
    "content": " ",
    "url": "/optimist/guided_tour/step13/",
    
    "relUrl": "/optimist/guided_tour/step13/"
  },"319": {
    "doc": "14: Your first steps with HEAT",
    "title": "Step 14: Your first steps with HEAT",
    "content": " ",
    "url": "/optimist/guided_tour/step14/#step-14-your-first-steps-with-heat",
    
    "relUrl": "/optimist/guided_tour/step14/#step-14-your-first-steps-with-heat"
  },"320": {
    "doc": "14: Your first steps with HEAT",
    "title": "Start",
    "content": "In the previous steps, you used a pre-made heat template. The next step is to understand how heat templates are built. ",
    "url": "/optimist/guided_tour/step14/#start",
    
    "relUrl": "/optimist/guided_tour/step14/#start"
  },"321": {
    "doc": "14: Your first steps with HEAT",
    "title": "The Template",
    "content": "Every heat template follows the following structure: . heat_template_version:   description: # The template description   parameter_groups: # Group definitions and their order   parameters: # Parameter definitions   resources: # Resource definitions   outputs: # Definitions of possible outputs   conditions: # Definitions of conditions . ",
    "url": "/optimist/guided_tour/step14/#the-template",
    
    "relUrl": "/optimist/guided_tour/step14/#the-template"
  },"322": {
    "doc": "14: Your first steps with HEAT",
    "title": "Heat Template Version",
    "content": "Template versions cannot be chosen arbitrarily as they have fixed specifications that define which commands are available to use. You can use any of the following versions, although we recommended using the latest one: . | 2013-05-23 | 2014-10-16 | 2015-04-30 | 2015-10-15 | 2016-04-08 | 2016-10-14 | 2017-02-24 | . ",
    "url": "/optimist/guided_tour/step14/#heat-template-version",
    
    "relUrl": "/optimist/guided_tour/step14/#heat-template-version"
  },"323": {
    "doc": "14: Your first steps with HEAT",
    "title": "Description",
    "content": "The description is an optional section that describes the stack. We recommend adding a description of how to use the template, what it is about, any other useful information. This makes it easier to share with others and However, it makes sense, because the template can be described in its basic features and any special features can be pointed out directly. You can also add comments by starting them with the # character. It can be used to temporarily disable lines, or to add more documentation to the template. ",
    "url": "/optimist/guided_tour/step14/#description",
    
    "relUrl": "/optimist/guided_tour/step14/#description"
  },"324": {
    "doc": "14: Your first steps with HEAT",
    "title": "Parameter Groups",
    "content": "In this section, you can specify the parameters, how they should group, and their order. The groups are divided into a list containing single parameters. Every parameter should only have one group to avoid possible errors later. Each parameter group is structured as follows: . parameter_groups: - label: &lt;name of the group&gt; description: &lt;description of the group&gt; parameters: - &lt;name of the parameter&gt; - &lt;name of the parameter&gt; . | label: Name of the group | description: Description of the group | parameter: A list of all parameters in this group | name of the parameter: Name of the parameter that was defined in the parameter section | . ",
    "url": "/optimist/guided_tour/step14/#parameter-groups",
    
    "relUrl": "/optimist/guided_tour/step14/#parameter-groups"
  },"325": {
    "doc": "14: Your first steps with HEAT",
    "title": "Parameters",
    "content": "In this section you can specify the required parameters for your template. Parameters are typically used to make it easy to change certain parts of the template, for example, which SSH key is used. Each parameter is defined separately, starting with the name, with the attributes defined underneath: .  parameters: &lt;Parameter Name&gt;: type: &lt;string | number | json | comma_delimited_list | boolean&gt; label: &lt;Name of the parameter&gt; description: &lt;description of the parameter&gt; default: &lt;default of the parameter&gt; hidden: &lt;true | false&gt; constraints: &lt;constraints for the parameter&gt; immutable: &lt;true | false&gt; . | Parameter Name: Name of the parameter | type: The parameter type (string, number, json, comma_delimited_list, boolean) | label: The parameter name (optional) | description: Description of the parameter (optional) | default: Default value of the parameter. Will be used if the parameter isn’t defined (optional) | hidden:  To hide the parameter in the creation process, you can set hidden: true as a parameter (Optional, and set to false by default) | constraints: You can set a list of constraints. If these aren’t fulfilled, the stack creation will fail. | immutable: If this parameter is set to true, the parameter cannot be changed during a stack update. (This will generate an error if attempted) | . ",
    "url": "/optimist/guided_tour/step14/#parameters",
    
    "relUrl": "/optimist/guided_tour/step14/#parameters"
  },"326": {
    "doc": "14: Your first steps with HEAT",
    "title": "Resources",
    "content": "This block specifies the resources that will be created, with every resource in its own sub block: . resources: &lt;ID of the resource&gt;: type: &lt;resource type&gt; properties: &lt;name of the property&gt;: &lt;value of the property&gt; metadata: &lt;specific metatdata&gt; depends_on: &lt;Resource ID or a list of resources&gt; update_policy: &lt;update rule&gt; deletion_policy: &lt;deletion rule&gt; external_id: &lt;external resource id&gt; condition: &lt;condition name&gt; . | ID of the resource: Must be unique | type: Resource type, for example: OS::NEUTRON::SecurityGroup (for a security group) (required) | properties: A list of properties for resources (optional) | metadata: Metadata for the respective resource (optional) | depends_on: Various dependencies to other resources can be stored here (optional) | update_policy: Update rules can be defined here. As a prerequisite, ensure that the corresponding resource also supports this (optional). | deletion_policy: Specifies rules for deletion. The options are Delete, Retain and Snapshot. With heat_template_version 2016-10-14, you can also enter them in lowercase. | external_id: You can use external resource IDs, if required | condition: You can set specific conditions which need to be met to allow this resource to be created. (optional) | . ",
    "url": "/optimist/guided_tour/step14/#resources",
    
    "relUrl": "/optimist/guided_tour/step14/#resources"
  },"327": {
    "doc": "14: Your first steps with HEAT",
    "title": "Output",
    "content": "With the output block, you can specify the parameters to be shown after creation. Examples include the IP address of a VM, or the URL of a deployed web application. Outputs are specified in sub blocks like this: . outputs: &lt;name of the output&gt;: description: &lt;description&gt; value: &lt;value of the output&gt; condition: &lt;name of the condition&gt; . | name of the output: Must be unique | description: A description can be stored here. (optional) | value: Value of the output (mandatory) | condition: Possible conditions can be specified here (optional) | . ",
    "url": "/optimist/guided_tour/step14/#output",
    
    "relUrl": "/optimist/guided_tour/step14/#output"
  },"328": {
    "doc": "14: Your first steps with HEAT",
    "title": "Conditions",
    "content": "Conditions based on the parameters entered by the user when creating or updating the stack can also be specified in a block. You can set conditions, and if they are not fulfilled, the stack creation fails. The conditions can be linked to resources, resource properties and outputs. conditions: &lt;name of condition 1&gt;: {term1} &lt;name of condition 2&gt;: {term2} . | name of condition: Must be unique | term: true or false are expected as a result | . ",
    "url": "/optimist/guided_tour/step14/#conditions",
    
    "relUrl": "/optimist/guided_tour/step14/#conditions"
  },"329": {
    "doc": "14: Your first steps with HEAT",
    "title": "Conclusion",
    "content": "You have learned the basic structure of a heat template and can now begin creating your own. With this knowledge, you will create your own heat template in the next step. ",
    "url": "/optimist/guided_tour/step14/#conclusion",
    
    "relUrl": "/optimist/guided_tour/step14/#conclusion"
  },"330": {
    "doc": "14: Your first steps with HEAT",
    "title": "14: Your first steps with HEAT",
    "content": " ",
    "url": "/optimist/guided_tour/step14/",
    
    "relUrl": "/optimist/guided_tour/step14/"
  },"331": {
    "doc": "15: The first heat template",
    "title": "Step 15: The first heat template",
    "content": " ",
    "url": "/optimist/guided_tour/step15/#step-15-the-first-heat-template",
    
    "relUrl": "/optimist/guided_tour/step15/#step-15-the-first-heat-template"
  },"332": {
    "doc": "15: The first heat template",
    "title": "Start",
    "content": "In the previous step, you learned the basic layout of a heat template. Now you can create your own. ",
    "url": "/optimist/guided_tour/step15/#start",
    
    "relUrl": "/optimist/guided_tour/step15/#start"
  },"333": {
    "doc": "15: The first heat template",
    "title": "The first template",
    "content": "As stated earlier, your template needs to start with a version definition. Version 2016-10-14 is used for the below example: . heat_template_version: 2016-10-14 . Although it is optional, it is best practice to add a description to your template. heat_template_version: 2016-10-14   description: A simple template to deploy a vm . Next, add the resource “Instanz”. Be sure to pay attention to the structure of our template and to indent the “Instanz” under resources. To indent, use 4 spaces (not tabs). If you use tabs or an inconsistent amount of spaces, it will cause errors that may be hard to locate. Your template should now look like this: . heat_template_version: 2016-10-14 description: A simple template to deploy a vm resources: Instanz: . Next, define the resource type. A detailed list of all types is available in the official OpenStack documentation . In our example, you can define Instanz as a VM: . heat_template_version: 2016-10-14 description: A simple template to deploy a vm resources: Instanz: type: OS::Nova::Server . Now that you have defined the type, you should next define its properties. Let’s define the key, image, and the flavor: . heat_template_version: 2016-10-14 description: A simple template to deploy a vm resources: Instanz: type: OS::Nova::Server properties: key_name: BeispielKey image: Ubuntu 16.04 Xenial Xerus - Latest flavor: m1.small . ",
    "url": "/optimist/guided_tour/step15/#the-first-template",
    
    "relUrl": "/optimist/guided_tour/step15/#the-first-template"
  },"334": {
    "doc": "15: The first heat template",
    "title": "Conclusion",
    "content": "You have now defined a template that creates a single VM instance. If you like, you can run it like you did previously in Step 13: “The structured way to create an instance (with stacks)”. ",
    "url": "/optimist/guided_tour/step15/#conclusion",
    
    "relUrl": "/optimist/guided_tour/step15/#conclusion"
  },"335": {
    "doc": "15: The first heat template",
    "title": "15: The first heat template",
    "content": " ",
    "url": "/optimist/guided_tour/step15/",
    
    "relUrl": "/optimist/guided_tour/step15/"
  },"336": {
    "doc": "16: Get to know HEAT better",
    "title": "Step 16: Get to know HEAT better",
    "content": " ",
    "url": "/optimist/guided_tour/step16/#step-16-get-to-know-heat-better",
    
    "relUrl": "/optimist/guided_tour/step16/#step-16-get-to-know-heat-better"
  },"337": {
    "doc": "16: Get to know HEAT better",
    "title": "Start",
    "content": "At first, it may look like that creating a VM with a Heat template, or alternatively with the OpenStack client takes the same amount of time. While this is true if you only want to create the VM once, the advantage of using Heat is that you can reuse the template. Now that you have a simple template, you can familiarise yourself with Heat by adding a variable parameter to your template. ",
    "url": "/optimist/guided_tour/step16/#start",
    
    "relUrl": "/optimist/guided_tour/step16/#start"
  },"338": {
    "doc": "16: Get to know HEAT better",
    "title": "Parameters",
    "content": "In this example, you will add a parameter for the SSH key. The advantage of this is that you can use a VM with different keys without changing the template. Define the parameter and its type. The correct type for what we want to accomplish here is string: . heat_template_version: 2014-10-16   parameters: key_name: type: string . Now that you have defined the first parameter, you can add the same resource to your template as follows: . heat_template_version: 2014-10-16 parameters: key_name: type: string resources: Instanz: type: OS::Nova::Server properties: key_name: Beispiel image: Ubuntu 16.04 Xenial Xerus - Latest flavor: m1.small . Now you can actually use your parameter, and can replace Beispiel with your parameter. You can do this with the get_param syntax (to get the parameter). The template is now ready to use and you can define the key_name from the command line as demonstrated in our previous command: . heat_template_version: 2014-10-16 parameters: key_name: type: string resources: Instanz: type: OS::Nova::Server properties: key_name: { get_param: key_name } image: Ubuntu 16.04 Xenial Xerus - Latest flavor: m1.small . ",
    "url": "/optimist/guided_tour/step16/#parameters",
    
    "relUrl": "/optimist/guided_tour/step16/#parameters"
  },"339": {
    "doc": "16: Get to know HEAT better",
    "title": "Conclusion",
    "content": "You have now added a variable parameter to your template. In the next step, you will add the network. ",
    "url": "/optimist/guided_tour/step16/#conclusion",
    
    "relUrl": "/optimist/guided_tour/step16/#conclusion"
  },"340": {
    "doc": "16: Get to know HEAT better",
    "title": "16: Get to know HEAT better",
    "content": " ",
    "url": "/optimist/guided_tour/step16/",
    
    "relUrl": "/optimist/guided_tour/step16/"
  },"341": {
    "doc": "17: The network in Heat",
    "title": "Step 17: The network in Heat",
    "content": " ",
    "url": "/optimist/guided_tour/step17/#step-17-the-network-in-heat",
    
    "relUrl": "/optimist/guided_tour/step17/#step-17-the-network-in-heat"
  },"342": {
    "doc": "17: The network in Heat",
    "title": "Start",
    "content": "Now that you have a simple template with a parameter, you can add the network. ",
    "url": "/optimist/guided_tour/step17/#start",
    
    "relUrl": "/optimist/guided_tour/step17/#start"
  },"343": {
    "doc": "17: The network in Heat",
    "title": "The template",
    "content": "Continue using the template you previously created. First, add a new parameter, the ID of the external network, and name it public_network_id,. Also define a default provider: . heat_template_version: 2014-10-16 parameters: key_name: type: string public_network_id: type: string default: provider resources: Instanz: type: OS::Nova::Server properties: key_name: { get_param: key_name } image: Ubuntu 16.04 Xenial Xerus - Latest flavor: m1.small . ",
    "url": "/optimist/guided_tour/step17/#the-template",
    
    "relUrl": "/optimist/guided_tour/step17/#the-template"
  },"344": {
    "doc": "17: The network in Heat",
    "title": "Network",
    "content": "Next, add the network. Like the VM, the network is a resource, it will be added to that block. The type for network resources is OS::Neutron::Net . heat_template_version: 2014-10-16 parameters: key_name: type: string public_network_id: type: string default: provider resources: Instanz: type: OS::Nova::Server properties: key_name: { get_param: key_name } image: Ubuntu 16.04 Xenial Xerus - Latest flavor: m1.small Netzwerk: type: OS::Neutron::Net properties: name: BeispielNetzwerk . ",
    "url": "/optimist/guided_tour/step17/#network",
    
    "relUrl": "/optimist/guided_tour/step17/#network"
  },"345": {
    "doc": "17: The network in Heat",
    "title": "The port",
    "content": "Now that you have defined a network you can add the port, which is a resource with type OS::Neutron::Port. To ensure that this port is used by your VM, add the networks property to it. Define a port property that then will use the get_resource function to link it to the Port. Furthermore, you want to link the port to the network by adding a network property that also uses the get_resource function to link it to the Netzwerk. At this point, your template looks like this: . heat_template_version: 2014-10-16 parameters: key_name: type: string public_network_id: type: string default: provider resources: Instanz: type: OS::Nova::Server properties: key_name: { get_param: key_name } image: Ubuntu 16.04 Xenial Xerus - Latest flavor: m1.small networks: - port: {get_resource: Port } Netzwerk: type: OS::Neutron::Net properties: name: BeispielNetzwerk   Port: type: OS::Neutron::Port properties: network: { get_resource: Netzwerk } . ",
    "url": "/optimist/guided_tour/step17/#the-port",
    
    "relUrl": "/optimist/guided_tour/step17/#the-port"
  },"346": {
    "doc": "17: The network in Heat",
    "title": "The router",
    "content": "Your network needs a Router resource, with the type OS::Neutron::Router. With this type it is important to define the external network it will use: . heat_template_version: 2014-10-16 parameters: key_name: type: string public_network_id: type: string default: provider resources: Instanz: type: OS::Nova::Server properties: key_name: { get_param: key_name } image: Ubuntu 16.04 Xenial Xerus - Latest flavor: m1.small networks: - port: {get_resource: Port } Netzwerk: type: OS::Neutron::Net properties: name: BeispielNetzwerk Port: type: OS::Neutron::Port properties: network: { get_resource: Netzwerk }   Router: type: OS::Neutron::Router properties: external_gateway_info: { \"network\": { get_param: public_network_id } name: BeispielRouter . ",
    "url": "/optimist/guided_tour/step17/#the-router",
    
    "relUrl": "/optimist/guided_tour/step17/#the-router"
  },"347": {
    "doc": "17: The network in Heat",
    "title": "The subnet",
    "content": "Next, define a subnet for your network. This is the Subnet resource with type OS::Neutron::Subnet. It is in the subnet that you define IP information like nameserver(s), the IP version, the IP range, and other IP-related settings: . heat_template_version: 2014-10-16 parameters: key_name: type: string public_network_id: type: string default: provider resources: Instanz: type: OS::Nova::Server properties: key_name: { get_param: key_name } image: Ubuntu 16.04 Xenial Xerus - Latest flavor: m1.small networks: - port: {get_resource: Port } Netzwerk: type: OS::Neutron::Net properties: name: BeispielNetzwerk Port: type: OS::Neutron::Port properties: network: { get_resource: Netzwerk } Router: type: OS::Neutron::Router properties: external_gateway_info: { \"network\": { get_param: public_network_id } name: BeispielRouter   Subnet: type: OS::Neutron::Subnet properties: name: BeispielSubnet dns_nameservers: - 8.8.8.8 - 8.8.4.4 network: { get_resource: Netzwerk } ip_version: 4 cidr: 10.0.0.0/24 allocation_pools: - { start: 10.0.0.10, end: 10.0.0.250 } . ",
    "url": "/optimist/guided_tour/step17/#the-subnet",
    
    "relUrl": "/optimist/guided_tour/step17/#the-subnet"
  },"348": {
    "doc": "17: The network in Heat",
    "title": "Subnet bridge",
    "content": "Finally, define a subnet bridge with type OS::Neutron::RouterInterface. This associates the subnet with the router to ensure that VMs in that subnet will use the router. Additionally, you can define the depends_on property, which ensures that the subnet bridge will only be created if Subnet is available: . heat_template_version: 2014-10-16 parameters: key_name: type: string public_network_id: type: string default: provider resources: Instanz: type: OS::Nova::Server properties: key_name: { get_param: key_name } image: Ubuntu 16.04 Xenial Xerus - Latest flavor: m1.small networks: - port: {get_resource: Port } Netzwerk: type: OS::Neutron::Net properties: name: BeispielNetzwerk Port: type: OS::Neutron::Port properties: network: { get_resource: Netzwerk } Router: type: OS::Neutron::Router properties: external_gateway_info: { \"network\": { get_param: public_network_id } name: BeispielRouter Subnet: type: OS::Neutron::Subnet properties: name: BeispielSubnet dns_nameservers: - 8.8.8.8 - 8.8.4.4 network: { get_resource: Netzwerk } ip_version: 4 cidr: 10.0.0.0/24 allocation_pools: - { start: 10.0.0.10, end: 10.0.0.250 }   Router_Subnet_Bridge: type: OS::Neutron::RouterInterface depends_on: Subnet properties: router: { get_resource: Router } subnet: { get_resource: Subnet } . ",
    "url": "/optimist/guided_tour/step17/#subnet-bridge",
    
    "relUrl": "/optimist/guided_tour/step17/#subnet-bridge"
  },"349": {
    "doc": "17: The network in Heat",
    "title": "Conclusion",
    "content": "You have now created the full network. When the stack is created, it will create a VM and all the required components to give it connectivity. The next step is to assign a public IP address to the instance. ",
    "url": "/optimist/guided_tour/step17/#conclusion",
    
    "relUrl": "/optimist/guided_tour/step17/#conclusion"
  },"350": {
    "doc": "17: The network in Heat",
    "title": "17: The network in Heat",
    "content": " ",
    "url": "/optimist/guided_tour/step17/",
    
    "relUrl": "/optimist/guided_tour/step17/"
  },"351": {
    "doc": "18: Making your VM reachable via IPv4",
    "title": "Step 18: Making your VM reachable via IPv4",
    "content": " ",
    "url": "/optimist/guided_tour/step18/#step-18-making-your-vm-reachable-via-ipv4",
    
    "relUrl": "/optimist/guided_tour/step18/#step-18-making-your-vm-reachable-via-ipv4"
  },"352": {
    "doc": "18: Making your VM reachable via IPv4",
    "title": "Start",
    "content": "Now that your template has defined the full network and can reach the internet, you next need to ensure that the VM is reachable externally. ",
    "url": "/optimist/guided_tour/step18/#start",
    
    "relUrl": "/optimist/guided_tour/step18/#start"
  },"353": {
    "doc": "18: Making your VM reachable via IPv4",
    "title": "Floating-IP",
    "content": "Define a floating public IPv4 address, which is a resource with type OS::Neutron::FloatingIP. Note that it is important to define the external network this IP is assigned from and the port this IP leads to: . heat_template_version: 2014-10-16 parameters: key_name: type: string public_network_id: type: string default: provider resources: Instanz: type: OS::Nova::Server properties: key_name: { get_param: key_name } image: Ubuntu 16.04 Xenial Xerus - Latest flavor: m1.small networks: - port: {get_resource: Port } Netzwerk: type: OS::Neutron::Net properties: name: BeispielNetzwerk Port: type: OS::Neutron::Port properties: network: { get_resource: Netzwerk } Router: type: OS::Neutron::Router properties: external_gateway_info: { \"network\": { get_param: public_network_id } name: BeispielRouter Subnet: type: OS::Neutron::Subnet properties: name: BeispielSubnet dns_nameservers: - 8.8.8.8 - 8.8.4.4 network: { get_resource: Netzwerk } ip_version: 4 cidr: 10.0.0.0/24 allocation_pools: - { start: 10.0.0.10, end: 10.0.0.250 } Router_Subnet_Bridge: type: OS::Neutron::RouterInterface depends_on: Subnet properties: router: { get_resource: Router } subnet: { get_resource: Subnet }   Floating_IP: type: OS::Neutron::FloatingIP properties: floating_network: { get_param: public_network_id } port_id: { get_resource: Port } . ",
    "url": "/optimist/guided_tour/step18/#floating-ip",
    
    "relUrl": "/optimist/guided_tour/step18/#floating-ip"
  },"354": {
    "doc": "18: Making your VM reachable via IPv4",
    "title": "Security Groups",
    "content": "If you create a stack as outlined above, the VM would start but it would not be reachable. As previously stated, VMs do not receive traffic without a security group in place which explicitly allows this. So, the logical next step is to create a resource with type OS::Neutron::SecurityGroup. The security group must be defined to use the Port. On the resource itself, the rules are specified. These rules include the direction, port range, remote IP prefix, and protocol that these rules intend to allow. heat_template_version: 2014-10-16 parameters: key_name: type: string public_network_id: type: string default: provider resources: Instanz: type: OS::Nova::Server properties: key_name: { get_param: key_name } image: Ubuntu 16.04 Xenial Xerus - Latest flavor: m1.small networks: - port: {get_resource: Port } Netzwerk: type: OS::Neutron::Net properties: name: BeispielNetzwerk Port: type: OS::Neutron::Port properties: network: { get_resource: Netzwerk } security_groups: { get_resource: Sec_SSH } Router: type: OS::Neutron::Router properties: external_gateway_info: { \"network\": { get_param: public_network_id } name: BeispielRouter Subnet: type: OS::Neutron::Subnet properties: name: BeispielSubnet dns_nameservers: - 8.8.8.8 - 8.8.4.4 network: { get_resource: Netzwerk } ip_version: 4 cidr: 10.0.0.0/24 allocation_pools: - { start: 10.0.0.10, end: 10.0.0.250 } Router_Subnet_Bridge: type: OS::Neutron::RouterInterface depends_on: Subnet properties: router: { get_resource: Router } subnet: { get_resource: Subnet } Floating_IP: type: OS::Neutron::FloatingIP properties: floating_network: { get_param: public_network_id } port_id: { get_resource: Port }   Sec_SSH: type: OS::Neutron::SecurityGroup properties: description: Diese Security Group erlaubt den eingehenden SSH-Traffic über Port22 und ICMP name: Ermöglicht SSH (Port22) und ICMP rules: - { direction: ingress, remote_ip_prefix: 0.0.0.0/0, port_range_min: 22, port_range_max: 22, protocol:tcp } - { direction: ingress, remote_ip_prefix: 0.0.0.0/0, protocol: icmp } . ",
    "url": "/optimist/guided_tour/step18/#security-groups",
    
    "relUrl": "/optimist/guided_tour/step18/#security-groups"
  },"355": {
    "doc": "18: Making your VM reachable via IPv4",
    "title": "Conclusion",
    "content": "You can now create a stack that contains a single reachable instance. In the next step, you will customize the instance using CloudConfig. ",
    "url": "/optimist/guided_tour/step18/#conclusion",
    
    "relUrl": "/optimist/guided_tour/step18/#conclusion"
  },"356": {
    "doc": "18: Making your VM reachable via IPv4",
    "title": "18: Making your VM reachable via IPv4",
    "content": " ",
    "url": "/optimist/guided_tour/step18/",
    
    "relUrl": "/optimist/guided_tour/step18/"
  },"357": {
    "doc": "19: Add IPv6 to your template",
    "title": "Step 19: Add IPv6 to your template",
    "content": " ",
    "url": "/optimist/guided_tour/step19/#step-19-add-ipv6-to-your-template",
    
    "relUrl": "/optimist/guided_tour/step19/#step-19-add-ipv6-to-your-template"
  },"358": {
    "doc": "19: Add IPv6 to your template",
    "title": "Start",
    "content": "At this point, you have a VM that is reachable with IPv4. The next step is to add IPv6 support. ",
    "url": "/optimist/guided_tour/step19/#start",
    
    "relUrl": "/optimist/guided_tour/step19/#start"
  },"359": {
    "doc": "19: Add IPv6 to your template",
    "title": "CloudConfig",
    "content": "Cloud config has resource type OS::HEAT::CloudConfig. Cloud config hs a variety of uses, but in this case it will be used to configure IPv6. You will continue using the template that you have been working on in the previous steps. heat_template_version: 2014-10-16 parameters: key_name: type: string public_network_id: type: string default: provider resources: Instanz: type: OS::Nova::Server properties: key_name: { get_param: key_name } image: Ubuntu 16.04 Xenial Xerus - Latest flavor: m1.small networks: - port: {get_resource: Port }   Instanz-Config: type: OS::Heat::CloudConfig properties: cloud_config: write_files: - path: /etc/dhcp/dhclient6.conf content: \"timeout 30;\" - path: /etc/cloud/cloud.cfg.d/99-disable-network-config.cfg content: \"network: {config: disabled}\" - path: /etc/network/interfaces.d/lo.cfg content: | auto lo iface lo inet loopback - path: /etc/network/interfaces.d/ens3.cfg content: | iface ens3 inet6 auto up sleep 5 up dhclient -1 -6 -cf /etc/dhcp/dhclient6.conf -lf /var/lib/dhcp/dhclient6.ens3.leases -v ens3 || true Netzwerk: type: OS::Neutron::Net properties: name: BeispielNetzwerk Port: type: OS::Neutron::Port properties: network: { get_resource: Netzwerk } security_groups: { get_resource: Sec_SSH } Router: type: OS::Neutron::Router properties: external_gateway_info: { \"network\": { get_param: public_network_id } name: BeispielRouter Subnet: type: OS::Neutron::Subnet properties: name: BeispielSubnet dns_nameservers: - 8.8.8.8 - #MussNochEingetragenWerden network: { get_resource: Netzwerk } ip_version: 4 cidr: 10.0.0.0/24 allocation_pools: - { start: 10.0.0.10, end: 10.0.0.250 } Router_Subnet_Bridge: type: OS::Neutron::RouterInterface depends_on: Subnet properties: router: { get_resource: Router } subnet: { get_resource: Subnet } Floating_IP: type: OS::Neutron::FloatingIP properties: floating_network: { get_param: public_network_id } port_id: { get_resource: Port } Sec_SSH: type: OS::Neutron::SecurityGroup properties: description: Diese Security Group erlaubt den eingehenden SSH-Traffic über Port22 und ICMP name: Ermöglicht SSH (Port22) und ICMP rules: - { direction: ingress, remote_ip_prefix: 0.0.0.0/0, port_range_min: 22, port_range_max: 22, protocol:tcp } - { direction: ingress, remote_ip_prefix: 0.0.0.0/0, protocol: icmp } . The files have been created and the appropriate content added. As stated in Step 11: Prepare access to the internet: Add IPv6 to our network, the interface still needs to be restarted using the command runcmd. heat_template_version: 2014-10-16 parameters: key_name: type: string public_network_id: type: string default: provider resources: Instanz: type: OS::Nova::Server properties: key_name: { get_param: key_name } image: Ubuntu 16.04 Xenial Xerus - Latest flavor: m1.small networks: - port: {get_resource: Port } Instanz-Config: type: OS::Heat::CloudConfig properties: cloud_config: write_files: - path: /etc/dhcp/dhclient6.conf content: \"timeout 30;\" - path: /etc/cloud/cloud.cfg.d/99-disable-network-config.cfg content: \"network: {config: disabled}\" - path: /etc/network/interfaces.d/lo.cfg content: | auto lo iface lo inet loopback - path: /etc/network/interfaces.d/ens3.cfg content: | iface ens3 inet6 auto up sleep 5 up dhclient -1 -6 -cf /etc/dhcp/dhclient6.conf -lf /var/lib/dhcp/dhclient6.ens3.leases -v ens3 || true runcmd: - [ ifdown, ens3] - [ ifup, ens3] Netzwerk: type: OS::Neutron::Net properties: name: BeispielNetzwerk Port: type: OS::Neutron::Port properties: network: { get_resource: Netzwerk } security_groups: { get_resource: Sec_SSH } Router: type: OS::Neutron::Router properties: external_gateway_info: { \"network\": { get_param: public_network_id } name: BeispielRouter Subnet: type: OS::Neutron::Subnet properties: name: BeispielSubnet dns_nameservers: - 8.8.8.8 - 8.8.4.4 network: { get_resource: Netzwerk } ip_version: 4 cidr: 10.0.0.0/24 allocation_pools: - { start: 10.0.0.10, end: 10.0.0.250 } Router_Subnet_Bridge: type: OS::Neutron::RouterInterface depends_on: Subnet properties: router: { get_resource: Router } subnet: { get_resource: Subnet } Floating_IP: type: OS::Neutron::FloatingIP properties: floating_network: { get_param: public_network_id } port_id: { get_resource: Port } Sec_SSH: type: OS::Neutron::SecurityGroup properties: description: Diese Security Group erlaubt den eingehenden SSH-Traffic über Port22 und ICMP name: Ermöglicht SSH (Port22) und ICMP rules: - { direction: ingress, remote_ip_prefix: 0.0.0.0/0, port_range_min: 22, port_range_max: 22, protocol:tcp } - { direction: ingress, remote_ip_prefix: 0.0.0.0/0, protocol: icmp } . The last step is to adjust the security group rules to allow access via IPv6. heat_template_version: 2014-10-16 parameters: key_name: type: string public_network_id: type: string default: provider resources: Instanz: type: OS::Nova::Server properties: key_name: { get_param: key_name } image: Ubuntu 16.04 Xenial Xerus - Latest flavor: m1.small networks: - port: {get_resource: Port } Instanz-Config: type: OS::Heat::CloudConfig properties: cloud_config: write_files: - path: /etc/dhcp/dhclient6.conf content: \"timeout 30;\" - path: /etc/cloud/cloud.cfg.d/99-disable-network-config.cfg content: \"network: {config: disabled}\" - path: /etc/network/interfaces.d/lo.cfg content: | auto lo iface lo inet loopback - path: /etc/network/interfaces.d/ens3.cfg content: | iface ens3 inet6 auto up sleep 5 up dhclient -1 -6 -cf /etc/dhcp/dhclient6.conf -lf /var/lib/dhcp/dhclient6.ens3.leases -v ens3 || true runcmd: - [ ifdown, ens3] - [ ifup, ens3] Netzwerk: type: OS::Neutron::Net properties: name: BeispielNetzwerk Port: type: OS::Neutron::Port properties: network: { get_resource: Netzwerk } security_groups: { get_resource: Sec_SSH } Router: type: OS::Neutron::Router properties: external_gateway_info: { \"network\": { get_param: public_network_id } name: BeispielRouter Subnet: type: OS::Neutron::Subnet properties: name: BeispielSubnet dns_nameservers: - 8.8.8.8 - 8.8.4.4 network: { get_resource: Netzwerk } ip_version: 4 cidr: 10.0.0.0/24 allocation_pools: - { start: 10.0.0.10, end: 10.0.0.250 } Router_Subnet_Bridge: type: OS::Neutron::RouterInterface depends_on: Subnet properties: router: { get_resource: Router } subnet: { get_resource: Subnet } Floating_IP: type: OS::Neutron::FloatingIP properties: floating_network: { get_param: public_network_id } port_id: { get_resource: Port } Sec_SSH: type: OS::Neutron::SecurityGroup properties: description: Diese Security Group erlaubt den eingehenden SSH-Traffic über Port22 und ICMP name: Ermöglicht SSH (Port22) und ICMP rules: - { direction: ingress, remote_ip_prefix: 0.0.0.0/0, port_range_min: 22, port_range_max: 22, protocol:tcp } - { direction: ingress, remote_ip_prefix: 0.0.0.0/0, protocol: icmp } - { direction: ingress, remote_ip_prefix: \"::/0\", port_range_min: 22, port_range_max: 22, protocol: tcp, ethertype: IPv6 } - { direction: ingress, remote_ip_prefix: \"::/0\", protocol: ipv6-icmp, ethertype: IPv6 } . ",
    "url": "/optimist/guided_tour/step19/#cloudconfig",
    
    "relUrl": "/optimist/guided_tour/step19/#cloudconfig"
  },"360": {
    "doc": "19: Add IPv6 to your template",
    "title": "Conclusion",
    "content": "You can now customize instances with Cloud Init and use IPv6 usable. In the final step you will learn how to start multiple instances with Heat. ",
    "url": "/optimist/guided_tour/step19/#conclusion",
    
    "relUrl": "/optimist/guided_tour/step19/#conclusion"
  },"361": {
    "doc": "19: Add IPv6 to your template",
    "title": "19: Add IPv6 to your template",
    "content": " ",
    "url": "/optimist/guided_tour/step19/",
    
    "relUrl": "/optimist/guided_tour/step19/"
  },"362": {
    "doc": "20: Build multiple VMs with HEAT",
    "title": "Step 20: Build multiple VMs with HEAT",
    "content": " ",
    "url": "/optimist/guided_tour/step20/#step-20-build-multiple-vms-with-heat",
    
    "relUrl": "/optimist/guided_tour/step20/#step-20-build-multiple-vms-with-heat"
  },"363": {
    "doc": "20: Build multiple VMs with HEAT",
    "title": "Start",
    "content": "Previous steps outlined how to create a single VM. The next step is to create multiple VMs at the same time. In this step, two VMs that share a common network will be created. ",
    "url": "/optimist/guided_tour/step20/#start",
    
    "relUrl": "/optimist/guided_tour/step20/#start"
  },"364": {
    "doc": "20: Build multiple VMs with HEAT",
    "title": "First Steps",
    "content": "To begin with, split the template into two parts. It is best practice to break larger setups up into multiple files. First, start with a simple template which contains only the network and the port. heat_template_version: 2014-10-16 description: A simple template which deploys 3 VMs resources: ExampleNet: type: OS::Neutron::Net properties: name: ExampleNet ExampleSubnet: type: OS::Neutron::Subnet properties: name: ExampleSubnet dns_nameservers: - 8.8.8.8 - 8.8.4.4 network: {get_resource: ExampleNet} ip_version: 4 cidr: 10.0.0.0/24 allocation_pools: - {start: 10.0.0.10, end: 10.0.0.250} . This is the basic structure for your stack, the file can be saved under the name groups.yaml. Create a new template exampleserver.yaml and define the VM here. Make sure that name and network_id are not defined. Use a valid value to fill image:. You can use the image name or ID. Exact image names / IDs can be obtained by running openstack image list. ß . heat_template_version: 2014-10-16 description: a single server description parameters: network_id: type: string server_name: type: string resources: VM: type: OS::Nova::Server properties: user_data_format: RAW image: Ubuntu 16.04 Xenial Xerus - Latest flavor: m1.small name: { get_param: server_name } networks: - port: { get_resource: ExamplePort } ExamplePort: type: OS::Neutron::Port properties: network: { get_param: network_id } . You can now modify your groups.yaml and add a resource group where you add the VMs with the required arguments. After the file has been updated, it can be saved as exampleserver.yaml . The next step is to integrate the second template created as a resource group. The number of instances, the names, etc. can also be specified here: . heat_template_version: 2014-10-16 description: A simple template which deploys 3 VMs resources:   ExampleVM: type: OS::Heat::ResourceGroup depends_on: ExampleSubnet properties: count: 3 resource_def: type: exampleserver.yaml properties: network_id: { get_resource: ExampleNet} server_name: ExampleVM_%index% ExampleNet: type: OS::Neutron::Net properties: name: ExampleNet ExampleSubnet: type: OS::Neutron::Subnet properties: name: ExampleSubnet dns_nameservers: - 8.8.8.8 - 8.8.4.4 network: {get_resource: ExampleNet} ip_version: 4 cidr: 10.0.0.0/24 allocation_pools: - {start: 10.0.0.10, end: 10.0.0.250} . Now that you have supplied all data, you can create your stack: . openstack stack create -t groups.yaml &lt;Name of the stack&gt; . ",
    "url": "/optimist/guided_tour/step20/#first-steps",
    
    "relUrl": "/optimist/guided_tour/step20/#first-steps"
  },"365": {
    "doc": "20: Build multiple VMs with HEAT",
    "title": "Conclusion",
    "content": "Congratulations, you went from creating a single VM with the web interface to creating full stacks with the OpenStack client. Several instances can now be rolled out at the same time using a template, a good starting point for OpenStack administration. ",
    "url": "/optimist/guided_tour/step20/#conclusion",
    
    "relUrl": "/optimist/guided_tour/step20/#conclusion"
  },"366": {
    "doc": "20: Build multiple VMs with HEAT",
    "title": "20: Build multiple VMs with HEAT",
    "content": " ",
    "url": "/optimist/guided_tour/step20/",
    
    "relUrl": "/optimist/guided_tour/step20/"
  },"367": {
    "doc": "21: Start a VM with an SSD volume",
    "title": "Step 21: Start a VM with an SSD volume",
    "content": " ",
    "url": "/optimist/guided_tour/step21/#step-21-start-a-vm-with-an-ssd-volume",
    
    "relUrl": "/optimist/guided_tour/step21/#step-21-start-a-vm-with-an-ssd-volume"
  },"368": {
    "doc": "21: Start a VM with an SSD volume",
    "title": "Start",
    "content": "Previously, you created a VM from scratch and also learned some HEAT basics. In this step you will boot a VM from an SSD volume. There are different ways to do this. In this step, we will first outline how to do so using Horizon (Dashboard) and secondly outline a different method which modifies the HEAT-Template from Step 18: Your VM will be reachable via IPv4. ",
    "url": "/optimist/guided_tour/step21/#start",
    
    "relUrl": "/optimist/guided_tour/step21/#start"
  },"369": {
    "doc": "21: Start a VM with an SSD volume",
    "title": "The Horizon (Dashboard) way",
    "content": "To get started, you need to log in to Horizon, as described in “Step 1: The Horizon (Dashboard)” Step 1: The Horizon (Dashboard). Next, create a new volume with Project → Volumes → Volumes and a click on + CREATE VOLUME. You need to fill in some information in the new overlay; a description for all required fields are below. After filling out the form, click on CREATE VOLUME . | Volume Name: Defines the name of the volume. In our example it will be prefilled with Ubuntu 16.04 Xenial Xerus - Latest. | Description: If required, you can add a short description. In our example, it is empty. | Volume Source: You can choose between Image and No source, empty image. Please use Image. | Use image as a source: You can choose any image, here we used Ubuntu 16.04 Xenial Xerus - Latest (276.2 MB). | Type: Three options are available high-iops, low-iops or default. To use the SSD storage, you need to choose high-iops. | Size: You can define the size of the volume, we chose 20 GiB. | Availability Zone: Again there are three options available Any Availability Zone, es1, or ix1. Use ix1. | . After volume creation, it should look like this: . There are two options to start a VM. The first option is to click on the down arrow symbol next to Edit Volume (as pictured above) and click on Launch as Instance. There will be a new overlay where you can choose the name (Instance Name) and the availability zone. Please use the same availability zone as previously used for the image (ix1). Switch to Source and choose Volume for Select Boot Source and click on the up-arrow next to the available volume. Switch to Flavor and choose one of the available flavors by clicking on the up-arrow. Next you need to choose a network in Networks. Use one of your networks and click on the up-arrow next to it. Now all required settings are in place and the VM can be started with Launch Instance. If required you can add your own Security Groups and/or Key Pairs. ",
    "url": "/optimist/guided_tour/step21/#the-horizon-dashboard-way",
    
    "relUrl": "/optimist/guided_tour/step21/#the-horizon-dashboard-way"
  },"370": {
    "doc": "21: Start a VM with an SSD volume",
    "title": "The HEAT way",
    "content": "This step uses the HEAT Template from Step 18. It will start a VM by default, so the next step is to adapt it, to allow it to create and boot from an SSD volume. First, add a new parameter “availability_zone”: . heat_template_version: 2014-10-16 parameters: key_name: type: string public_network_id: type: string default: provider availability_zone: type: string default: ix1 . Now add boot_ssd at the end of our template: . boot_ssd: type: OS::Cinder::Volume properties: name: boot_ssd size: 20 availability_zone: { get_param: availability_zone } volume_type: high-iops image: \"Ubuntu 16.04 Xenial Xerus - Latest\" . You have added a new parameter and will use this in your new volume. To start the VM from the volume, you need to edit Instanz in your template. You can delete or comment out image, because it is already associated with your volume. Now you can add availability_zone, name, networks, and block_device_mapping: . Instanz: type: OS::Nova::Server properties: name: SSD-Test availability_zone: { get_param: availability_zone } key_name: { get_param: key_name } #image: Ubuntu 16.04 Xenial Xerus - Latest flavor: m1.small networks: - port: { get_resource: Port } block_device_mapping: [ { device_name: \"vda\", volume_id: { get_resource: boot_ssd }, delete_on_termination: \"true\" } ] . The template is finished and should look like this: . heat_template_version: 2014-10-16 parameters: key_name: type: string public_network_id: type: string default: provider availability_zone: type: string default: ix1 resources: Instanz: type: OS::Nova::Server properties: name: SSD-Test availability_zone: { get_param: availability_zone } key_name: { get_param: key_name } #image: Ubuntu 16.04 Xenial Xerus - Latest flavor: m1.small networks: - port: { get_resource: Port } block_device_mapping: [ { device_name: \"vda\", volume_id: { get_resource: boot_ssd }, delete_on_termination: \"true\" } ] Netzwerk: type: OS::Neutron::Net properties: name: BeispielNetzwerk Port: type: OS::Neutron::Port properties: network: { get_resource: Netzwerk } Router: type: OS::Neutron::Router properties: external_gateway_info: { \"network\": { get_param: public_network_id } name: BeispielRouter Subnet: type: OS::Neutron::Subnet properties: name: BeispielSubnet dns_nameservers: - 8.8.8.8 - 8.8.4.4 network: { get_resource: Netzwerk } ip_version: 4 cidr: 10.0.0.0/24 allocation_pools: - { start: 10.0.0.10, end: 10.0.0.250 } Router_Subnet_Bridge: type: OS::Neutron::RouterInterface depends_on: Subnet properties: router: { get_resource: Router } subnet: { get_resource: Subnet } Floating_IP: type: OS::Neutron::FloatingIP properties: floating_network: { get_param: public_network_id } port_id: { get_resource: Port } Sec_SSH: type: OS::Neutron:SecurityGroup properties: description: Diese Security Group erlaubt den eingehenden SSH-Traffic über Port22 und ICMP name: Ermöglicht SSH (Port22) und ICMP rules: - { direction: ingress, remote_ip_prefix: 0.0.0.0/0, port_range_min: 22, port_range_max: 22, protocol: tcp } - { direction: ingress, remote_ip_prefix: 0.0.0.0/0, protocol: icmp } boot_ssd: type: OS::Cinder::Volume properties: name: boot_ssd size: 20 availability_zone: { get_param: availability_zone } volume_type: high-iops image: \"Ubuntu 16.04 Xenial Xerus - Latest\" . ",
    "url": "/optimist/guided_tour/step21/#the-heat-way",
    
    "relUrl": "/optimist/guided_tour/step21/#the-heat-way"
  },"371": {
    "doc": "21: Start a VM with an SSD volume",
    "title": "Conclusion",
    "content": "You have learned to start an instance from a volume and how to use SSD storage. Additionally, you have refreshed your heat knowledge and included a volume. ",
    "url": "/optimist/guided_tour/step21/#conclusion",
    
    "relUrl": "/optimist/guided_tour/step21/#conclusion"
  },"372": {
    "doc": "21: Start a VM with an SSD volume",
    "title": "21: Start a VM with an SSD volume",
    "content": " ",
    "url": "/optimist/guided_tour/step21/",
    
    "relUrl": "/optimist/guided_tour/step21/"
  },"373": {
    "doc": "22: Create a DNS record in Designate",
    "title": "Step 22: Create a DNS record in Designate",
    "content": " ",
    "url": "/optimist/guided_tour/step22/#step-22-create-a-dns-record-in-designate",
    
    "relUrl": "/optimist/guided_tour/step22/#step-22-create-a-dns-record-in-designate"
  },"374": {
    "doc": "22: Create a DNS record in Designate",
    "title": "Start",
    "content": "The Openstack Optimist platform includes a technology called DNS-as-a-Service (DNSaaS), also known as Designate. DNSaaS includes a REST API for domain and records management, is multi-tenant and integrates the OpenStack Identity Service (Keystone) for authentication. In this step, we will create a fictitious zone (domain) with MX and A records and store the appropriate IP/CNAME. The first step is to revisit the access data in “Step 4: Our way to the console” and ensure that the python-designateclient is installed (pip install python-openstackclient python-designateclient) The next step is to serve the Openstack client and create a zone for our project. $ openstack zone create --email webmaster@foobar.cloud foobar.cloud. +----------------+--------------------------------------+ | Field | Value | +----------------+--------------------------------------+ | action | CREATE | attributes | | created_at | 2018-08-15T06:45:24.000000 | description | None | email | webmaster@foobar.cloud | id | 036ae6e6-6318-47e1-920f-be518d845fb5 | masters | | name | foobar.cloud. | pool_id | bb031d0d-b8ca-455a-8963-50ec70fe57cf | project_id | 2b62bc8ff48445f394d0318dbd058967 | serial | 1534315524 | status | PENDING | transferred_at | None | ttl | 3600 | type | PRIMARY | updated_at | None | version | 1 | +----------------+--------------------------------------+ . Note the trailing “.” is required for the zone/domain to be created. The result so far: . $ openstack zone list +--------------------------------------+-----------------------+---------+------------+--------+--------+ | id | name | type | serial | status | action | +--------------------------------------+-----------------------+---------+------------+--------+--------+ | 036ae6e6-6318-47e1-920f-be518d845fb5 | foobar.cloud. | PRIMARY | 1534315524 | ACTIVE | NONE | +--------------------------------------+-----------------------+---------+------------+--------+--------+ $ openstack zone show foobar.cloud. +----------------+--------------------------------------+ | Field | Value | +----------------+--------------------------------------+ | action | NONE | attributes | | created_at | 2018-08-15T06:45:24.000000 | description | None | email | webmaster@foobar.cloud | id | 036ae6e6-6318-47e1-920f-be518d845fb5 | masters | | name | foobar.cloud. | pool_id | bb031d0d-b8ca-455a-8963-50ec70fe57cf | project_id | 2b62bc8ff48445f394d0318dbd058967 | serial | 1534315524 | status | ACTIVE | transferred_at | None | ttl | 3600 | type | PRIMARY | updated_at | 2018-08-15T06:45:30.000000 | version | 2 | +----------------+--------------------------------------+ . The domain “foobar.cloud” is now registered and ready to use (status: ACTIVE) for our project. In the next step, we want to create MX records (records for mail servers in this zone) for this domain. But first let’s see the content (recordsets) that already exist in your new zone. $ openstack recordset list foobar.cloud. +--------------------------------------+---------------+------+--------------------------------------------------------------------------------+--------+--------+ | id | name | type | records | status | action | +--------------------------------------+---------------+------+--------------------------------------------------------------------------------+--------+--------+ | 4b666317-6170-4d71-8962-94f1cbe94dda | foobar.cloud. | NS | dns1.ddns.innovo.cloud. | ACTIVE | NONE | | | dns2.ddns.innovo.cloud. | | 7915c9c1-de30-4144-b82b-36295687b0fe | foobar.cloud. | SOA | dns2.ddns.innovo.cloud. webmaster.foobar.cloud. 1534315524 3507 600 86400 3600 | ACTIVE | NONE | +--------------------------------------+---------------+------+--------------------------------------------------------------------------------+--------+--------+ . Here you see an “empty shell” of a domain with automatically generated NS and SOA records which are ready to be queried. $ dig +short @dns1.ddns.innovo.cloud foobar.cloud NS dns1.ddns.innovo.cloud. dns2.ddns.innovo.cloud. $ dig +short @dns1.ddns.innovo.cloud foobar.cloud SOA dns2.ddns.innovo.cloud. webmaster.foobar.cloud. 1534315524 3507 600 86400 3600 . Creating an MX record: You can now add, modify, or delete records within this zone (openstack recordset –help). For MX records we also set up the typical mail server priorities (10,20), where the lower value is always selected first and the second entry serves as a “backup”. $ openstack recordset create --record '10 mx1.foobar.cloud.' --record '20 mx2.foobar.cloud.' --type MX foobar.cloud. foobar.cloud. +-------------+--------------------------------------+ | Field | Value | +-------------+--------------------------------------+ | action | CREATE | created_at | 2018-08-15T07:15:32.000000 | description | None | id | 4197e380-dc61-4e1d-bf92-0dcf5344eafa | name | foobar.cloud. | project_id | 2b62bc8ff48445f394d0318dbd058967 | records | 10 mx1.foobar.cloud. | | 20 mx2.foobar.cloud. | status | PENDING | ttl | None | type | MX | updated_at | None | version | 1 | zone_id | 036ae6e6-6318-47e1-920f-be518d845fb5 | zone_name | foobar.cloud. | +-------------+--------------------------------------+ $ openstack recordset list foobar.cloud. +--------------------------------------+---------------+------+--------------------------------------------------------------------------------+---------+--------+ | id | name | type | records | status | action | +--------------------------------------+---------------+------+--------------------------------------------------------------------------------+---------+--------+ | 4b666317-6170-4d71-8962-94f1cbe94dda | foobar.cloud. | NS | dns1.ddns.innovo.cloud. | ACTIVE | NONE | | | dns2.ddns.innovo.cloud. | | 7915c9c1-de30-4144-b82b-36295687b0fe | foobar.cloud. | SOA | dns2.ddns.innovo.cloud. webmaster.foobar.cloud. 1534317332 3507 600 86400 3600 | PENDING | UPDATE | 4197e380-dc61-4e1d-bf92-0dcf5344eafa | foobar.cloud. | MX | 20 mx2.foobar.cloud. | PENDING | CREATE | | | 10 mx1.foobar.cloud. | | +--------------------------------------+---------------+------+--------------------------------------------------------------------------------+---------+--------+ . $ openstack recordset create --type A --record 1.2.3.4 foobar.cloud. www +-------------+--------------------------------------+ | Field | Value | +-------------+--------------------------------------+ | action | CREATE | created_at | 2018-08-15T07:28:15.000000 | description | None | id | d932688f-21d5-44b1-aa27-030c342788e7 | name | www.foobar.cloud. | project_id | 2b62bc8ff48445f394d0318dbd058967 | records | 1.2.3.4 | status | PENDING | ttl | None | type | A | updated_at | None | version | 1 | zone_id | 036ae6e6-6318-47e1-920f-be518d845fb5 | zone_name | foobar.cloud. | +-------------+--------------------------------------+ . Result: . $ openstack recordset list foobar.cloud. +--------------------------------------+-------------------+------+--------------------------------------------------------------------------------+--------+--------+ | id | name | type | records | status | action | +--------------------------------------+-------------------+------+--------------------------------------------------------------------------------+--------+--------+ | 4b666317-6170-4d71-8962-94f1cbe94dda | foobar.cloud. | NS | dns1.ddns.innovo.cloud. | ACTIVE | NONE | | | dns2.ddns.innovo.cloud. | | 7915c9c1-de30-4144-b82b-36295687b0fe | foobar.cloud. | SOA | dns2.ddns.innovo.cloud. webmaster.foobar.cloud. 1534318095 3507 600 86400 3600 | ACTIVE | NONE | 4197e380-dc61-4e1d-bf92-0dcf5344eafa | foobar.cloud. | MX | 20 mx2.foobar.cloud. | ACTIVE | NONE | | | 10 mx1.foobar.cloud. | | d932688f-21d5-44b1-aa27-030c342788e7 | www.foobar.cloud. | A | 1.2.3.4 | ACTIVE | NONE | +--------------------------------------+-------------------+------+--------------------------------------------------------------------------------+--------+--------+ . Once the recordsets are active you can use the designated DNS servers: . | dns1.ddns.innovo.cloud | dns2.ddns.innovo.cloud | . Query for these records: . $ dig +short @dns1.ddns.innovo.cloud foobar.cloud NS dns1.ddns.innovo.cloud. dns2.ddns.innovo.cloud. $ dig +short @dns1.ddns.innovo.cloud foobar.cloud MX 10 mx1.foobar.cloud. 20 mx2.foobar.cloud. $ dig +short @dns1.ddns.innovo.cloud foobar.cloud www.foobar.cloud 1.2.3.4 . ATTENTION! At this time, this domain (foobar.cloud) is not yet resolvable worldwide. For this construct to be used worldwide, each domain managed by Designate must have delegation to the name servers dns1.ddns.innovo.cloud and dns2.ddns.innovo.cloud established by the respective registrar. Details about our authoritative DNS servers: . | dns1.ddns.innovo.cloud: ‘185.116.244.45’ / ‘2a00:c320:0:1::d’ | dns2.ddns.innovo.cloud: ‘185.116.244.46’ / ‘2a00:c320:0:1::e’ | . In order to complete the mail records, it is still advisable to store corresponding A-records for the mail servers . $ openstack recordset create --type A --record 2.3.4.5 foobar.cloud. mx1 +-------------+--------------------------------------+ | Field | Value | +-------------+--------------------------------------+ | action | CREATE | created_at | 2018-08-15T07:42:31.000000 | description | None | id | 630d5103-7c02-4a58-83a5-97f802cf141c | name | mx1.foobar.cloud. | project_id | 2b62bc8ff48445f394d0318dbd058967 | records | 2.3.4.5 | status | PENDING | ttl | None | type | A | updated_at | None | version | 1 | zone_id | 036ae6e6-6318-47e1-920f-be518d845fb5 | zone_name | foobar.cloud. | +-------------+--------------------------------------+ and $ openstack recordset create --type A --record 3.4.5.6 foobar.cloud. mx2 +-------------+--------------------------------------+ | Field | Value | +-------------+--------------------------------------+ | action | CREATE | created_at | 2018-08-15T07:42:56.000000 | description | None | id | 2b47dbe4-70b9-4edb-ac2f-25cb8398bacb | name | mx2.foobar.cloud. | project_id | 2b62bc8ff48445f394d0318dbd058967 | records | 3.4.5.6 | status | PENDING | ttl | None | type | A | updated_at | None | version | 1 | zone_id | 036ae6e6-6318-47e1-920f-be518d845fb5 | zone_name | foobar.cloud. | +-------------+--------------------------------------+ . Result after a few seconds: . $ openstack recordset list foobar.cloud. +--------------------------------------+-------------------+------+--------------------------------------------------------------------------------+--------+--------+ | id | name | type | records | status | action | +--------------------------------------+-------------------+------+--------------------------------------------------------------------------------+--------+--------+ | 4b666317-6170-4d71-8962-94f1cbe94dda | foobar.cloud. | NS | dns1.ddns.innovo.cloud. | ACTIVE | NONE | | | dns2.ddns.innovo.cloud. | | 7915c9c1-de30-4144-b82b-36295687b0fe | foobar.cloud. | SOA | dns2.ddns.innovo.cloud. webmaster.foobar.cloud. 1534318976 3507 600 86400 3600 | ACTIVE | NONE | 4197e380-dc61-4e1d-bf92-0dcf5344eafa | foobar.cloud. | MX | 20 mx2.foobar.cloud. | ACTIVE | NONE | | | 10 mx1.foobar.cloud. | | d932688f-21d5-44b1-aa27-030c342788e7 | www.foobar.cloud. | A | 1.2.3.4 | ACTIVE | NONE | 630d5103-7c02-4a58-83a5-97f802cf141c | mx1.foobar.cloud. | A | 2.3.4.5 | ACTIVE | NONE | 2b47dbe4-70b9-4edb-ac2f-25cb8398bacb | mx2.foobar.cloud. | A | 3.4.5.6 | ACTIVE | NONE | +--------------------------------------+-------------------+------+--------------------------------------------------------------------------------+--------+--------+ . ",
    "url": "/optimist/guided_tour/step22/#start",
    
    "relUrl": "/optimist/guided_tour/step22/#start"
  },"375": {
    "doc": "22: Create a DNS record in Designate",
    "title": "Conclusion",
    "content": "In this step, you have learned how to create a zone, configure a record set, and query it. ",
    "url": "/optimist/guided_tour/step22/#conclusion",
    
    "relUrl": "/optimist/guided_tour/step22/#conclusion"
  },"376": {
    "doc": "22: Create a DNS record in Designate",
    "title": "22: Create a DNS record in Designate",
    "content": " ",
    "url": "/optimist/guided_tour/step22/",
    
    "relUrl": "/optimist/guided_tour/step22/"
  },"377": {
    "doc": "23: Object Storage (S3 compatible)",
    "title": "Step 23: Object Storage (S3 compatible)",
    "content": " ",
    "url": "/optimist/guided_tour/step23/#step-23-object-storage-s3-compatible",
    
    "relUrl": "/optimist/guided_tour/step23/#step-23-object-storage-s3-compatible"
  },"378": {
    "doc": "23: Object Storage (S3 compatible)",
    "title": "Start",
    "content": "In the previous steps, you learned about various building blocks in OpenStack. Now we will take a look at the Object Storage, which offers some interesting ways to save data. ",
    "url": "/optimist/guided_tour/step23/#start",
    
    "relUrl": "/optimist/guided_tour/step23/#start"
  },"379": {
    "doc": "23: Object Storage (S3 compatible)",
    "title": "Credentials",
    "content": "The first step is to obtain login data (ec2 credentials) in order to access Object Storage. Therefore, you need the OpenStackClient (as mentioned in Step 4: Our way to the console”), to create the credentials with the OpenStack API. To create the credentials, run the following command: . openstack ec2 credentials create . The output should look like this: . $ openstack ec2 credentials create +------------+-----------------------------------------------------------------+ | Field | Value | +------------+-----------------------------------------------------------------+ | access | &lt;your access_key&gt; | links | {u'self': u'https://identity.optimist.gec.io/v3/users/ | | user-id/credentials/OS-EC2/access_key'} | project_id | &lt;your project_id&gt; | secret | &lt;your secret_key&gt; | trust_id | None | user_id | &lt;your user_id&gt; | +------------+-----------------------------------------------------------------+ . Once the credentials have been created, you need some tools to interact with the ObjectStorage. ",
    "url": "/optimist/guided_tour/step23/#credentials",
    
    "relUrl": "/optimist/guided_tour/step23/#credentials"
  },"380": {
    "doc": "23: Object Storage (S3 compatible)",
    "title": "How to get access to the ObjectStorage (S3 compatible)",
    "content": "There are several tools available which allow us to interact with Object Storage, however we recommend using s3cmd as it is straightforward to use and handle. You have already installed “pip” as package-manager (in Step 4) you can also use it to install s3cmd: . pip install s3cmd . Since S3cmd is now installed, the previously created credentials must be entered in a file called .s3cfg in order to begin using it. The file should be located in the user’s home directory, for example, /home/username/ . The following process can now be used to create the .s3cfg file: . touch .s3cfg . You can open .s3cfg with your preferred text editor (for example, vi, vim, nano) and enter your credentials as follows: . access_key = &lt;your access_key&gt; check_ssl_certificate = True check_ssl_hostname = True host_base = s3.es1.fra.optimist.gec.io host_bucket = s3.es1.fra.optimist.gec.io secret_key = &lt;your secret_key&gt; use_https = True . ",
    "url": "/optimist/guided_tour/step23/#how-to-get-access-to-the-objectstorage-s3-compatible",
    
    "relUrl": "/optimist/guided_tour/step23/#how-to-get-access-to-the-objectstorage-s3-compatible"
  },"381": {
    "doc": "23: Object Storage (S3 compatible)",
    "title": "The bucket",
    "content": "After you have access to ObjectStorage (S3 compatible), you can start working with it. If required, you can see all s3cmd commands with: . s3cmd --help . You can now create a bucket. In the broadest sense, buckets are similar to folders, which are required for a structure. A file can only be saved in a bucket. It is important that the name is unique (for all customers). If there is already a bucket available with the name test, you cannot create another one with the name test. We recommend using a UUID and then resolving it in the corresponding application. You can also differentiate between public and private buckets. By default, all buckets are private, and only the creator of the bucket can access them. If needed, you can change it, for example with the Access Control List (ACL). IMPORTANT: If you set a bucket to public, all files in it are reachable. Information about files in this bucket that are set to private can also be retrieved. We recommend only setting specific files to public. Now that we know the key details, it is time to create a bucket with a UUID: . $ s3cmd mb s3://e4d05df3-aa8e-4a37-b1b5-2745d189b189 Bucket 's3://e4d05df3-aa8e-4a37-b1b5-2745d189b189/' created . ",
    "url": "/optimist/guided_tour/step23/#the-bucket",
    
    "relUrl": "/optimist/guided_tour/step23/#the-bucket"
  },"382": {
    "doc": "23: Object Storage (S3 compatible)",
    "title": "Upload a file",
    "content": "After the bucket has been created, let’s upload a file with the command s3cmd put file_name s3://bucket_name. The outcome should be similar to the below: . $ s3cmd put test.yaml s3://e4d05df3-aa8e-4a37-b1b5-2745d189b189 upload: 'test.yaml' -&gt; 's3://e4d05df3-aa8e-4a37-b1b5-2745d189b189/test.yaml' [1 of 1] 4218 of 4218 100% in 0s 4.61 kB/s done . ",
    "url": "/optimist/guided_tour/step23/#upload-a-file",
    
    "relUrl": "/optimist/guided_tour/step23/#upload-a-file"
  },"383": {
    "doc": "23: Object Storage (S3 compatible)",
    "title": "Get access to the files",
    "content": "The general URL for accessing files in Optimist is https://s3.es1.fra.optimist.gec.io/bucket_name/file_name. To get access to your example file, you need to change the settings from private to public. To do this, use the Access Control List (ACL): . $ s3cmd setacl s3://e4d05df3-aa8e-4a37-b1b5-2745d189b189/test.yaml --acl-public s3://e4d05df3-aa8e-4a37-b1b5-2745d189b189/test.yaml: ACL set to Public [1 of 1] . Now you can access the file with the following link: https://s3.es1.fra.optimist.gec.io/e4d05df3-aa8e-4a37-b1b5-2745d189b189/test.yaml . To set the file to private once again, use this command: . $ s3cmd setacl s3://e4d05df3-aa8e-4a37-b1b5-2745d189b189/test.yaml --acl-private s3://e4d05df3-aa8e-4a37-b1b5-2745d189b189/test.yaml: ACL set to Private [1 of 1] . ",
    "url": "/optimist/guided_tour/step23/#get-access-to-the-files",
    
    "relUrl": "/optimist/guided_tour/step23/#get-access-to-the-files"
  },"384": {
    "doc": "23: Object Storage (S3 compatible)",
    "title": "Conclusion",
    "content": "You have taken your first steps with S3 compatible storage. ",
    "url": "/optimist/guided_tour/step23/#conclusion",
    
    "relUrl": "/optimist/guided_tour/step23/#conclusion"
  },"385": {
    "doc": "23: Object Storage (S3 compatible)",
    "title": "23: Object Storage (S3 compatible)",
    "content": " ",
    "url": "/optimist/guided_tour/step23/",
    
    "relUrl": "/optimist/guided_tour/step23/"
  },"386": {
    "doc": "Networking",
    "title": "Networking",
    "content": " ",
    "url": "/optimist/networking/",
    
    "relUrl": "/optimist/networking/"
  },"387": {
    "doc": "Port Forwarding on Floating IPs",
    "title": "Port Forwarding on Floating IPs",
    "content": "Floating IP port forwarding allows users to forward traffic from a TCP/UDP/other protocol port of a floating IP to a TCP/UDP/other protocol port associated to one of the fixed IPs of a Neutron port. ",
    "url": "/optimist/networking/port_forwarding/",
    
    "relUrl": "/optimist/networking/port_forwarding/"
  },"388": {
    "doc": "Port Forwarding on Floating IPs",
    "title": "Create a port forwarding rule on a floating IP",
    "content": "In order to apply port forwarding on a floating IP, the following information is required: . | The internal IP address to be used | The UUID of the port to be associated with the floating IP | The port number of the network port’s fixed IPv4 address | The external port number of the port forwarding’s floating IP address | The specific protocol to be used in the port forwarding (in this example TCP) | The floating IP for the port forwarding rule to be applied on. | . The example below demonstrates creation of port forwarding on a floating IP, using the required options: . $ openstack floating ip port forwarding create \\ --internal-ip-address 10.0.0.14 \\ --port 12c29300-0f8a-4c54-a9dc-bee4c12c6ad2 \\ --internal-protocol-port 80 \\ --external-protocol-port 8080 \\ --protocol tcp 185.116.244.141 . ",
    "url": "/optimist/networking/port_forwarding/#create-a-port-forwarding-rule-on-a-floating-ip",
    
    "relUrl": "/optimist/networking/port_forwarding/#create-a-port-forwarding-rule-on-a-floating-ip"
  },"389": {
    "doc": "Port Forwarding on Floating IPs",
    "title": "List port forwarding settings applied to floating IPs",
    "content": "Within a project, a list of port forwarding rules applied to specific floating IPs can be obtained with the following command. $ openstack floating ip port forwarding list 185.116.244.141 . The command above can be further refined using --sort-column --port, --external-protcol-port and --protocol flags before the floating IP. ",
    "url": "/optimist/networking/port_forwarding/#list-port-forwarding-settings-applied-to-floating-ips",
    
    "relUrl": "/optimist/networking/port_forwarding/#list-port-forwarding-settings-applied-to-floating-ips"
  },"390": {
    "doc": "Port Forwarding on Floating IPs",
    "title": "Display details of a specific port forwarding rule",
    "content": "To display the specific details of a Port Forwarding rule for a Floating IP, the following command can be used: . $ openstack floating ip port forwarding show &lt;floating-ip&gt; &lt;port-forwarding-id&gt; . ",
    "url": "/optimist/networking/port_forwarding/#display-details-of-a-specific-port-forwarding-rule",
    
    "relUrl": "/optimist/networking/port_forwarding/#display-details-of-a-specific-port-forwarding-rule"
  },"391": {
    "doc": "Port Forwarding on Floating IPs",
    "title": "Modifying Floating IP Port Forwarding Properties",
    "content": "If a port forwarding configuration on a floating IP has already been created using $ openstack floating ip port forwarding create, further changes can be made to the existing configuration using $ openstack floating ip port forwarding set .... The following aspects of the port forwarding can be modified: . | --port: The UUID of the network port | --internal-ip-address: The fixed internal address associated with the floating IP port forwarding rule. | --internal-protocol-port: The TCP/UDP/etc. port number of the network port fixed IPv4 address associated with the floating IP port forwarding rule | --external-protocol-port: The TCP/UDP/etc. port number of the port forwarding rule’s floating IP address | --protocol: The IP protocol used in the floating IP port forwarding rule (TCP/UDP/other) | --description: Text describing/contextualizing the use of the port forwarding configuration | . The configuration of any of the above options can be modified with a variation of the following command: . $ openstack floating ip port forwarding set \\ --port &lt;port&gt; \\ --internal-ip-address &lt;internal-ip-address&gt; \\ --internal-protocol-port &lt;port-number&gt; \\ --external-protocol-port &lt;port-number&gt; \\ --protocol &lt;protocol&gt; \\ --description &lt;description&gt;] \\ &lt;floating-ip&gt; &lt;port-forwarding-id&gt;` . ",
    "url": "/optimist/networking/port_forwarding/#modifying-floating-ip-port-forwarding-properties",
    
    "relUrl": "/optimist/networking/port_forwarding/#modifying-floating-ip-port-forwarding-properties"
  },"392": {
    "doc": "Port Forwarding on Floating IPs",
    "title": "Delete port forwarding from a floating IP",
    "content": "To remove a port forwarding rule from a floating IP, we need the following information: . | The floating IP from which the port forwarding rule is to be removed from. | The port forwarding ID (This ID is applied upon creation and can be obtained using the $ openstack floating ip port forwarding list ... command) | . The following command removes the port forwarding rule from a floating ip: . $ openstack floating ip port forwarding delete &lt;floating-ip&gt; &lt;port-forwarding-id&gt; . ",
    "url": "/optimist/networking/port_forwarding/#delete-port-forwarding-from-a-floating-ip",
    
    "relUrl": "/optimist/networking/port_forwarding/#delete-port-forwarding-from-a-floating-ip"
  },"393": {
    "doc": "Shared Networks",
    "title": "Shared Networks",
    "content": " ",
    "url": "/optimist/networking/shared_networks/",
    
    "relUrl": "/optimist/networking/shared_networks/"
  },"394": {
    "doc": "Shared Networks",
    "title": "Motivation",
    "content": "The question often arises of whether it is possible to share a network between two OpenStack projects. In this document, we will explain what is needed and how this can be implemented. ",
    "url": "/optimist/networking/shared_networks/#motivation",
    
    "relUrl": "/optimist/networking/shared_networks/#motivation"
  },"395": {
    "doc": "Shared Networks",
    "title": "Share Network",
    "content": "If access to both projects is available: . In order to share the network, we needto use the OpenStackClient, the Project ID into which the network is to be shared, as well as the Network ID of the network to be shared. The Project ID can be found in the output under “id” if we use the following command: . openstack project show &lt;Name of Project&gt; -f value -c id . Next, we need the Network ID of the network to be shared. We can find this in the output under “id” if the following command is used: . openstack network show &lt;Name of Network&gt; -f value -c id . With the obtained IDs, the network can now be shared into the corresponding project. To do this, we use Role-Based Access Control (RBAC): . openstack network rbac create --type network --action access_as_shared --target-project &lt;ID of Project&gt; &lt;ID of Network to share&gt; . If access to both projects is not available: . In this case, the network can only be shared by support after the approval of the other project owner. To share a network with a project, please send us an e-mail to support@gec.io with the following information: . | Name and ID of the network to be shared | Name and ID of the project in which the network should be visible | . ",
    "url": "/optimist/networking/shared_networks/#share-network",
    
    "relUrl": "/optimist/networking/shared_networks/#share-network"
  },"396": {
    "doc": "Shared Networks",
    "title": "Important information about shared networks",
    "content": "When accessing a shared network, there are limitations that must be considered. One limitation is that no remote security groups can be used. Additionally, there is no insight into ports and IP addresses from the other project. Therefore, one can also specify any specific IP addresses for new ports in a subnet (in the shared network), as it would be possible to find IPs that are already in use. In order to make use of the shared network, there is the option to create a new port. This then receives a random IP address to use, for example, to add a router through this port. This is not possible in the Horizon dashboard; we need to use the OpenStackClient. Please ensure that no spaces or special characters are used in names, as using these can lead to problems. First, we create the port and specify the shared network there: . openstack port create --network &lt;ID of shared Networks&gt; &lt;Name of Ports&gt; . Now, for example, a router can be created and then mapped to the newly created port: . ##Creation of the router $ openstack router create &lt;Name of Router&gt; ##Assigning a port to the router $ openstack router add port &lt;Name of Router&gt; &lt;Name of Port&gt; . ",
    "url": "/optimist/networking/shared_networks/#important-information-about-shared-networks",
    
    "relUrl": "/optimist/networking/shared_networks/#important-information-about-shared-networks"
  },"397": {
    "doc": "Shared Networks",
    "title": "Network Topology Project 1",
    "content": ". The network “shared” is shared from project 1 to project 2. The service “Example” is available in this network and runs on an instance there. ",
    "url": "/optimist/networking/shared_networks/#network-topology-project-1",
    
    "relUrl": "/optimist/networking/shared_networks/#network-topology-project-1"
  },"398": {
    "doc": "Shared Networks",
    "title": "Network Topology Project 2",
    "content": ". The network “shared” is also visible in project 2 and was attached to the router “router2”. In addition, we have the network “network” from which the services in the network “shared” should be accessed. You have to make sure that the corresponding route is set in the subnet “host route” configuration of the “shared” network in project 1 in order to enable the correct return transport of the packets. In our example the following route is required: 10.0.1.0/24,10.0.0.1 . ",
    "url": "/optimist/networking/shared_networks/#network-topology-project-2",
    
    "relUrl": "/optimist/networking/shared_networks/#network-topology-project-2"
  },"399": {
    "doc": "Octavia Loadbalancers",
    "title": "The Octavia Loadbalancer",
    "content": " ",
    "url": "/optimist/networking/octavia_loadbalancer/#the-octavia-loadbalancer",
    
    "relUrl": "/optimist/networking/octavia_loadbalancer/#the-octavia-loadbalancer"
  },"400": {
    "doc": "Octavia Loadbalancers",
    "title": "Preface",
    "content": "Octavia is a highly-available and scalable open source load balancing solution designed to work with OpenStack. Octavia handles load balancing services by managing and configuring a fleet of virtual machines – also known as amphorae – in its project. These amphorae run a HAproxy. ",
    "url": "/optimist/networking/octavia_loadbalancer/#preface",
    
    "relUrl": "/optimist/networking/octavia_loadbalancer/#preface"
  },"401": {
    "doc": "Octavia Loadbalancers",
    "title": "First Steps",
    "content": "To use Octavia, the client first needs to be installed on your system. Instructions for the installation can be found in Step 04 of our guide. ",
    "url": "/optimist/networking/octavia_loadbalancer/#first-steps",
    
    "relUrl": "/optimist/networking/octavia_loadbalancer/#first-steps"
  },"402": {
    "doc": "Octavia Loadbalancers",
    "title": "Creating an Octavia-Ladbalancer",
    "content": "In our example we use the example subnet we’ve already created in Step 10. $ openstack loadbalancer create --name Beispiel-LB --vip-subnet-id 32259126-dd37-44d5-922c-99d68ee870cd +---------------------+--------------------------------------+ | Field | Value | +---------------------+--------------------------------------+ | admin_state_up | True | created_at | 2019-05-01T09:00:00 | description | | flavor_id | | id | e94827f0-f94d-40c7-a7fd-b91bf2676177 | listeners | | name | Beispiel-LB | operating_status | OFFLINE | pools | | project_id | b15cde70d85749689e08106f973bb002 | provider | amphora | provisioning_status | PENDING_CREATE | updated_at | None | vip_address | 10.0.0.10 | vip_network_id | f2a8f00e-204b-4c37-9d19-1d5c8e4efbf6 | vip_port_id | 37fc5b34-ee07-49c8-b054-a8d591a9679f | vip_qos_policy_id | None | vip_subnet_id | 32259126-dd37-44d5-922c-99d68ee870cd | +---------------------+--------------------------------------+ . Now Octavia spawns amphorae instances in the background. $ openstack loadbalancer list +--------------------------------------+-------------+----------------------------------+--------------+---------------------+----------+ | id | name | project_id | vip_address | provisioning_status | provider | +--------------------------------------+-------------+----------------------------------+--------------+---------------------+----------+ | e94827f0-f94d-40c7-a7fd-b91bf2676177 | Beispiel-LB | b15cde70d85749689e08106f973bb002 | 10.0.0.10 | PENDING_CREATE | amphora | +--------------------------------------+-------------+----------------------------------+--------------+---------------------+----------+ . Once the provisioning_status is ACTIVE, the process has completed successfully and the Octavia load balancer can be further configured. $ openstack loadbalancer list +--------------------------------------+-------------+----------------------------------+--------------+---------------------+----------+ | id | name | project_id | vip_address | provisioning_status | provider | +--------------------------------------+-------------+----------------------------------+--------------+---------------------+----------+ | e94827f0-f94d-40c7-a7fd-b91bf2676177 | Beispiel-LB | b15cde70d85749689e08106f973bb002 | 10.0.0.10 | ACTIVE | amphora | +--------------------------------------+-------------+----------------------------------+--------------+---------------------+----------+ . ",
    "url": "/optimist/networking/octavia_loadbalancer/#creating-an-octavia-ladbalancer",
    
    "relUrl": "/optimist/networking/octavia_loadbalancer/#creating-an-octavia-ladbalancer"
  },"403": {
    "doc": "Octavia Loadbalancers",
    "title": "Create LB listener",
    "content": "In our example, we want to create a listener for HTTP on port 80. A listener here - as in other LB solutions - refers to the port of the front end. $ openstack loadbalancer listener create --name Beispiel-listener --protocol HTTP --protocol-port 80 Beispiel-LB +-----------------------------+--------------------------------------+ | Field | Value | +-----------------------------+--------------------------------------+ | admin_state_up | True | connection_limit | -1 | created_at | 2019-05-01T09:00:00 | default_pool_id | None | default_tls_container_ref | None | description | | id | 0a3312d1-8cf7-41a8-8d24-181246468cd7 | insert_headers | None | l7policies | | loadbalancers | e94827f0-f94d-40c7-a7fd-b91bf2676177 | name | Beispiel-listener | operating_status | OFFLINE | project_id | b15cde70d85749689e08106f973bb002 | protocol | HTTP | protocol_port | 80 | provisioning_status | PENDING_CREATE | sni_container_refs | [] | timeout_client_data | 50000 | timeout_member_connect | 5000 | timeout_member_data | 50000 | timeout_tcp_inspect | 0 | updated_at | None | client_ca_tls_container_ref | | client_authentication | | client_crl_container_ref | +-----------------------------+--------------------------------------+ . Once the admin_state_up is true the loadbalancer has been successfully created. The Octavia loadbalancer can be further configured at this point. $ openstack loadbalancer listener list +--------------------------------------+-----------------+-------------------+----------------------------------+----------+---------------+----------------+ | id | default_pool_id | name | project_id | protocol | protocol_port | admin_state_up | +--------------------------------------+-----------------+-------------------+----------------------------------+----------+---------------+----------------+ | 0a3312d1-8cf7-41a8-8d24-181246468cd7 | None | Beispiel-listener | b15cde70d85749689e08106f973bb002 | HTTP | 80 | True | +--------------------------------------+-----------------+-------------------+----------------------------------+----------+---------------+----------------+ . ",
    "url": "/optimist/networking/octavia_loadbalancer/#create-lb-listener",
    
    "relUrl": "/optimist/networking/octavia_loadbalancer/#create-lb-listener"
  },"404": {
    "doc": "Octavia Loadbalancers",
    "title": "Create LB pool",
    "content": "The LB-pool here refers to a collection of all objects (listeners, members, etc.) - comparable to a pool of public IP addresses from which one can be assigned. A pool for our example is created as follows: . $ openstack loadbalancer pool create --name Beispiel-pool --lb-algorithm ROUND_ROBIN --listener Beispiel-listener --protocol HTTP +----------------------+--------------------------------------+ | Field | Value | +----------------------+--------------------------------------+ | admin_state_up | True | created_at | 2019-05-01T09:00:00 | description | | healthmonitor_id | | id | 4053e88e-c2b5-47c6-987e-4387d837c88d | lb_algorithm | ROUND_ROBIN | listeners | 0a3312d1-8cf7-41a8-8d24-181246468cd7 | loadbalancers | e94827f0-f94d-40c7-a7fd-b91bf2676177 | members | | name | Beispiel-pool | operating_status | OFFLINE | project_id | b15cde70d85749689e08106f973bb002 | protocol | HTTP | provisioning_status | PENDING_CREATE | session_persistence | None | updated_at | None | tls_container_ref | | ca_tls_container_ref | | crl_container_ref | | tls_enabled | +----------------------+--------------------------------------+ . It should be noted that with openstack loadbalancer pool create --help all possible settings can be displayed. The most common settings and their choices: . --protocol: {TCP,HTTP,HTTPS,TERMINATED_HTTPS,PROXY,UDP} --lb-algorithm {SOURCE_IP,ROUND_ROBIN,LEAST_CONNECTIONS} . The pool has been successfully created when the provisioning_status has reached the status ACTIVE. $ openstack loadbalancer pool list +--------------------------------------+---------------+----------------------------------+---------------------+----------+--------------+----------------+ | id | name | project_id | provisioning_status | protocol | lb_algorithm | admin_state_up | +--------------------------------------+---------------+----------------------------------+---------------------+----------+--------------+----------------+ | 4053e88e-c2b5-47c6-987e-4387d837c88d | Beispiel-pool | b15cde70d85749689e08106f973bb002 | ACTIVE | HTTP | ROUND_ROBIN | True | +--------------------------------------+---------------+----------------------------------+---------------------+----------+--------------+----------------+ . ",
    "url": "/optimist/networking/octavia_loadbalancer/#create-lb-pool",
    
    "relUrl": "/optimist/networking/octavia_loadbalancer/#create-lb-pool"
  },"405": {
    "doc": "Octavia Loadbalancers",
    "title": "Create the LB member",
    "content": "For our loadbalancer to know which backends it is allowed to forward to, we still need a member, which we define as follows: . $ openstack loadbalancer member create --subnet-id 32259126-dd37-44d5-922c-99d68ee870cd --address 10.0.0.11 --protocol-port 80 Beispiel-pool +---------------------+--------------------------------------+ | Field | Value | +---------------------+--------------------------------------+ | address | 10.0.0.11 | admin_state_up | True | created_at | 2019-05-01T09:00:00 | id | 703e27e0-e7fe-474b-b32d-68f9a8aeef07 | name | | operating_status | NO_MONITOR | project_id | b15cde70d85749689e08106f973bb002 | protocol_port | 80 | provisioning_status | PENDING_CREATE | subnet_id | 32259126-dd37-44d5-922c-99d68ee870cd | updated_at | None | weight | 1 | monitor_port | None | monitor_address | None | backup | False | +---------------------+--------------------------------------+ . and . $ openstack loadbalancer member create --subnet-id 32259126-dd37-44d5-922c-99d68ee870cd --address 10.0.0.12 --protocol-port 80 Beispiel-pool +---------------------+--------------------------------------+ | Field | Value | +---------------------+--------------------------------------+ | address | 10.0.0.12 | admin_state_up | True | created_at | 2019-05-01T09:00:00 | id | 2add1e17-73a6-4002-82af-538a3374e5dc | name | | operating_status | NO_MONITOR | project_id | b15cde70d85749689e08106f973bb002 | protocol_port | 80 | provisioning_status | PENDING_CREATE | subnet_id | 32259126-dd37-44d5-922c-99d68ee870cd | updated_at | None | weight | 1 | monitor_port | None | monitor_address | None | backup | False | +---------------------+--------------------------------------+ . It should be noted here that the two IP’s from 10.0.0. * already exist, listening on port 80 (web server), and deliver a simple website with the information about their service name. Assuming these web servers in the following example are Ubuntu/Debian and you have root permissions, you could quickly create a simple web page with: . root@BeispielInstanz1:~# apt-get update; apt-get -y install apache2; echo \"you hit: you hit: webserver1\" &gt; /var/www/html/index.html . root@BeispielInstanz2:~# apt-get update; apt-get -y install apache2; echo \"you hit: you hit: webserver2\" &gt; /var/www/html/index.html . We can check the status of created members as follows: . $ openstack loadbalancer member list Beispiel-pool +--------------------------------------+------+----------------------------------+---------------------+--------------+---------------+------------------+--------+ | id | name | project_id | provisioning_status | address | protocol_port | operating_status | weight | +--------------------------------------+------+----------------------------------+---------------------+--------------+---------------+------------------+--------+ | 703e27e0-e7fe-474b-b32d-68f9a8aeef07 | b15cde70d85749689e08106f973bb002 | ACTIVE | 10.0.0.11 | 80 | NO_MONITOR | 1 | 2add1e17-73a6-4002-82af-538a3374e5dc | b15cde70d85749689e08106f973bb002 | ACTIVE | 10.0.0.12 | 80 | NO_MONITOR | 1 | +--------------------------------------+------+----------------------------------+---------------------+--------------+---------------+------------------+--------+ . Now the “internal” construct of the loadbalancer is configured. We now have: . | 2 members who provide the actual service via port 80 and between which the loadbalancing takes place, | a pool for this member, | a listener which listens on port TCP/80 and makes a ROUND_ROBIN to the two endpoints and | a loadbalancer, which we used to combine all components. | . The operating_status NO_MONITOR is corrected under healthmonitor. ",
    "url": "/optimist/networking/octavia_loadbalancer/#create-the-lb-member",
    
    "relUrl": "/optimist/networking/octavia_loadbalancer/#create-the-lb-member"
  },"406": {
    "doc": "Octavia Loadbalancers",
    "title": "Create and configure the floating IP",
    "content": "In order to be able to use the loadbalancer outside of our example network, we must reserve a floating IP and then link it to the vip_port_id of the example LB. Using the following command we can create a Floating IP from the provider network: . $ openstack floating ip create provider +---------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | Field | Value | +---------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | created_at | 2019-05-01T09:00:00Z | description | | dns_domain | None | dns_name | None | fixed_ip_address | None | floating_ip_address | 185.116.247.133 | floating_network_id | 54258498-a513-47da-9369-1a644e4be692 | id | 46c0e8cf-783d-44a0-8256-79f8ae0be7fe | location | Munch({'project': Munch({'domain_id': 'default', 'id': u'b15cde70d85749689e08106f973bb002', 'name': 'beispiel-tenant', 'domain_name': None}), 'cloud': '', 'region_name': 'fra', 'zone': None}) | name | 185.116.247.133 | port_details | None | port_id | None | project_id | b15cde70d85749689e08106f973bb002 | qos_policy_id | None | revision_number | 0 | router_id | None | status | DOWN | subnet_id | None | tags | [] | updated_at | 2019-05-01T09:00:00Z | +---------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ . In the next step we need the vip_port_id of the loadbalancer. Accomplished with the following command: . $ openstack loadbalancer show Beispiel-LB -f value -c vip_port_id 37fc5b34-ee07-49c8-b054-a8d591a9679f . With the following command we can now assign the public IP address to the loadbalancer. The LB (and thus also the endpoints behind it) can now be reached from the Internet. openstack floating ip set --port 37fc5b34-ee07-49c8-b054-a8d591a9679f 185.116.247.133 . We are now ready to test our loadbalancer deployment. With the following command we query our loadbalancer via port TCP/80 and then get a corresponding response from the single member: . $ for ((i=1;i&lt;=10;i++)); do curl http://185.116.247.133; sleep 1; done you hit: webserver1 you hit: webserver2 you hit: webserver1 you hit: webserver2 you hit: webserver1 you hit: webserver2 you hit: webserver1 ... (usw.) . ",
    "url": "/optimist/networking/octavia_loadbalancer/#create-and-configure-the-floating-ip",
    
    "relUrl": "/optimist/networking/octavia_loadbalancer/#create-and-configure-the-floating-ip"
  },"407": {
    "doc": "Octavia Loadbalancers",
    "title": "Create a healthmonitor",
    "content": "With the following command we create a monitor that, in the event of a failure of one of the backends, can remove a faulty backend from the load distribution, thus allowing the website or application to continue to be delivered cleanly. $ openstack loadbalancer healthmonitor create --delay 5 --max-retries 2 --timeout 10 --type HTTP --name Beispielmonitor --url-path / Beispiel-pool +---------------------+--------------------------------------+ | Field | Value | +---------------------+--------------------------------------+ | project_id | b15cde70d85749689e08106f973bb002 | name | Beispielmonitor | admin_state_up | True | pools | 4053e88e-c2b5-47c6-987e-4387d837c88d | created_at | 2019-05-01T09:00:00 | provisioning_status | PENDING_CREATE | updated_at | None | delay | 5 | expected_codes | 200 | max_retries | 2 | http_method | GET | timeout | 10 | max_retries_down | 3 | url_path | / | type | HTTP | id | 368f9baa-708c-4e7f-ace1-02de15598c5d | operating_status | OFFLINE | http_version | | domain_name | +---------------------+--------------------------------------+ . In this example, the monitor removes the failing backend from the pool if the integrity check (–type HTTP, –url-path / ) fails every two five-second intervals(–delay 5, –max-retries 2, –timeout 10). Once the server is restored and responds to TCP/80 again, it will be added back to the pool. A manual failover can be enforced if status code of the web server is not equal to “200” or in the event that there is no response from the web server at all. $ openstack loadbalancer healthmonitor show Beispielmonitor +---------------------+--------------------------------------+ | Field | Value | +---------------------+--------------------------------------+ | project_id | b15cde70d85749689e08106f973bb002 | name | Beispielmonitor | admin_state_up | True | pools | 4053e88e-c2b5-47c6-987e-4387d837c88d | created_at | 2019-05-01T09:00:00 | provisioning_status | ACTIVE | updated_at | 2019-05-01T09:00:00 | delay | 5 | expected_codes | 200 | max_retries | 2 | http_method | GET | timeout | 10 | max_retries_down | 3 | url_path | / | type | HTTP | id | 368f9baa-708c-4e7f-ace1-02de15598c5d | operating_status | ONLINE | http_version | | domain_name | +---------------------+--------------------------------------+ . From our deployment example, the result should look something like this: . you hit: webserver1 Mi 22 Mai 2019 17:09:39 CEST you hit: webserver2 Mi 22 Mai 2019 17:09:40 CEST you hit: webserver1 Mi 22 Mai 2019 17:09:41 CEST you hit: webserver2 Mi 22 Mai 2019 17:09:42 CEST you hit: webserver1 Mi 22 Mai 2019 17:09:43 CEST - up to now both webservers were online, but now the webserver2 went offline. you hit: webserver1 Mi 22 Mai 2019 17:09:44 CEST - still expected hit by ROUND_ROBIN you hit: webserver1 Mi 22 Mai 2019 17:09:50 CEST - first retry to webserver2 fails you hit: webserver1 Mi 22 Mai 2019 17:09:56 CEST - second retry to webserver2 fails you hit: webserver1 Mi 22 Mai 2019 17:10:01 CEST - the backend (webserver2) was taken out of the pool. you hit: webserver1 Mi 22 Mai 2019 17:10:02 CEST you hit: webserver1 Mi 22 Mai 2019 17:10:03 CEST you hit: webserver1 Mi 22 Mai 2019 17:10:04 CEST you hit: webserver1 Mi 22 Mai 2019 17:10:05 CEST you hit: webserver1 Mi 22 Mai 2019 17:10:06 CEST . ",
    "url": "/optimist/networking/octavia_loadbalancer/#create-a-healthmonitor",
    
    "relUrl": "/optimist/networking/octavia_loadbalancer/#create-a-healthmonitor"
  },"408": {
    "doc": "Octavia Loadbalancers",
    "title": "Monitoring with Prometheus",
    "content": "The Octavia amphora driver provides a Prometheus endpoint. This allows you to collect metrics from Octavia load balancers. To add a Prometheus endpoint on an existing Octavia load balancer, create a listener with the protocol PROMETHEUS. This will enable the endpoint as /metrics on the listener. The listener supports all the features of an Octavia load balancer, such as allowed_cidrs, but it does not support attaching pools or L7 policies. All metrics are identified by the Octavia object ID (UUID) of the resources. Note: Currently, UDP and SCTP metrics are not reported via Prometheus endpoints when using the amphora provider. For example, to create a Prometheus endpoint on port 8088 for load balancer lb1, run the following command: . $ openstack loadbalancer listener create --name stats-listener --protocol PROMETHEUS --protocol-port 8088 lb1 +-----------------------------+--------------------------------------+ | Field | Value | +-----------------------------+--------------------------------------+ | admin_state_up | True | connection_limit | -1 | created_at | 2021-10-03T01:44:25 | default_pool_id | None | default_tls_container_ref | None | description | | id | fb57d764-470a-4b6b-8820-627452f55b96 | insert_headers | None | l7policies | | loadbalancers | b081ed89-f6f8-48cb-a498-5e12705e2cf9 | name | stats-listener | operating_status | OFFLINE | project_id | 4c1caeee063747f8878f007d1a323b2f | protocol | PROMETHEUS | protocol_port | 8088 | provisioning_status | PENDING_CREATE | sni_container_refs | [] | timeout_client_data | 50000 | timeout_member_connect | 5000 | timeout_member_data | 50000 | timeout_tcp_inspect | 0 | updated_at | None | client_ca_tls_container_ref | None | client_authentication | NONE | client_crl_container_ref | None | allowed_cidrs | None | tls_ciphers | None | tls_versions | None | alpn_protocols | None | tags | +-----------------------------+--------------------------------------+ . Once the PROMETHEUS listener is ACTIVE, you can configure Prometheus to collect metrics from the load balancer by updating the prometheus.yml file. [scrape_configs] - job_name: 'Octavia LB1' static_configs: - targets: ['192.0.2.10:8088'] . ",
    "url": "/optimist/networking/octavia_loadbalancer/#monitoring-with-prometheus",
    
    "relUrl": "/optimist/networking/octavia_loadbalancer/#monitoring-with-prometheus"
  },"409": {
    "doc": "Octavia Loadbalancers",
    "title": "Known Issues",
    "content": "If you get the following error when assigning the public IP address to the loadbalancer: . ResourceNotFound: 404: Client Error for url: https://network.fra.optimist.gec.io/v2.0/floatingips/46c0e8cf-783d-44a0-8256-79f8ae0be7fe, External network 54258498-a513-47da-9369-1a644e4be692 is not reachable from subnet 32259126-dd37-44d5-922c-99d68ee870cd. Therefore, cannot associate Port 37fc5b34-ee07-49c8-b054-a8d591a9679f with a Floating IP. Then a connection between its example network (router) and the provider network is missing Step 10 . The default connect settings of the haproxy processes within an amphora is 50 seconds. If a connection lasts longer than 50 seconds, you must configure these values on the listener. An example of a connect with timeout: . $ time kubectl -n kube-system exec -ti machine-controller-5f649c5ff4-pksps /bin/sh ~ $ 50.69 real 0.08 user 0.05 sys . An example extending the timeout to 4h: . openstack loadbalancer listener set --timeout_client_data 14400000 &lt;Listener ID&gt; openstack loadbalancer listener set --timeout_member_data 14400000 &lt;Listener ID&gt; . If Octavia tries to start a LB in a network with port_security_enabled = False, the LB will end up in an ERROR state. ",
    "url": "/optimist/networking/octavia_loadbalancer/#known-issues",
    
    "relUrl": "/optimist/networking/octavia_loadbalancer/#known-issues"
  },"410": {
    "doc": "Octavia Loadbalancers",
    "title": "Conclusion",
    "content": "It always makes sense to establish a monitor for your pool. ",
    "url": "/optimist/networking/octavia_loadbalancer/#conclusion",
    
    "relUrl": "/optimist/networking/octavia_loadbalancer/#conclusion"
  },"411": {
    "doc": "Octavia Loadbalancers",
    "title": "Octavia Loadbalancers",
    "content": " ",
    "url": "/optimist/networking/octavia_loadbalancer/",
    
    "relUrl": "/optimist/networking/octavia_loadbalancer/"
  },"412": {
    "doc": "VPN as a Service",
    "title": "VPN as a Service (VPNaaS)",
    "content": "OpenStack supports on demand Site-to-Site VPNs as a service. This allows the user to connect two private networks to each other. To achieve this, OpenStack will configure a fully functional IPsec VPN within a project, without the need for additional networking VMs. ",
    "url": "/optimist/networking/vpnaas/#vpn-as-a-service-vpnaas",
    
    "relUrl": "/optimist/networking/vpnaas/#vpn-as-a-service-vpnaas"
  },"413": {
    "doc": "VPN as a Service",
    "title": "Setting up a Site-to-Site IPSec VPN",
    "content": "Create left and right networks and subnets . Before we can create a VPN, we need two separate networks to connect to each other. For this guide, we will create these networks in two different OpenStack projects which will be referred to as “left” and “right”. The following steps have to be repeated for both networks (“left” and “right”), assuming you want to connect two different OpenStack clusters. For the sake of simplicity, this guide will only demonstrate how to create the left network. For OpenStack, the steps to create the right network are exactly the same with exception to naming and subnet prefix In this example, we will be using the subnet prefix 2001:db8:1:33bc::/64 for the left network and 2001:db8:1:33bd::/64 for the right. If you already have two networks you would like to connect via Site-to-Site VPN, you can skip to creating IKE and IPSec policies. Using Horizon (GUI) . | Create the left network with a new subnet. | . Within your project, navigate to Network → Networks and click Create Network. Give your new network a name, select Enable Admin State to enable the network and Create Subnet to create the network and subnet all in one step. Click Next. Assign a name to your new network subnet, select Enter Network Address manually and enter your desired subnet within Network Address, if you would like to use your own subnet. To use a subnet from a predefined pool instead, select Allocate Network Address from a pool and pick a pool. Click Next. For documentation purposes, we will be using our own previously mentioned prefixes. Select Enable DHCP and IPv6 Address Configuration Mode “DHCPV6 STATEFUL”. Allocation pools will be generated automatically. Click Create. Using the CLI . | Create the left network using the openstack network create command. | . $ openstack network create vpnaas-left-network +---------------------------+--------------------------------------+ | Field | Value | +---------------------------+--------------------------------------+ | admin_state_up | UP | availability_zone_hints | | availability_zones | | created_at | 2022-09-12T12:45:42Z | description | | dns_domain | | id | ff7c61f1-4dcb-49bf-be9f-efdcaa1e0aaa | ipv4_address_scope | None | ipv6_address_scope | None | is_default | False | is_vlan_transparent | None | mtu | 1500 | name | vpnaas-left-network | port_security_enabled | True | project_id | 281fa14f782e4d4cbfd4e34a121c2680 | provider:network_type | None | provider:physical_network | None | provider:segmentation_id | None | qos_policy_id | None | revision_number | 1 | router:external | Internal | segments | None | shared | False | status | ACTIVE | subnets | | tags | | updated_at | 2022-09-12T12:45:42Z | +---------------------------+--------------------------------------+ . | Create a new subnet and allocate it to the newly created network, using the openstack subnet create command. | . $ openstack subnet create \\ vpnaas-left-network-subnet \\ --subnet-range 2001:db8:1:33bc::/64 --ip-version 6 \\ --network vpnaas-left-network +----------------------+--------------------------------------------------------+ | Field | Value | +----------------------+--------------------------------------------------------+ | allocation_pools | 2001:db8:1:33bc::1-2001:db8:1:33bc:ffff:ffff:ffff:ffff | cidr | 2001:db8:1:33bc::/64 | created_at | 2022-09-12T12:47:51Z | description | | dns_nameservers | | dns_publish_fixed_ip | None | enable_dhcp | True | gateway_ip | 2001:db8:1:33bc:: | host_routes | | id | e217a377-48c7-4c18-93b5-cfd805bde40a | ip_version | 6 | ipv6_address_mode | None | ipv6_ra_mode | None | name | vpnaas-left-network-subnet | network_id | ff7c61f1-4dcb-49bf-be9f-efdcaa1e0aaa | project_id | 281fa14f782e4d4cbfd4e34a121c2680 | revision_number | 0 | segment_id | None | service_types | | subnetpool_id | None | tags | | updated_at | 2022-09-12T12:47:51Z | +----------------------+--------------------------------------------------------+ . Create left and right routers . Using Horizon (GUI) . | Create a router with the provider network as an external gateway. | . Within your project, navigate to Network → Routers and click Create Router. Give your new router a name, select Enable Admin State to enable the router and “PROVIDER” as “External Network”. Click Create Router. Using the CLI . | Create a router using the openstack router create command. | . $ openstack router create vpnaas-left-router +-------------------------+--------------------------------------+ | Field | Value | +-------------------------+--------------------------------------+ | admin_state_up | UP | availability_zone_hints | | availability_zones | | created_at | 2022-09-12T12:48:15Z | description | | enable_ndp_proxy | None | external_gateway_info | null | flavor_id | None | id | 052e968a-a63b-4824-b904-eb70c42c53e5 | name | vpnaas-left-router | project_id | 281fa14f782e4d4cbfd4e34a121c2680 | revision_number | 2 | routes | | status | ACTIVE | tags | | tenant_id | 281fa14f782e4d4cbfd4e34a121c2680 | updated_at | 2022-09-12T12:48:15Z | +-------------------------+--------------------------------------+ . | Use the openstack router set command to set the provider network as an external gateway for the router. | . $ openstack router set vpnaas-left-router --external-gateway provider . Attach the subnet to the router . Using Horizon (GUI) . Within your project, navigate to Network → Routers and select the previously created router. Select Interfaces and click Add Interface. Select your subnet and click Submit. Using the CLI . Use the openstack router add subnet command to add the subnet to the router. $ openstack router add subnet vpnaas-left-router vpnaas-left-network-subnet . Create IKE and IPSec policies on both sides . The IKE and IPSec policies need to be configured identically on both sides. For the purpose of this guide, we will be using the following parameters. | Parameter | IKE Policy | IPSec Policy | . | Authorization algorithm | SHA256 | SHA256 | . | Encryption algorithm | AES-256 | AES-256 | . | Encapsulation mode | N/A | TUNNEL | . | IKE Version | V2 | N/A | . | Perfect Forward Secrecy | GROUP14 | GROUP14 | . | Transform Protocol | N/A | ESP | . Using Horizon (GUI) . | Create the IKE policy | . Within your project, navigate to Network → VPN, select IKE Policies and click Add IKE Policy. Give your IKE policy a name, and fill in the IKE policy parameters. Click Add. | Create the IPSec policy. | . Still within Network → VPN, select IPSec Policies and click Add IPsec Policy. Give your IPSec policy a name, and fill in the IPSec policy parameters. Click Add. Using the CLI . | Create the IKE policy using the openstack vpn ike policy create command. | . $ openstack vpn ike policy create \\ vpnaas-left-ike-policy \\ --auth-algorithm sha256 \\ --encryption-algorithm aes-256 \\ --ike-version v2 \\ --pfs group14 +-------------------------------+--------------------------------------+ | Field | Value | +-------------------------------+--------------------------------------+ | Authentication Algorithm | sha256 | Description | | Encryption Algorithm | aes-256 | ID | 561387b8-b5c1-415e-abc9-79ba93dd48ff | IKE Version | v2 | Lifetime | {'units': 'seconds', 'value': 3600} | Name | vpnaas-left-ike-policy | Perfect Forward Secrecy (PFS) | group14 | Phase1 Negotiation Mode | main | Project | 281fa14f782e4d4cbfd4e34a121c2680 | project_id | 281fa14f782e4d4cbfd4e34a121c2680 | +-------------------------------+--------------------------------------+ . | Create the IPSec policy using the openstack vpn ipsec policy create command. | . $ openstack vpn ipsec policy create \\ vpnaas-left-ipsec-policy \\ --auth-algorithm sha256 \\ --encryption-algorithm aes-256 \\ --pfs group14 \\ --transform-protocol esp +-------------------------------+--------------------------------------+ | Field | Value | +-------------------------------+--------------------------------------+ | Authentication Algorithm | sha256 | Description | | Encapsulation Mode | tunnel | Encryption Algorithm | aes-256 | ID | 553a600e-f39d-47a0-9550-97f2b4033685 | Lifetime | {'units': 'seconds', 'value': 3600} | Name | vpnaas-left-ipsec-policy | Perfect Forward Secrecy (PFS) | group14 | Project | 281fa14f782e4d4cbfd4e34a121c2680 | Transform Protocol | esp | project_id | 281fa14f782e4d4cbfd4e34a121c2680 | +-------------------------------+--------------------------------------+ . Create the VPN service on both sides . Using Horizon (GUI) . Within your project, navigate to Network → VPN, select VPN Services and click Add VPN Service. Give your VPN service a name, select your router and Enable Admin State. A subnet is not needed as we will be using endpoint groups. Click Add. Using the CLI . Use the openstack vpn service create command to create the VPN service. $ openstack vpn service create vpnaas-left-vpn --router vpnaas-left-router +----------------+--------------------------------------+ | Field | Value | +----------------+--------------------------------------+ | Description | | Flavor | None | ID | cc258fd7-0e87-4058-ad7d-355f32c1ab5e | Name | vpnaas-left-vpn | Project | 281fa14f782e4d4cbfd4e34a121c2680 | Router | 052e968a-a63b-4824-b904-eb70c42c53e5 | State | True | Status | PENDING_CREATE | Subnet | None | external_v4_ip | 185.116.244.85 | external_v6_ip | 2a00:c320:1003::23a | project_id | 281fa14f782e4d4cbfd4e34a121c2680 | +----------------+--------------------------------------+ . Create the endpoint groups . When using multiple subnets, make sure your VPN endpoint supports routing multiple subnets through the same connection. While OpenStack does, for implementations that do not support this, multiple endpoint groups need to be created, one for each subnet. Using Horizon (GUI) . | Create the local endpoint group for the left side. | . Within your project, navigate to Network → VPN, select Endpoint Groups and click Add Endpoint Group. Give your endpoint group a name, select the Type “Subnet” and select your subnet under Local System Subnets. Click Add. | Create the peer endpoint group for the left side. | . Still within Network → VPN, Endpoint Groups, click Add Endpoint Group again. Give your endpoint group a name, select the Type “CIDR” and enter the subnet of the right side. Click Add. Using the CLI . | Use the openstack vpn endpoint group create command to create the local endpoint group for the left side. | . $ openstack vpn endpoint group create \\ vpnaas-left-local \\ --type subnet \\ --value vpnaas-left-network-subnet +-------------+------------------------------------------+ | Field | Value | +-------------+------------------------------------------+ | Description | | Endpoints | ['e217a377-48c7-4c18-93b5-cfd805bde40a'] | ID | 949ccc53-5dc6-457d-95bf-278fdf9a3e5d | Name | vpnaas-left-local | Project | 281fa14f782e4d4cbfd4e34a121c2680 | Type | subnet | project_id | 281fa14f782e4d4cbfd4e34a121c2680 | +-------------+------------------------------------------+ . | Use the openstack vpn endpoint group create command again to create the peer endpoint group for the left side. | . $ openstack vpn endpoint group create \\ vpnaas-left-remote \\ --type cidr \\ --value 2001:db8:1:33bd::/64 +-------------+--------------------------------------+ | Field | Value | +-------------+--------------------------------------+ | Description | | Endpoints | ['2001:db8:1:33bd::/64'] | ID | 9146346d-1306-4b03-a3ce-04ee51832ed8 | Name | vpnaas-left-remote | Project | 281fa14f782e4d4cbfd4e34a121c2680 | Type | cidr | project_id | 281fa14f782e4d4cbfd4e34a121c2680 | +-------------+--------------------------------------+ . Create the site connections . Just like with the endpoint groups, if your VPN endpoint does not support routing multiple subnets through the same connection, you need to create multiple site connections, one for each subnet/endpoint group. Using Horizon (GUI) . Within your project, navigate to Network → VPN, select IPSec Site Connections and click Add IPSec Site Connection. Give your connection a name, select the previously created VPN service, local endpoint group, IKE and IPSec policy, Pre-Shared Key, Peer IP and router identity. For the purposes of this guide, we will assume that 2001:db8::4:703 is the IP address of the right router. Using the CLI . Use the openstack vpn ipsec site connection create command to create the VPN service. $ openstack vpn ipsec site connection create \\ vpnaas-left-connection \\ --vpnservice vpnaas-left-vpn \\ --ikepolicy vpnaas-left-ike-policy \\ --ipsecpolicy vpnaas-left-ipsec-policy \\ --local-endpoint-group vpnaas-left-local \\ --peer-address 2001:db8::4:703 \\ --peer-id 2001:db8::4:703 \\ --peer-endpoint-group vpnaas-left-remote \\ --psk 1gHAsAeR8lFEDDu7 +--------------------------+----------------------------------------------------+ | Field | Value | +--------------------------+----------------------------------------------------+ | Authentication Algorithm | psk | Description | | ID | d81dbe28-ccda-4ee3-ba96-145fadc74e0f | IKE Policy | 561387b8-b5c1-415e-abc9-79ba93dd48ff | IPSec Policy | 553a600e-f39d-47a0-9550-97f2b4033685 | Initiator | bi-directional | Local Endpoint Group ID | 949ccc53-5dc6-457d-95bf-278fdf9a3e5d | Local ID | | MTU | 1500 | Name | vpnaas-left-connection | Peer Address | 2001:db8::4:703 | Peer CIDRs | | Peer Endpoint Group ID | 9146346d-1306-4b03-a3ce-04ee51832ed8 | Peer ID | 2001:db8::4:703 | Pre-shared Key | 1gHAsAeR8lFEDDu7 | Project | 281fa14f782e4d4cbfd4e34a121c2680 | Route Mode | static | State | True | Status | PENDING_CREATE | VPN Service | cc258fd7-0e87-4058-ad7d-355f32c1ab5e | dpd | {'action': 'hold', 'interval': 30, 'timeout': 120} | project_id | 281fa14f782e4d4cbfd4e34a121c2680 | +--------------------------+----------------------------------------------------+ . ",
    "url": "/optimist/networking/vpnaas/#setting-up-a-site-to-site-ipsec-vpn",
    
    "relUrl": "/optimist/networking/vpnaas/#setting-up-a-site-to-site-ipsec-vpn"
  },"414": {
    "doc": "VPN as a Service",
    "title": "VPN as a Service",
    "content": " ",
    "url": "/optimist/networking/vpnaas/",
    
    "relUrl": "/optimist/networking/vpnaas/"
  },"415": {
    "doc": "Storage",
    "title": "Storage",
    "content": " ",
    "url": "/optimist/storage/",
    
    "relUrl": "/optimist/storage/"
  },"416": {
    "doc": "S3 Compatible Object Storage",
    "title": "S3 Compatible Object Storage Introduction",
    "content": " ",
    "url": "/optimist/storage/s3_documentation/#s3-compatible-object-storage-introduction",
    
    "relUrl": "/optimist/storage/s3_documentation/#s3-compatible-object-storage-introduction"
  },"417": {
    "doc": "S3 Compatible Object Storage",
    "title": "What exactly is Object Storage?",
    "content": "Object storage is an alternative to classic block storage. The individual data is not assigned to individual blocks on a device, but is stored as binary objects within a storage cluster. The necessary metadata is stored in a separate database. The storage cluster consists of several individual servers (nodes), which in turn have several storage devices installed. With the storage devices, we have a mix of classic HDDs, SSDs, and modern NVMe solutions. The CRUSH algorithm, which is implemented on the server-side of the Object Storage, decides which device an object ultimately lands on. ",
    "url": "/optimist/storage/s3_documentation/#what-exactly-is-object-storage",
    
    "relUrl": "/optimist/storage/s3_documentation/#what-exactly-is-object-storage"
  },"418": {
    "doc": "S3 Compatible Object Storage",
    "title": "How can I access it?",
    "content": "Access to this type of storage is accomplished exclusively via HTTPS. For this, we provide a highly available endpoint, where the individual operations can be executed. We support two independent protocols: . | S3 | Swift | . S3 is a protocol created by Amazon in order to work with this type of data. Swift is the protocol provided by the OpenStack service of the same name. Regardless of which protocol you use, you always have access to all your data. You can, therefore, use both protocols in combination. There are tools for all common platforms to work with the data in the object storage: . | Windows: s3cmd, Cyberduck | MacOS: s3cmd, Cyberduck | Linux: s3cmd | . In addition, there are integrations in all popular programming languages. ",
    "url": "/optimist/storage/s3_documentation/#how-can-i-access-it",
    
    "relUrl": "/optimist/storage/s3_documentation/#how-can-i-access-it"
  },"419": {
    "doc": "S3 Compatible Object Storage",
    "title": "How secure is my data?",
    "content": "The Object Storage is based on our Openstack Cloud Platform with the distributed Ceph Storage Cluster. The objects are distributed and replicated on the server-side across several storage devices. Ceph ensures the replication and integrity of the data sets. If a server or hard disk fails, the affected data records are replicated to available servers and the desired replication level is automatically restored. In addition, the data is mirrored to another data centre on another dedicated storage cluster and can be used from there in the event of a disaster. ",
    "url": "/optimist/storage/s3_documentation/#how-secure-is-my-data",
    
    "relUrl": "/optimist/storage/s3_documentation/#how-secure-is-my-data"
  },"420": {
    "doc": "S3 Compatible Object Storage",
    "title": "Advantages at a glance",
    "content": ". | Deployment via API: The HTTPS interface is compatible with both the Amazon S3 API and the OpenStack Swift API. | Supports all common operating systems and programming languages. | Full scalability - Storage can be used dynamically. | Maximum reliability thanks to integrated replication and mirroring via 2 independent data centres. | Access is possible from almost any internet-enabled device - thus a good alternative to NFS and Co. | PAYG billing according to used monthly average. | Transparent billing and therefore good predictability - no extra traffic costs or costs for access to the data. | Ability to define s3 lifecycle policies to manage the objects inside the buckets. | . ",
    "url": "/optimist/storage/s3_documentation/#advantages-at-a-glance",
    
    "relUrl": "/optimist/storage/s3_documentation/#advantages-at-a-glance"
  },"421": {
    "doc": "S3 Compatible Object Storage",
    "title": "S3 Compatible Object Storage",
    "content": " ",
    "url": "/optimist/storage/s3_documentation/",
    
    "relUrl": "/optimist/storage/s3_documentation/"
  },"422": {
    "doc": "Create and Use S3 Credentials",
    "title": "Create and Use S3 Credentials",
    "content": "Contents: . | Create S3 credentials | Entering User Data in the Configuration File . | S3cmd | S3Browser | Cyberduck | Boto3 | . | Show s3 credentials | Delete s3 credentials | . ",
    "url": "/optimist/storage/s3_documentation/createanduses3credentials/",
    
    "relUrl": "/optimist/storage/s3_documentation/createanduses3credentials/"
  },"423": {
    "doc": "Create and Use S3 Credentials",
    "title": "Create S3 credentials",
    "content": "In order to access Object Storage, we first need login data (credentials). To generate this data via the OpenStackAPI, we need to use the OpenStack Client and execute the following command there: . $ openstack ec2 credentials create . If the data has been created correctly, the output will be similar to the below: . $ openstack ec2 credentials create +------------+-----------------------------------------------------------------+ | Field | Value | +------------+-----------------------------------------------------------------+ | access | aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa | links | {u'self': u'https://identity.optimist.gec.io/v3/users/bbb | | bbbbbbbbbbbbbbbbbbbbbbbbbbbbb/credentials/OS- | | EC2/aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa'} | project_id | cccccccccccccccccccccccccccccccc | secret | dddddddddddddddddddddddddddddddd | trust_id | None | user_id | bbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbb | +------------+-----------------------------------------------------------------+ . Once the credentials are available, we need a way to access the S3 compatible ObjectStorage. For this, there are different options, in this documentation we present 3 possibilities: S3cmd for Linux/Mac, S3Browser for Windows, Cyberduck and Boto3. ",
    "url": "/optimist/storage/s3_documentation/createanduses3credentials/#create-s3-credentials",
    
    "relUrl": "/optimist/storage/s3_documentation/createanduses3credentials/#create-s3-credentials"
  },"424": {
    "doc": "Create and Use S3 Credentials",
    "title": "Entering User Data in the Configuration File",
    "content": "S3cmd . To install s3cmd, we need a package manager such as “pip”. The installation and usage is explained in Step 4: “Our way to the console” of our Guided Tour. Once pip is installed, the command for the installation of S3cmd is then: . $ pip install s3cmd . After the successful installation of S3cmd, the previously created credentials must be entered into S3cmd configuration file. The file responsible for this is “.s3cfg”, which is located in the home directory by default. If this file does not yet exist, it must first be created. We then enter the following data in the .s3cfg and save it: . access_key = aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa check_ssl_certificate = True check_ssl_hostname = True host_base = s3.es1.fra.optimist.gec.io host_bucket = s3.es1.fra.optimist.gec.io secret_key = dddddddddddddddddddddddddddddddd use_https = True . S3Browser . The S3Browser can be downloaded here and easily installed afterwards. After this has been successfully installed, we then need to enter all necessary credentials. To do this, we open the S3Browser and the following window opens automatically the first time we run the application: . Here we enter the following values and click on “Add new account”. * Account Name: Freely selectable name for the account. * Account Type: S3 Compatible Storage * REST Endpoint: s3.es1.fra.optimist.gec.io * Signature Version: Signature V2 * Access Key ID: The corresponding Access Key (in our example: ddddddddddddddddddddddddddddddddddddd) * Secret Access Key: The corresponding secret (in our example: ddddddddddddddddddddddddddddddddddddd) . Cyberduck . To use Cyberduck it is first necessary to download the application here. After installing and running the program for the first time, click on “New connection”. (1) A new window opens in which “Amazon S3” is selected in the dropdown menu (2). The following data is then required: . | Server(3): s3.es1.fra.optimist.gec.io | Access Key ID(4): The corresponding Access Key (in our example: ddddddddddddddddddddddddddddddddddddd) | Secret Access Key(5): The corresponding Secret (In the example: dddddddddddddddddddddddddddddddddd) | . Finally, to establish a connection, click on “Connect”. Boto3 . To install boto3, we need a package manager such as “pip”. The installation and usage of pip is explained in Step 4: “Our way to the console” of our Guided Tour. Once pip is installed, the command for the installation of Boto3 is then: . $ pip install boto3 . After the successful installation of Boto3 it is now usable; it is important that Boto3 creates a script which is executed at the end. Therefore, the configuration section which is shown below, is always part of subsequent scripts used later. For this we create a Python file such as “Example.py” and add the following content: . | endpoint_url: s3.es1.fra.optimist.gec.io | aws_access_key_id: The corresponding Access Key (in our example: dddddddddddddddddddddddddddddddddddd) | aws_secret_access_key: The corresponding Secret (In the example: dddddddddddddddddddddddddddddddddd) | . #!/usr/bin/env/python import boto3 from botocore.client import Config s3 = boto3.resource('s3', endpoint_url='https://s3.es1.fra.optimist.gec.io', aws_access_key_id='aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa', aws_secret_access_key='dddddddddddddddddddddddddddddddd', ) . This serves as a starting point and is referenced and used in the following scripts. ",
    "url": "/optimist/storage/s3_documentation/createanduses3credentials/#entering-user-data-in-the-configuration-file",
    
    "relUrl": "/optimist/storage/s3_documentation/createanduses3credentials/#entering-user-data-in-the-configuration-file"
  },"425": {
    "doc": "Create and Use S3 Credentials",
    "title": "Show s3 credentials",
    "content": "In order to show existing Object Storage ec2-credentials we need to use the OpenStack Client and execute the following command there: . $ openstack ec2 credentials list . the output will be similar to the below: . $ openstack ec2 credentials list +----------------------------------+----------------------------------+----------------------------------+----------------------------------+ | Access | Secret | Project ID | User ID | +----------------------------------+----------------------------------+----------------------------------+----------------------------------+ | aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa | xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx | 12341234123412341234123412341234 | 32132132132132132132132132132132 | bbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbb | yyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyy | 56756756756756756756756756756756 | 65465465465465465465465465465465 | cccccccccccccccccccccccccccccccc | zzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz | 89089089089089089089089089089089 | 09809809809809809809809809809809 | +----------------------------------+----------------------------------+----------------------------------+----------------------------------+ . ",
    "url": "/optimist/storage/s3_documentation/createanduses3credentials/#show-s3-credentials",
    
    "relUrl": "/optimist/storage/s3_documentation/createanduses3credentials/#show-s3-credentials"
  },"426": {
    "doc": "Create and Use S3 Credentials",
    "title": "Delete s3 credentials",
    "content": "In order to delete Object Storage ec2-credentials we need to use the OpenStack Client and execute the following command there: . $ openstack ec2 credentials delete &lt;access-key&gt; . ",
    "url": "/optimist/storage/s3_documentation/createanduses3credentials/#delete-s3-credentials",
    
    "relUrl": "/optimist/storage/s3_documentation/createanduses3credentials/#delete-s3-credentials"
  },"427": {
    "doc": "Create and Delete a Bucket",
    "title": "Create and Delete a Bucket",
    "content": "Contents: . | S3cmd | S3Browser | Cyberduck | Boto3 | . To upload your data (documents, photos, videos, etc.) it is necessary to create a bucket, which is similar to a folder. First create an S3 bucket and then you can upload as many objects as required into the bucket. Due to the way our object storage works, it is necessary to use a globally unique name for your bucket. If a bucket with the selected name already exists, the name cannot be used until the existing bucket has been deleted. If the desired name is already in use by another customer, you must choose another name. It is advisable to use names of the format “content-description.bucket.my-domain.tld” or similar. ",
    "url": "/optimist/storage/s3_documentation/createanddeletebucket/",
    
    "relUrl": "/optimist/storage/s3_documentation/createanddeletebucket/"
  },"428": {
    "doc": "Create and Delete a Bucket",
    "title": "S3cmd",
    "content": "Create a bucket . To create a bucket, use the following command: . s3cmd mb s3://NameOfTheBucket . The output in the command line will look similar to this: . $ s3cmd mb s3://iNNOVO-Test Bucket 's3://iNNOVO-Test/' created . Delete a bucket . To delete a bucket, use the following command: . s3cmd rb s3://NameOfTheBucket . The output in the command line will look similar to this: . $ s3cmd rb s3://iNNOVO-Test Bucket 's3://iNNOVO-Test/' removed . ",
    "url": "/optimist/storage/s3_documentation/createanddeletebucket/#s3cmd",
    
    "relUrl": "/optimist/storage/s3_documentation/createanddeletebucket/#s3cmd"
  },"429": {
    "doc": "Create and Delete a Bucket",
    "title": "S3Browser",
    "content": "Create a bucket . After opening S3Browser, we click on “New bucket”(1) in the upper left corner, in the newly opened window, we assign the name of the bucket via “Bucket name”(2) and then click on “Create new bucket”(3). Delete a bucket . First select the bucket you want to delete(1) and then click on “Delete bucket”(2) in the upper left corner. In the window that opens, confirm that you want to delete the file by checking the checkbox(1) and then click on “Delete Bucket”(2). ",
    "url": "/optimist/storage/s3_documentation/createanddeletebucket/#s3browser",
    
    "relUrl": "/optimist/storage/s3_documentation/createanddeletebucket/#s3browser"
  },"430": {
    "doc": "Create and Delete a Bucket",
    "title": "Cyberduck",
    "content": "Create a bucket . After opening Cyberduck, we click on “Action”(1) and on “New folder”(2) in the middle of the top. A new window opens, here we can define the name(1) and confirm this with “Create”(2): . Delete a bucket . To delete a bucket, select it with a left mouse click. The bucket is then deleted via “Action”(1) and “Delete”(2). Confirm the action by clicking on “Delete”(1) once again. ",
    "url": "/optimist/storage/s3_documentation/createanddeletebucket/#cyberduck",
    
    "relUrl": "/optimist/storage/s3_documentation/createanddeletebucket/#cyberduck"
  },"431": {
    "doc": "Create and Delete a Bucket",
    "title": "Boto3",
    "content": "In Boto3 we first need the S3 identifier so that a script can be used. For details see: Create and use S3 credentials #Boto3 . Create a bucket . To create a bucket, we first need a client for it and we will then create the bucket afterwards. One option looks like this: . ## Create the S3 client s3 = boto3.client('s3') ## Create a bucket s3.create_bucket(Bucket='iNNOVO-Test') . A complete script for boto 3 including authentication may look like this: . #!/usr/bin/env/python ## Define that boto3 should be used import boto3 from botocore.client import Config ## Authentication s3 = boto3.resource('s3', endpoint_url='https://s3.es1.fra.optimist.gec.io', aws_access_key_id='aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa', aws_secret_access_key='bbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbb', ) ## Create the S3 client s3 = boto3.client('s3') ## Create a bucket s3.create_bucket(Bucket='iNNOVO-Test') . Delete a bucket . As before, we first need a client to delete the bucket. One option looks like this: . ## Create the S3 client s3 = boto3.client('s3') ## Delete a bucket s3.delete_bucket(Bucket='iNNOVO-Test') . A complete script for boto 3 including authentication may look like this: . #!/usr/bin/env/python ## Define that boto3 should be used import boto3 from botocore.client import Config ## Authentication s3 = boto3.resource('s3', endpoint_url='https://s3.es1.fra.optimist.gec.io', aws_access_key_id='aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa', aws_secret_access_key='bbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbb', ) ## Create the S3 client s3 = boto3.client('s3') ## Delete a bucket s3.delete_bucket(Bucket='iNNOVO-Test') . ",
    "url": "/optimist/storage/s3_documentation/createanddeletebucket/#boto3",
    
    "relUrl": "/optimist/storage/s3_documentation/createanddeletebucket/#boto3"
  },"432": {
    "doc": "Upload and delete an object",
    "title": "Upload and delete an object",
    "content": " ",
    "url": "/optimist/storage/s3_documentation/uploadanddeleteobject/",
    
    "relUrl": "/optimist/storage/s3_documentation/uploadanddeleteobject/"
  },"433": {
    "doc": "Upload and delete an object",
    "title": "Contents:",
    "content": ". | S3cmd | S3Browser | Cyberduck | Boto3 | . To upload your data (documents, photos, videos, etc.) it is first necessary to create a bucket. A file can only be saved in a bucket. ",
    "url": "/optimist/storage/s3_documentation/uploadanddeleteobject/#contents",
    
    "relUrl": "/optimist/storage/s3_documentation/uploadanddeleteobject/#contents"
  },"434": {
    "doc": "Upload and delete an object",
    "title": "S3cmd",
    "content": "Upload an object . To upload a file, use the following command: . s3cmd put NameOfTheFile s3://NameOfTheBucket/NameOfTheFile . The output in the command will be similar to this: . $ s3cmd put innovo.txt s3://innovo-test/innovo.txt upload: 'innovo.txt' -&gt; 's3://innovo-test/innovo.txt' [1 of 1] 95 of 95 100% in 0s 176.63 B/s done . Delete an object . To delete a file, use the following command: . s3cmd del s3://NameOfTheBucket/NameOfTheFile . The output in the command will be similar to this: . $ s3cmd del s3://innovo-test/innovo.txt delete: 's3://innovo-test/innovo.txt' . ",
    "url": "/optimist/storage/s3_documentation/uploadanddeleteobject/#s3cmd",
    
    "relUrl": "/optimist/storage/s3_documentation/uploadanddeleteobject/#s3cmd"
  },"435": {
    "doc": "Upload and delete an object",
    "title": "S3Browser",
    "content": "Upload an object . After opening S3Browser, we click on the desired “Bucket”(1), then select “Upload”(2) and finally “Upload file(s)”(3) . Here we select the file(1) and click on Open(2). Delete an object . To delete a file, select it with a left mouse click(1). Then click on “Delete”(2). Finally, confirm the action with “Yes”. ",
    "url": "/optimist/storage/s3_documentation/uploadanddeleteobject/#s3browser",
    
    "relUrl": "/optimist/storage/s3_documentation/uploadanddeleteobject/#s3browser"
  },"436": {
    "doc": "Upload and delete an object",
    "title": "Cyberduck",
    "content": "Upload an object . After opening Cyberduck, click on the Bucket(1), then click on Action(2) and then on Upload(3). Here we choose our file and click on Upload. Delete an object . To delete a file, select it with a left mouse click(1). It is then deleted via “Action”(2) and “Delete”(3). This action is then confirmed by clicking on “Delete” again. ",
    "url": "/optimist/storage/s3_documentation/uploadanddeleteobject/#cyberduck",
    
    "relUrl": "/optimist/storage/s3_documentation/uploadanddeleteobject/#cyberduck"
  },"437": {
    "doc": "Upload and delete an object",
    "title": "Boto3",
    "content": "At boto3 we first need the S3 identifier so that a script can be used. For details: Create and use S3 credentials #Boto3. Upload an object . To upload a file, we have to use a client and specify the bucket which the file should be uploaded to. One option could look like this: . ## Create the S3 client s3 = boto3.client('s3') ## Upload an object s3.upload_file(Bucket='iNNOVO-Test', Key='innovo.txt') . A complete script for boto 3 including authentication may be similar to this: . #!/usr/bin/env/python ## Define that boto3 should be used import boto3 from botocore.client import Config ## Authentication s3 = boto3.resource('s3', endpoint_url='https://s3.es1.fra.optimist.gec.io', aws_access_key_id='aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa', aws_secret_access_key='bbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbb', ) ## Create the S3 client s3 = boto3.client('s3') ## Upload an object s3.upload_file(Bucket='iNNOVO-Test', Key='innovo.txt') . Delete an object . As well as being used to upload a file, the client is also required to delete the file. For this, we specify the bucket in which the file is stored, in addition to the file itself. One option could look like this: . ## Create the S3 client s3 = boto3.client('s3') ## Delete an object s3.delete_object(Bucket='iNNOVO-Test', Key='innovo.txt') . A complete script for boto 3 including authentication may look like this: . #!/usr/bin/env/python ## Define that boto3 should be used import boto3 from botocore.client import Config ## Authentication s3 = boto3.resource('s3', endpoint_url='https://s3.es1.fra.optimist.gec.io', aws_access_key_id='aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa', aws_secret_access_key='bbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbb', ) ## Create the S3 client s3 = boto3.client('s3') ## Delete an object s3.delete_object(Bucket='iNNOVO-Test', Key='innovo.txt') . ",
    "url": "/optimist/storage/s3_documentation/uploadanddeleteobject/#boto3",
    
    "relUrl": "/optimist/storage/s3_documentation/uploadanddeleteobject/#boto3"
  },"438": {
    "doc": "Enable and Disable Versioning, and Delete a versioned object",
    "title": "Enable and Disable Versioning, and Delete a versioned object",
    "content": " ",
    "url": "/optimist/storage/s3_documentation/versioning/",
    
    "relUrl": "/optimist/storage/s3_documentation/versioning/"
  },"439": {
    "doc": "Enable and Disable Versioning, and Delete a versioned object",
    "title": "Contents:",
    "content": ". | S3cmd | S3Browser | Cyberduck | Boto3 | . Versioning makes it possible to store multiple versions of an object in a bucket. For example, files named innovo.txt (version 1) and innovo.txt (version 2) can be stored in a single bucket. Versioning can protect you from the consequences of accidental overwrites or deletion. ",
    "url": "/optimist/storage/s3_documentation/versioning/#contents",
    
    "relUrl": "/optimist/storage/s3_documentation/versioning/#contents"
  },"440": {
    "doc": "Enable and Disable Versioning, and Delete a versioned object",
    "title": "S3cmd",
    "content": "With S3cmd it is not possible to enable versioning or to delete versioned files. ",
    "url": "/optimist/storage/s3_documentation/versioning/#s3cmd",
    
    "relUrl": "/optimist/storage/s3_documentation/versioning/#s3cmd"
  },"441": {
    "doc": "Enable and Disable Versioning, and Delete a versioned object",
    "title": "S3Browser",
    "content": "Enable versioning . To enable versioning, select a Bucket(1). Right click on the bucket and then click on “Edit Versioning Settings”(2). In the newly opened window, click the checkbox for “Enable versioning for bucket”(1) and confirm this with “OK”(2). Disable versioning . To disable versioning, select a Bucket(1). Right-click on the bucket and select “Edit Versioning Settings”(2). In the newly opened window, remove the checkbox at “Enable versioning for bucket”(1) and confirm this with “OK”(2). Delete a versioned object . This is not possible in the free version of S3Browser. ",
    "url": "/optimist/storage/s3_documentation/versioning/#s3browser",
    
    "relUrl": "/optimist/storage/s3_documentation/versioning/#s3browser"
  },"442": {
    "doc": "Enable and Disable Versioning, and Delete a versioned object",
    "title": "Cyberduck",
    "content": "To see the different versions of a file, hidden files must be displayed. This option can be found at View(1) → Show hidden files(2) . Enable versioning . After opening Cyberduck, we select the file on which we wish to activate versioning(1) for. Then click Action(2) and Info(3). Then the following window opens, here we check the box “Bucket Versioning”(1): . Disable versioning . To disable versioning, we select the file(1) again, go to Action(2) and Info(3). In the window that opens, the check mark for “Bucket Versioning” should be removed. Delete a versioned object . Simply select the file to be deleted(1) and click Action(2) → Delete(3) to remove it. ",
    "url": "/optimist/storage/s3_documentation/versioning/#cyberduck",
    
    "relUrl": "/optimist/storage/s3_documentation/versioning/#cyberduck"
  },"443": {
    "doc": "Enable and Disable Versioning, and Delete a versioned object",
    "title": "Boto3",
    "content": "In Boto3 we first need S3 credentials so that a script can be used. For details see: Create and use S3 credentials #Boto3. Enable versioning . To enable the versioning, we will enter the bucket first and then activate the versioning. One option looks like this: . ## Specifies the bucket in which versioning is to be activated. bucket = s3.Bucket('iNNOVO-Test') ## Activate versioning bucket.configure_versioning(True) . A complete script for boto 3, including authentication, could look like this: . #!/usr/bin/env/python ## Define that boto3 should be used import boto3 from botocore.client import Config ## Authentication s3 = boto3.resource('s3', endpoint_url='https://s3.es1.fra.optimist.gec.io', aws_access_key_id='aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa', aws_secret_access_key='bbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbb', ) ## Specifies the bucket in which versioning is to be activated. bucket = s3.Bucket('iNNOVO-Test') ## Activate versioning bucket.configure_versioning(True) . Disable versioning . As with the activation of versioning, the bucket is needed to deactivate versioning. One option looks like this: . ## Specifies the bucket in which versioning is to be activated. bucket = s3.Bucket('iNNOVO-Test') ## Deactivate versioning bucket.configure_versioning(False) . A complete script for boto 3 including authentication could look like this: . #!/usr/bin/env/python ## Define that boto3 should be used import boto3 from botocore.client import Config ## Authentication s3 = boto3.resource('s3', endpoint_url='https://s3.es1.fra.optimist.gec.io', aws_access_key_id='aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa', aws_secret_access_key='bbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbb', ) ## Specifies the bucket in which versioning is to be activated. bucket = s3.Bucket('iNNOVO-Test') ## Deactivate versioning bucket.configure_versioning(False) . Delete a versioned object . To delete a versioned object completely, the following command is helpful: . ## Specifies the bucket in which versioning is to be activated. bucket = s3.Bucket('iNNOVO-Test') ## Delete versioned object bucket.object_versions.all().delete('innovo.txt') . ",
    "url": "/optimist/storage/s3_documentation/versioning/#boto3",
    
    "relUrl": "/optimist/storage/s3_documentation/versioning/#boto3"
  },"444": {
    "doc": "S3 Security",
    "title": "S3 Security",
    "content": " ",
    "url": "/optimist/storage/s3_documentation/security/",
    
    "relUrl": "/optimist/storage/s3_documentation/security/"
  },"445": {
    "doc": "S3 Security",
    "title": "Introduction",
    "content": "This page gives an overview of the following topics relating to S3 buckets / Swift: . | Container Access Control Lists (ACLs) | Bucket Policies | . Operations on container ACLs must be performed at the OpenStack level using Swift commands, while Bucket Policies must be set on each bucket within a project using the s3cmd command line. In this document we will outline some examples of each type of operation. ",
    "url": "/optimist/storage/s3_documentation/security/#introduction",
    
    "relUrl": "/optimist/storage/s3_documentation/security/#introduction"
  },"446": {
    "doc": "S3 Security",
    "title": "Container Access Control Lists (ACLs)",
    "content": "By default, only project owners have permission to create, read and modify containers and objects. However, an owner can grant access to other users on by using an Access Control List (ACL). The ACL can be set on each container, and is applicable only to that container and the objects within that container. Some of the main elements which can be used to set an ACL on a container are listed below: . | Element | Description | . | .r:*. | Any user has access to objects. No token is required in the request. | . | .r:&lt;referrer&gt; | The referrer is granted access to objects. The referrer is identified by the Referer request header in the request. No token is required. | . | .r:-&lt;referrer&gt; | This syntax (with “-” prepended to the referrer) is supported. However, it does not deny access if another element (e.g., .r:*) grants access. | . | .rlistings | Any user can perform HEAD or GET operations on the container if the user also has read access on objects (e.g., also has .r:* or .r:&lt;referrer&gt;. No token is required. | . As an example, we will set the policy .r:*. on a container called &lt;example-container&gt;. This policy will allow any external user access to the objects within the container. swift post example-container --read-acl \".r:*\" . Conversely, we can also allow users to list but not access the list of objects within a container by setting the .rlistings policy on our example-container: . swift post example-container --read-acl \".rlistings\" . To remove any read policy and set the container to its default private state, the following command can be used: . swift post -r \"\" example-container . To check which ACL is set on a container, use the following command. swift stat example-container . This gives an overview of the stats for the container and displays the current ACL rule for a container. Prevent Listing on Containers when using the .r:*. policy: . In the current version of OpenStack, to prevent the contents from being listed while using the .r:*. policy on a container, we recommend creating an empty index.html object within the container. This will allow users to download objects without listing the contents of the buckets. This can be accomplished with the following steps: . Firstly, add the blank index.html file to our example-container: . swift post -m 'web-index: index.html’ example-container . Then create the index.html file as an object within the container: . touch index.html &amp;&amp; openstack object create example-container index.html . This will allow external users access to specific files without listing the contents of the container. ",
    "url": "/optimist/storage/s3_documentation/security/#container-access-control-lists-acls",
    
    "relUrl": "/optimist/storage/s3_documentation/security/#container-access-control-lists-acls"
  },"447": {
    "doc": "S3 Security",
    "title": "Bucket Policies",
    "content": "Bucket policies are used to control access to each bucket in a project. It is recommended to set a policy on all buckets upon creation. The first step is to create a policy as follows. The following template only requires that you change the bucket name for subsequent policies, The below example creates a policy for bucket example-bucket: . cat &gt; examplepolicy { \"Version\": \"2008-10-17\", \"Statement\": [ { \"Sid\": \"AddPerm\", \"Effect\": \"Allow\", \"Principal\": \"*\", \"Action\": \"s3:GetObject\", \"Resource\": \"arn:aws:s3:::example-bucket/*\" } ] } . Breakdown of each element within the above policy example: . | Version: Specifies the language syntax rules to be used to process the policy. It is recommended to always use: “2012-10-17” as this is the current version of the policy language. | Statement: The main element of the policy, the other elements are located within this statement. | SID: The Statement ID, this is an optional identifier which can be used to describe the policy statement. Recommended so that the purpose of each policy is clear. | Effect: Set either to “Allow” or “Deny” | Principal: Specifies the principal that is allowed or denied access to a resource. Here, the wildcard “*” is used to apply the rule to all. | Action: Describes the specific actions that will be allowed or denied. | . (For further information on the available policy options and how to tailor this to your specific needs, please see the official AWS documentation). Next, apply the newly created policy to bucket example-bucket: . s3cmd setpolicy examplepolicy s3://example-bucket . You will also be able to run the following command afterwards to see that the policy is in place: . s3cmd info s3://example-bucket . Once the policy is applied, you can set once again set Public Access: Disabled on the Dashboard. Once the above steps have been taken we will have the following results: . | The container will be private, and files will not be listed or displayed via XML. | The policy now allows access to specific files with a direct link. | . ",
    "url": "/optimist/storage/s3_documentation/security/#bucket-policies",
    
    "relUrl": "/optimist/storage/s3_documentation/security/#bucket-policies"
  },"448": {
    "doc": "Swift - Serving a Static Website",
    "title": "Swift - Serving Static Websites",
    "content": " ",
    "url": "/optimist/storage/s3_documentation/swiftservestaticwebsite/#swift---serving-static-websites",
    
    "relUrl": "/optimist/storage/s3_documentation/swiftservestaticwebsite/#swift---serving-static-websites"
  },"449": {
    "doc": "Swift - Serving a Static Website",
    "title": "Introduction",
    "content": "Using the Swift command line, it is possible to serve the data in containers as a static website. The following guide will outline the main steps to get started, as well as including an example of a website. ",
    "url": "/optimist/storage/s3_documentation/swiftservestaticwebsite/#introduction",
    
    "relUrl": "/optimist/storage/s3_documentation/swiftservestaticwebsite/#introduction"
  },"450": {
    "doc": "Swift - Serving a Static Website",
    "title": "First Steps",
    "content": "Create a container . We will first create a container named example-webpage which we will use as the basis of this guide: . swift post example-webpage . Make the container publically readable . Next, we must ensure that the container is publically readable. You can learn more about securing containers and setting bucket policies here: . swift post -r '.r:*' example-webpage . Set site index file . Set the index file. In this case, index.html will be the default file displayed when the site appears: . swift post -m 'web-index:index.html' example-webpage . Enable file listing . Optionally, we can also enable file listing. If you need to provide multiple downloads, enabling the directory listing makes sense: . swift post -m 'web-listings: true' example-webpage . Enable CSS for file listing . Enable a custom listings style sheet: . swift post -m 'web-listings-css:style.css' example-webpage . Set error pages . Finally, we should include a custom error page: . swift post -m 'web-error:404error.html' example-webpage . ",
    "url": "/optimist/storage/s3_documentation/swiftservestaticwebsite/#first-steps",
    
    "relUrl": "/optimist/storage/s3_documentation/swiftservestaticwebsite/#first-steps"
  },"451": {
    "doc": "Swift - Serving a Static Website",
    "title": "Example Webpage",
    "content": "Let’s recap the steps we have taken so far to enable static webpages: . swift post example-webpage swift post -r '.r:*' example-webpage swift post -m 'web-index:index.html' example-webpage swift post -m 'web-listings: true' example-webpage swift post -m 'web-listings-css:style.css' example-webpage swift post -m 'web-error:404error.html' example-webpage . Once the steps above have been completed, we can now begin to customise our static webpage. The following demonstrates a quick setup using our container example-webpage . Customising index.html, page.html, and 404error.html pages . This will serve as the homepage, which will create a link to a secondary page. &lt;!-- index.html --&gt; &lt;html&gt; &lt;h1&gt; See the web page &lt;a href=\"mywebsite/page.html\"&gt;here&lt;/a&gt;. &lt;/h1&gt; &lt;/html&gt; . The next page (page.html) will display an image called sample.png: . &lt;!-- page.html --&gt; &lt;html&gt; &lt;img src=\"sample.png\"&gt; &lt;/html&gt; . We can also add custom error pages. Note that currently only 401 (Unauthorized) and 404 (Not Found) errors are supported. The following example demonstrates the creation of a 404 Error page: . &lt;!-- 404error.html --&gt; &lt;html&gt; &lt;h1&gt; 404 Not Found - We cannot find the page you are looking for! &lt;/h1&gt; &lt;/html&gt; . Upload the index.html and page.html files . Once the contents of the files have been added, upload the files with the following commands: . swift upload example-webpage index.html swift upload example-webpage mywebsite/page.html swift upload example-webpage mywebsite/sample.png swift upload example-webpage 404error.html . Viewing the website . Once all of the above steps have been completed, we can now view our newly created website. The link to the website can be found on the Optimist Dashboard &gt; Object Store &gt; Containers using the link shown. Clicking on the link displays our newly created website: . Click on “here” to navigate to the page where we uploaded our sample image: . In the event that we try to navigate to a page which does not exist, our custom 404 page will be displayed: . ",
    "url": "/optimist/storage/s3_documentation/swiftservestaticwebsite/#example-webpage",
    
    "relUrl": "/optimist/storage/s3_documentation/swiftservestaticwebsite/#example-webpage"
  },"452": {
    "doc": "Swift - Serving a Static Website",
    "title": "Swift - Serving a Static Website",
    "content": " ",
    "url": "/optimist/storage/s3_documentation/swiftservestaticwebsite/",
    
    "relUrl": "/optimist/storage/s3_documentation/swiftservestaticwebsite/"
  },"453": {
    "doc": "Secure Backups with Restic and Rclone",
    "title": "Problem statement",
    "content": "When performing file-level backups of a node into S3, the backup software needs write permissions for the S3 bucket. But if an attacker gains access to the machine, he can also destroy the backups in the bucket, since the S3 credentials are present on the compromised system. The solution can be as simple as limiting the level of access of the backup software to the bucket. Unfortunately, this isn’t trivial with S3. ",
    "url": "/optimist/storage/s3_documentation/backupwithrestic/#problem-statement",
    
    "relUrl": "/optimist/storage/s3_documentation/backupwithrestic/#problem-statement"
  },"454": {
    "doc": "Secure Backups with Restic and Rclone",
    "title": "Background",
    "content": "S3 access control lists (ACLs) enable you to manage access to buckets and objects, but they have limitations. They essentially differentiate READ and WRITE permissions: . | READ - Allows grantee to list the objects in the bucket | WRITE - Allows grantee to create, overwrite, and delete any object in the bucket | . The limitations of ACLs were addressed by the access policy permissions (ACP). We can attach a no-delete policy to the bucket, e.g. { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"nodelete1\", \"Effect\": \"Deny\", \"Action\": [ \"s3:DeleteBucket\", \"s3:DeleteBucketPolicy\", \"s3:DeleteBucketWebsite\", \"s3:DeleteObject\", \"s3:DeleteObjectVersion\" ], \"Resource\": [ \"arn:aws:s3:::*\" ] } ] } . Unfortunately, the S3 protocol itself wasn’t designed with the concept of WORM (write once read many) backups in mind. Access policy permissions do not differentiate between changing an existing object (which would effectively allow deleting it) and creating a new object. Attaching the above policy on a bucket does not prevent the objects in it from being overwritten. $ s3cmd put testfile s3://appendonly-bucket/testfile upload: 'testfile' -&gt; 's3://appendonly-bucket/testfile' [1 of 1] 1054 of 1054 100% in 0s 5.46 KB/s done # policy allows write $ s3cmd rm s3://appendonly-bucket/testfile ERROR: S3 error: 403 (AccessDenied) # policy denies deletion $ $ s3cmd put testfile s3://appendonly-bucket/testfile upload: 'testfile' -&gt; 's3://appendonly-bucket/testfile' [1 of 1] 1054 of 1054 100% in 0s 5.50 KB/s done # :( . ",
    "url": "/optimist/storage/s3_documentation/backupwithrestic/#background",
    
    "relUrl": "/optimist/storage/s3_documentation/backupwithrestic/#background"
  },"455": {
    "doc": "Secure Backups with Restic and Rclone",
    "title": "Proposed solution",
    "content": "Since an attacker on a compromised system will always have access to the S3 credentials and every service running on the system - including restic itself, proxies, etc. - we need a second, locked-down VM which can restrict delete operations. Restic can be integrated perfectly with rclone, so we’ll use it in this example. Our environment . | appsrv: the system which has access to the files we would like to backup | rclonesrv: the system running the rclone proxy (and nothing else, to minimise the attack surface) | . Set up the rclone proxy . | Install rclone on the rclonesrv . sudo apt install rclone . | Create user for rclone . sudo useradd -m rcloneproxy sudo su - rcloneproxy . | Create rclone backend configuration . mkdir -p .config/rclone cat &lt;&lt; EOF &gt; .config/rclone/rclone.conf [s3-resticrepo] type = s3 provider = Other env_auth = false access_key_id = 111122223333444455556666 secret_access_key = aaaabbbbccccddddeeeeffffgggghhhh region = eu-central-1 endpoint = s3.es1.fra.optimist.gec.io acl = private bucket_acl = private upload_concurrency = 8 EOF . | Verify that the access to the repository is working with: . rclone lsd s3-resticrepo:databucket 0 2021-11-21 20:02:10 -1 data 0 2021-11-21 20:02:10 -1 index 0 2021-11-21 20:02:10 -1 keys 0 2021-11-21 20:02:10 -1 snapshots . | . Configure the appserver . | Generate an SSH-Keypair on appsrv with the user you’re performing the backup with: . ssh-keygen -o -a 256 -t ed25519 -C \"$(hostname)-$(date +'%d-%m-%Y')\" . | Set the environment variables for restic: . export RESTIC_PASSWORD=\"MyV3ryS3cUr3r3571cP4ssW0rd\" export RESTIC_REPOSITORY=rclone:s3-resticrepo:databucket . | . Restrict the SSH key to restic-only commands . The last step is to go back to the rclonesrv and edit the SSH authorized_keys file to restrict the newly generated SSH key to a single command. This way, an attacker is not able to use the ssh keypair to run arbitrary commands on the rclone proxy and compromise the backups. vi ~/.ssh/authorized_keys # add an entry with the restic user's public key generated in a previous step: command=\"rclone serve restic --stdio --append-only s3-resticrepo:databucket\" ssh-ed25519 AAAAC3fdsC1lZddsDNTE5ADsaDgfTwNtWmwiocdT9q4hxcss6tGDfgGTdiNN0z7zN appsrv-18-11-2021 . ",
    "url": "/optimist/storage/s3_documentation/backupwithrestic/#proposed-solution",
    
    "relUrl": "/optimist/storage/s3_documentation/backupwithrestic/#proposed-solution"
  },"456": {
    "doc": "Secure Backups with Restic and Rclone",
    "title": "Using restic with the rclone proxy",
    "content": "With the environment variables set, restic should work now from appsrv. Example backing up /srv/myapp: . restic -o rclone.program=\"ssh rcloneproxy@rclonesrv.mydomain.com\" backup /srv/myapp . Listing snapshots: . restic -o rclone.program=\"ssh rcloneproxy@rclonesrv.mydomain.com\" snapshots . Deleting snapshots: . restic -o rclone.program=\"ssh rcloneproxy@rclonesrv.mydomain.com\" forget 2738e969 repository b71c391e opened successfully, password is correct Remove(&lt;snapshot/2738e9693b&gt;) returned error, retrying after 446.577749ms: blob not removed, server response: 403 Forbidden (403) . Oh, right, that doesn’t work. That was our goal! . ",
    "url": "/optimist/storage/s3_documentation/backupwithrestic/#using-restic-with-the-rclone-proxy",
    
    "relUrl": "/optimist/storage/s3_documentation/backupwithrestic/#using-restic-with-the-rclone-proxy"
  },"457": {
    "doc": "Secure Backups with Restic and Rclone",
    "title": "Summary",
    "content": "This way, . | The rclone proxy doesn’t even run on the rclonesrv as a service. It will just be spawned on demand, for the duration of the restic operation. Communication happens over HTTP2 over stdin/stdout, in an encrypted SSH tunnel. | Since rclone is running with --append-only, it is not possible to delete (or overwrite) snapshots in the S3 bucket. | All data (except credentials) is encrypted/decrypted locally, then sent/received via rclonesrv to/from S3. | All the credentials are only stored on rclonesrv to communicate with S3. | . Since the command is hard-coded into the SSH configuration for the user’s SSH key, there is no way to use the the keys to get access to the rclone proxy. ",
    "url": "/optimist/storage/s3_documentation/backupwithrestic/#summary",
    
    "relUrl": "/optimist/storage/s3_documentation/backupwithrestic/#summary"
  },"458": {
    "doc": "Secure Backups with Restic and Rclone",
    "title": "Some more thoughts",
    "content": "The advantages of this construct are probably clear already by now. Furthermore, . | Managing snapshots (both manually and with a retention policy) is only possible on the rclone proxy. | A single rclone proxy VM (or even a docker container on an isolated VM) can serve multiple backup clients. | It is highly recommended to use one key for every server which backs up data. | If you’d like to use more than one repository out of a node, you’ll need new SSH keys for them. You can then specify which key to use with -i ~/.ssh/id_ed25519_another_repo in the rclone.program arguments just like you would with SSH. | . ",
    "url": "/optimist/storage/s3_documentation/backupwithrestic/#some-more-thoughts",
    
    "relUrl": "/optimist/storage/s3_documentation/backupwithrestic/#some-more-thoughts"
  },"459": {
    "doc": "Secure Backups with Restic and Rclone",
    "title": "Secure Backups with Restic and Rclone",
    "content": "Restic is a very simple and powerful file-level backup solution, which is rapidly gaining popularity. It can be used in combination with S3 which makes it a great tool to use with Optimist. ",
    "url": "/optimist/storage/s3_documentation/backupwithrestic/",
    
    "relUrl": "/optimist/storage/s3_documentation/backupwithrestic/"
  },"460": {
    "doc": "Localstorage",
    "title": "Compute localstorage for your instances",
    "content": " ",
    "url": "/optimist/storage/localstorage/#compute-localstorage-for-your-instances",
    
    "relUrl": "/optimist/storage/localstorage/#compute-localstorage-for-your-instances"
  },"461": {
    "doc": "Localstorage",
    "title": "What exactly is Compute Localstorage?",
    "content": "With Localstorage, the storage of your instances is located directly on the hypervisor (server). The localstorage feature (available through our l1 flavors) is intended for use with applications that require low latency. ",
    "url": "/optimist/storage/localstorage/#what-exactly-is-compute-localstorage",
    
    "relUrl": "/optimist/storage/localstorage/#what-exactly-is-compute-localstorage"
  },"462": {
    "doc": "Localstorage",
    "title": "Data security and availability",
    "content": "Since your data is directly bound by your instance on our local hypervisor storage, we recommend that you distribute this data via a HA concept, over the provided Availability Zones. The storage backend of the local storage instances is protected against the failure of individual storage media in the array, however, the resulting redundancy compared to the Ceph-based instances only exists within the hypervisor node providing the instance. When replacing individual components following a hardware issue, there may be limited availability and performance for a short time until recovery. The hypervisors are subject to our defined patch cycle where we must boot through the hypervisors one by one. As the instances are using local storage, the maintenance work cannot be carried out uninterrupted, unlike flavors based on Ceph storage. Consequently, there is a regular maintenance window for l1 Flavors. Within each Availability Zone, one server after the other is updated and rebooted during the defined maintenance window. Within the maintenance window, running instances are shut down by our system and stopped after 10 minutes. ",
    "url": "/optimist/storage/localstorage/#data-security-and-availability",
    
    "relUrl": "/optimist/storage/localstorage/#data-security-and-availability"
  },"463": {
    "doc": "Localstorage",
    "title": "Standard Maintainance Windows",
    "content": "| Interval | day | time (in UTC) | . | weekly | Wednesday | 9:00 a.m. - 4:00 p.m. | . ",
    "url": "/optimist/storage/localstorage/#standard-maintainance-windows",
    
    "relUrl": "/optimist/storage/localstorage/#standard-maintainance-windows"
  },"464": {
    "doc": "Localstorage",
    "title": "OpenStack Features",
    "content": "OpenStack provides many ways to handle your instances, such as resizing, shelving, and snapshots. If you want to use l1 flavors for your instances please note the following: . Resize: The resize option will be displayed, but it is technically not possible to resize an instance based on an l1 flavor. However, you can address this by doing a cluster setup (application based) with l1 flavors, running larger l1 flavors in parallel and rolling your data from the old l1 to the new l1 flavors. Shelving/Snapshotting: Both features are possible, but due to the larger disk size within l1 flavors we do not recommend this, as the associated upload will take much longer. In this case we recommend using your external backup solution. ",
    "url": "/optimist/storage/localstorage/#openstack-features",
    
    "relUrl": "/optimist/storage/localstorage/#openstack-features"
  },"465": {
    "doc": "Localstorage",
    "title": "Localstorage",
    "content": " ",
    "url": "/optimist/storage/localstorage/",
    
    "relUrl": "/optimist/storage/localstorage/"
  },"466": {
    "doc": "FAQ",
    "title": "FAQ",
    "content": " ",
    "url": "/optimist/faq/",
    
    "relUrl": "/optimist/faq/"
  },"467": {
    "doc": "FAQ",
    "title": "The command openstack --help shows the error “Could not load EntryPoint.parse”",
    "content": "In this case some components of the OpenstackClient are outdated. To get an overview of the components that need to be updated, use the following command: . openstack --debug --help . To update the components, use the command below. (Replace &lt;PROJECT&gt; with the correct project): . pip install python-&lt;PROJECT&gt;client -U . ",
    "url": "/optimist/faq/#the-command-openstack---help-shows-the-error-could-not-load-entrypointparse",
    
    "relUrl": "/optimist/faq/#the-command-openstack---help-shows-the-error-could-not-load-entrypointparse"
  },"468": {
    "doc": "FAQ",
    "title": "How can I use VRRP?",
    "content": "To use VRRP, it must first be enabled in a security group which is then assigned to an actual VM. You can only add this with the OpenStack client. For example: . openstack security group rule create --remote-ip 10.0.0.0/24 --protocol vrrp --ethertype IPv4 --ingress default . ",
    "url": "/optimist/faq/#how-can-i-use-vrrp",
    
    "relUrl": "/optimist/faq/#how-can-i-use-vrrp"
  },"469": {
    "doc": "FAQ",
    "title": "Why am I charged for Floating IPs I am not using?",
    "content": "We have to charge for reserved Floating IPs. In this case, there is a high probability that Floating IPs were created but not deleted correctly after use. To get an overview of your Floating IPs, you can use the Horizon Dashboard, where you can find the list Project → Network → Floating-IPs. You can accomplish the same using the OpenStack client: . $ openstack floating ip list +--------------------------------------+---------------------+------------------+--------------------------------------+--------------------------------------+----------------------------------+ | ID | Floating IP Address | Fixed IP Address | Port | Floating Network | Project | +--------------------------------------+---------------------+------------------+--------------------------------------+--------------------------------------+----------------------------------+ | 84eca713-9ac1-42c3-baf6-860ba920a23c | 185.116.245.222 | 192.0.2.7 | a3097883-21cc-49fa-a060-bccc1678ece7 | 54258498-a513-47da-9369-1a644e4be692 | b15cde70d85749689e6568f973bb002 | +--------------------------------------+---------------------+------------------+--------------------------------------+--------------------------------------+----------------------------------+ . ",
    "url": "/optimist/faq/#why-am-i-charged-for-floating-ips-i-am-not-using",
    
    "relUrl": "/optimist/faq/#why-am-i-charged-for-floating-ips-i-am-not-using"
  },"470": {
    "doc": "FAQ",
    "title": "How can I change the flavor of a virtual machine (instance resize)?",
    "content": "In the Optimist Stack you can change the flavor of an instance – the available RAM and vCPUs – with the following steps: . Resizing using the command line . With the name or UUID of the server you wish to resize, you can resize it with the openstack server resize command. Specify the desired new flavor and then the instance name or UUID: . openstack server resize --flavor FLAVOR SERVER . Resizing can take some time. During this time, the instance status is displayed as RESIZE. When the resize is complete, the instance status is displayed as VERIFY_RESIZE. To change the status to ACTIVE, confirm the resize: . openstack server resize --confirm SERVER . Resizes are confirmed automatically after 1 hour, if not confirmed or reverted manually. Resizing with the Optimist dashboard . On Optimist Dashboard → Instances navigate to the instance to resize, then choose: Actions → _Resize Flavor:. The current flavor is shown. To choose the new flavor, use the “Select a new flavor” dropdown list, and confirm with “Resize”. This does not apply to l1 (localstorage) flavors. For more information please see Storage → Localstorage. ",
    "url": "/optimist/faq/#how-can-i-change-the-flavor-of-a-virtual-machine-instance-resize",
    
    "relUrl": "/optimist/faq/#how-can-i-change-the-flavor-of-a-virtual-machine-instance-resize"
  },"471": {
    "doc": "FAQ",
    "title": "Why are the logs of the compute instance in the optimist dashboard empty?",
    "content": "Due to maintenance work or load redistribution in OpenStack, the instance may have been migrated. In this case, the log file will be recreated and new messages logged here. ",
    "url": "/optimist/faq/#why-are-the-logs-of-the-compute-instance-in-the-optimist-dashboard-empty",
    
    "relUrl": "/optimist/faq/#why-are-the-logs-of-the-compute-instance-in-the-optimist-dashboard-empty"
  },"472": {
    "doc": "FAQ",
    "title": "Why do I get the error “Conflict (HTTP 409)” when creating a swift container?",
    "content": "Swift uses unique names across the entire OpenStack environment. The error message states that the selected name is already in use. ",
    "url": "/optimist/faq/#why-do-i-get-the-error-conflict-http-409-when-creating-a-swift-container",
    
    "relUrl": "/optimist/faq/#why-do-i-get-the-error-conflict-http-409-when-creating-a-swift-container"
  },"473": {
    "doc": "FAQ",
    "title": "HowTo Mount Cinder Volumes to Instances by UUID",
    "content": "When attaching multiple Cinder Volumes to an instance, the mount points may be shuffled on every reboot. Mounting the volumes by UUID ensures that the correct volumes are reattached to the correct mount points in the event the instance requires a power cycle. Change the mountpoint in /etc/fstab to use the UUID after fetching the infos with blkid on e.g.: . # /boot was on /dev/sda2 during installation /dev/disk/by-uuid/f6a0d6f3-b66c-bbe3-47ba-d264464cb5a2 /boot ext4 defaults 0 2 . ",
    "url": "/optimist/faq/#howto-mount-cinder-volumes-to-instances-by-uuid",
    
    "relUrl": "/optimist/faq/#howto-mount-cinder-volumes-to-instances-by-uuid"
  },"474": {
    "doc": "FAQ",
    "title": "Is it possible to have multiattached volumes on Cinder?",
    "content": "We do not support multiattached volumes on our instances as cluster-capable file systems are required for multi attach volumes to handle concurrent file system access. Attempts to use multi-attached volumes without cluster-capable file systems carry a high risk of data corruption, therefore this feature is not enabled on the Optimist platform. ",
    "url": "/optimist/faq/#is-it-possible-to-have-multiattached-volumes-on-cinder",
    
    "relUrl": "/optimist/faq/#is-it-possible-to-have-multiattached-volumes-on-cinder"
  },"475": {
    "doc": "FAQ",
    "title": "Why am I unable to create a snapshot of a running instance?",
    "content": "In order to provide consistent snapshots, the Optimist platform utilises the property os_require_quiesce=yes. This property allows fsfreeze to suspend and resume access on running instances and ensures ensures that a consistent image is created from the disk. The Optimist Platform supports the following options to facilitate the creation of snapshots on running instances: . The first option is to take a snapshot of the running instance by installing and running the qemu-guest-agent. It can be installed and run as follows: . apt install qemu-guest-agent systemctl start qemu-guest-agent systemctl enable qemu-guest-agent . Once the qemu-guest-agent is running, the snapshot can be created. Additionally, when uploading your own images, we recommend that you include --property hw_qemu_guest_agent=True to install this upon creation of the new image. The second option is to stop the running instance, create the snapshot, then start the instance again. This can be done via the Horizon Dashboard or on the CLI as follows: . openstack server stop ExampleInstance openstack server image create --name ExampleInstanceSnapshot ExampleInstance openstack server start ExampleInstance . ",
    "url": "/optimist/faq/#why-am-i-unable-to-create-a-snapshot-of-a-running-instance",
    
    "relUrl": "/optimist/faq/#why-am-i-unable-to-create-a-snapshot-of-a-running-instance"
  },"476": {
    "doc": "Specifications",
    "title": "Specifications",
    "content": " ",
    "url": "/optimist/specs/",
    
    "relUrl": "/optimist/specs/"
  },"477": {
    "doc": "Flavor Specifications",
    "title": "Flavor Specifications",
    "content": "In the OpenStack context the term “flavor” refers to a hardware profile that can be used for a virtual machine. In Optimist we have set up various standard hardware profiles (flavors). These have different limits, which are listed below for all available flavors. ",
    "url": "/optimist/specs/flavor_specification/",
    
    "relUrl": "/optimist/specs/flavor_specification/"
  },"478": {
    "doc": "Flavor Specifications",
    "title": "Migrating between Flavor Types",
    "content": "To change the flavors of existing instances, the OpenStack “Resize Instance” Option can be used either via the Dashboard or the CLI. This will result in a reboot of the Instance but the content of the instance will be preserved. Please note that changing Flavors from Large Root Disk Types to a Flavor with a smaller Root Disk is not possible. This does not apply to l1 (localstorage) flavors For more information please see Storage → Localstorage . ",
    "url": "/optimist/specs/flavor_specification/#migrating-between-flavor-types",
    
    "relUrl": "/optimist/specs/flavor_specification/#migrating-between-flavor-types"
  },"479": {
    "doc": "Flavor Specifications",
    "title": "Deprecated Flavor Types",
    "content": "The following Flavor Types are currently considered deprecated and the removal of these flavor families is planned for the near future. We will regularly check if these flavors are still in use, if not, we will set them to private in order to to avoid new instances being created with them. | m1-Family (Deprecated) | e1-Family (e = equal) (Deprecated) | r1-Family (r = ram) (Deprecated) | Memory Flavors (Deprecated) | Windows Flavors (Deprecated) | . ",
    "url": "/optimist/specs/flavor_specification/#deprecated-flavor-types",
    
    "relUrl": "/optimist/specs/flavor_specification/#deprecated-flavor-types"
  },"480": {
    "doc": "Flavor Specifications",
    "title": "Flavor Types",
    "content": "Standard Flavors . | Name | Cores | RAM | Disk | IOPS Limits (read/write) | IO throughput (read/write) | Network Bandwidth | . | s1.micro | 1 | 2 GB | 20 GB | 1000 / 1000 | 200 MB/s / 200 MB/s | 1 Gbit/s | . | s1.small | 2 | 4 GB | 20 GB | 1000 / 1000 | 200 MB/s / 200 MB/s | 2 Gbit/s | . | s1.medium | 4 | 8 GB | 20 GB | 1000 / 1000 | 200 MB/s / 200 MB/s | 3 Gbit/s | . | s1.large | 8 | 16 GB | 20 GB | 1000 / 1000 | 200 MB/s / 200 MB/s | 4 Gbit/s | . | s1.xlarge | 16 | 32 GB | 20 GB | 1000 / 1000 | 200 MB/s / 200 MB/s | 4 Gbit/s | . | s1.2xlarge | 30 | 64 GB | 20 GB | 1000 / 1000 | 200 MB/s / 200 MB/s | 4 Gbit/s | . Standard CPU Large Disk Flavors . | Name | Cores | RAM | Disk | IOPS Limits (read/write) | IO throughput (read/write) | Network Bandwidth | . | s1.micro.d | 1 | 2 GB | 100 GB | 2500 / 2500 | 250 MB/s / 250 MB/s | 1 Gbit/s | . | s1.small.d | 2 | 4 GB | 100 GB | 2500 / 2500 | 250 MB/s / 250 MB/s | 2 Gbit/s | . | s1.medium.d | 4 | 8 GB | 100 GB | 2500 / 2500 | 250 MB/s / 250 MB/s | 3 Gbit/s | . | s1.large.d | 8 | 16 GB | 100 GB | 2500 / 2500 | 250 MB/s / 250 MB/s | 4 Gbit/s | . | s1.xlarge.d | 16 | 32 GB | 100 GB | 2500 / 2500 | 250 MB/s / 250 MB/s | 4 Gbit/s | . | s1.2xlarge.d | 30 | 64 GB | 100 GB | 2500 / 2500 | 250 MB/s / 250 MB/s | 4 Gbit/s | . Dedicated CPU Flavors . | Name | Cores | RAM | Disk | IOPS Limits (read/write) | IO throughput (read/write) | Network Bandwidth | . | d1.micro | 1 | 8 GB | 20 GB | 1000 / 1000 | 200 MB/s / 200 MB/s | 1 Gbit/s | . | d1.small | 2 | 16 GB | 20 GB | 1000 / 1000 | 200 MB/s / 200 MB/s | 2 Gbit/s | . | d1.medium | 4 | 32 GB | 20 GB | 1000 / 1000 | 200 MB/s / 200 MB/s | 3 Gbit/s | . | d1.large | 8 | 64 GB | 20 GB | 1000 / 1000 | 200 MB/s / 200 MB/s | 4 Gbit/s | . | d1.xlarge | 16 | 128 GB | 20 GB | 1000 / 1000 | 200 MB/s / 200 MB/s | 4 Gbit/s | . | d1.2xlarge | 30 | 256 GB | 20 GB | 1000 / 1000 | 200 MB/s / 200 MB/s | 4 Gbit/s | . Dedicated CPU Large Disk Flavors . | Name | Cores | RAM | Disk | IOPS Limits (read/write) | IO throughput (read/write) | Network Bandwidth | . | d1.micro.d | 1 | 8 GB | 100 GB | 2500 / 2500 | 250 MB/s / 250 MB/s | 1 Gbit/s | . | d1.small.d | 2 | 16 GB | 100 GB | 2500 / 2500 | 250 MB/s / 250 MB/s | 2 Gbit/s | . | d1.medium.d | 4 | 32 GB | 100 GB | 2500 / 2500 | 250 MB/s / 250 MB/s | 3 Gbit/s | . | d1.large.d | 8 | 64 GB | 100 GB | 2500 / 2500 | 250 MB/s / 250 MB/s | 4 Gbit/s | . | d1.xlarge.d | 16 | 128 GB | 100 GB | 2500 / 2500 | 250 MB/s / 250 MB/s | 4 Gbit/s | . | d1.2xlarge.d | 30 | 256 GB | 100 GB | 2500 / 2500 | 250 MB/s / 250 MB/s | 4 Gbit/s | . Localstorage Flavors . | Name | Cores | RAM | Disk | IOPS Limits (read/write) | IO throughput (read/write) | Network Bandwidth | . | l1.micro | 1 | 8 GB | 300 GB | 25000 / 10000 | 125 MB/s / 60 MB/s | 1 Gbit/s | . | l1.small | 2 | 16 GB | 600 GB | 50000 / 25000 | 250 MB/s / 125 MB/s | 2 Gbit/s | . | l1.medium | 4 | 32 GB | 1200 GB | 100000 / 50000 | 500 MB/s / 250 MB/s | 3 Gbit/s | . | l1.large | 8 | 64 GB | 2500 GB | 100000 / 100000 | 1000 MB/s / 500 MB/s | 4 Gbit/s | . | l1.xlarge | 16 | 128 GB | 5000 GB | 100000 / 100000 | 2000 MB/s / 1125 MB/s | 4 Gbit/s | . | l1.2xlarge | 30 | 256 GB | 10000 GB | 100000 / 100000 | 2000 MB/s / 2000 MB/s | 4 Gbit/s | . m1-Family (Deprecated) . | Name | Cores | RAM | Disk | IOPS Limits (read/write) | IO throughput (read/write) | Network Bandwidth | . | m1.micro | 1 | 1 GB | 20 GB | 1000 / 1000 | 200 MB/s / 200 MB/s | 1 Gbit/s | . | m1.small | 2 | 4 GB | 20 GB | 1000 / 1000 | 200 MB/s / 200 MB/s | 2 Gbit/s | . | m1.medium | 4 | 8 GB | 20 GB | 1000 / 1000 | 200 MB/s / 200 MB/s | 3 Gbit/s | . | m1.large | 8 | 16 GB | 20 GB | 1000 / 1000 | 200 MB/s / 200 MB/s | 4 Gbit/s | . | m1.xlarge | 16 | 32 GB | 20 GB | 1000 / 1000 | 200 MB/s / 200 MB/s | 4 Gbit/s | . | m1.xxlarge | 30 | 64 GB | 20 GB | 1000 / 1000 | 200 MB/s / 200 MB/s | 4 Gbit/s | . e1-Family (e = equal) (Deprecated) . | Name | Cores | RAM | Disk | IOPS Limits (read/write) | IO throughput (read/write) | Network Bandwidth | . | e1.small | 2 | 2 GB | 20 GB | 1000 / 1000 | 200 MB/s / 200 MB/s | 1 Gbit/s | . | e1.medium | 4 | 4 GB | 20 GB | 1000 / 1000 | 200 MB/s / 200 MB/s | 2 Gbit/s | . | e1.large | 8 | 8 GB | 20 GB | 1000 / 1000 | 200 MB/s / 200 MB/s | 3 Gbit/s | . | e1.xlarge | 16 | 16 GB | 20 GB | 1000 / 1000 | 200 MB/s / 200 MB/s | 4 Gbit/s | . | e1.xxlarge | 30 | 32 GB | 20 GB | 1000 / 1000 | 200 MB/s / 200 MB/s | 4 Gbit/s | . r1-Family (r = ram) (Deprecated) . | Name | Cores | RAM | Disk | IOPS Limits (read/write) | IO throughput (read/write) | Network Bandwidth | . | r1.small | 2 | 6 GB | 20 GB | 1000 / 1000 | 200 MB/s / 200 MB/s | 1 Gbit/s | . | r1.medium | 4 | 12 GB | 20 GB | 1000 / 1000 | 200 MB/s / 200 MB/s | 2 Gbit/s | . | r1.large | 8 | 32 GB | 20 GB | 1000 / 1000 | 200 MB/s / 200 MB/s | 3 Gbit/s | . | r1.xlarge | 16 | 48 GB | 20 GB | 1000 / 1000 | 200 MB/s / 200 MB/s | 4 Gbit/s | . Memory Flavors (Deprecated) . | Name | Cores | RAM | Disk | IOPS Limits (read/write) | IO throughput (read/write) | Network Bandwidth | . | mem.micro | 4 | 16 GB | 20 GB | 1000 / 1000 | 200 MB/s / 200 MB/s | 1 Gbit/s | . | mem.small | 8 | 32 GB | 20 GB | 1000 / 1000 | 200 MB/s / 200 MB/s | 2 Gbit/s | . | mem.medium | 8 | 64 GB | 20 GB | 1000 / 1000 | 200 MB/s / 200 MB/s | 3 Gbit/s | . Windows Flavors (Deprecated) . | Name | Cores | RAM | Disk | IOPS Limits (read/write) | IO throughput (read/write) | Network Bandwidth | . | win.micro | 1 | 2 GB | 80 GB | 1000 / 1000 | 200 MB/s /200 MB/s | 1 Gbit/s | . | win.small | 2 | 8 GB | 80 GB | 1000 / 1000 | 200 MB/s /200 MB/s | 2 Gbit/s | . | win.medium | 4 | 16 GB | 80 GB | 1000 / 1000 | 200 MB/s /200 MB/s | 3 Gbit/s | . | win.large | 8 | 32 GB | 80 GB | 1000 / 1000 | 200 MB/s /200 MB/s | 4 Gbit/s | . | win.xlarge | 16 | 64 GB | 80 GB | 1000 / 1000 | 200 MB/s /200 MB/s | 4 Gbit/s | . ",
    "url": "/optimist/specs/flavor_specification/#flavor-types",
    
    "relUrl": "/optimist/specs/flavor_specification/#flavor-types"
  },"481": {
    "doc": "Default Quotas",
    "title": "OpenStack Default Quotas",
    "content": "In Optimist we have defined default quotas for the OpenStack Compute service, the OpenStack Block Storage service, and the OpenStack Networking service. We also have separate quotas for the Octavia Loadbalancer service and its associated components. These default values are listed below. ",
    "url": "/optimist/specs/default_quota/#openstack-default-quotas",
    
    "relUrl": "/optimist/specs/default_quota/#openstack-default-quotas"
  },"482": {
    "doc": "Default Quotas",
    "title": "Compute Settings",
    "content": "| Field | Value | . | Cores | 256 | . | Fixed IPs | Unlimited | . | Floating IPs | 15 | . | Injected File Size | 10240 | . | Injected Files | 100 | . | Instances | 100 | . | Key Pairs | 100 | . | Properties | 128 | . | Ram | 524288 | . | Server Groups | 10 | . | Server Group Members | 10 | . ",
    "url": "/optimist/specs/default_quota/#compute-settings",
    
    "relUrl": "/optimist/specs/default_quota/#compute-settings"
  },"483": {
    "doc": "Default Quotas",
    "title": "Block Storage settings",
    "content": "| Field | Value | . | Backups | 100 | . | Backup Gigabytes | 10000 | . | Gigabytes | 10000 | . | Per-volume-gigabytes | Unlimited | . | Snapshots | 100 | . | Volumes | 100 | . ",
    "url": "/optimist/specs/default_quota/#block-storage-settings",
    
    "relUrl": "/optimist/specs/default_quota/#block-storage-settings"
  },"484": {
    "doc": "Default Quotas",
    "title": "Network settings",
    "content": "| Field | Value | . | Floating IPs | 15 | . | Secgroup Rules | 1000 | . | Secgroups | 100 | . | Networks | 100 | . | Subnets | 200 | . | Ports | 500 | . | Routers | 50 | . | RBAC Policies | 100 | . | Subnetpools | Unlimited | . ",
    "url": "/optimist/specs/default_quota/#network-settings",
    
    "relUrl": "/optimist/specs/default_quota/#network-settings"
  },"485": {
    "doc": "Default Quotas",
    "title": "Octavia Loadbalancers",
    "content": "| Field | Value | . | Load Balancers | 100 | . | Listeners | 100 | . | Pools | 100 | . | Health Monitors | 100 | . | Members | 100 | . ",
    "url": "/optimist/specs/default_quota/#octavia-loadbalancers",
    
    "relUrl": "/optimist/specs/default_quota/#octavia-loadbalancers"
  },"486": {
    "doc": "Default Quotas",
    "title": "Default Quotas",
    "content": " ",
    "url": "/optimist/specs/default_quota/",
    
    "relUrl": "/optimist/specs/default_quota/"
  },"487": {
    "doc": "Application Credentials",
    "title": "Introduction",
    "content": "Users can create Application Credentials to allow their applications to authenticate to the OpenStack Authentication component (Keystone) without needing to use the user’s personal credentials. With application credentials, applications can authenticate with the application credential ID and a secret string which is not the user’s password. This way, the user’s password is not embedded in the application’s configuration. Users can delegate a subset of their role assignments on a project to application credentials, granting the application the same or restricted permissions within a project. ",
    "url": "/optimist/specs/application_credentials/#introduction",
    
    "relUrl": "/optimist/specs/application_credentials/#introduction"
  },"488": {
    "doc": "Application Credentials",
    "title": "Requirements for Application Credentials",
    "content": "Name / Secrets . Application credentials can be generated for your project via the command-line or via the dashboard. These will be associated with the project in which they are created. The only required parameter to create the credentials is a name, however a specific secret can be set by using the —-secret parameter. If the secret parameter is left blank, a secret will instead be auto-generated in the output. It is important to make note of the secret in either case as the secret is hashed before it is stored and will not be retrievable after it has been set. If the secret is lost, a new application credential should be created. Roles . We also recommend setting the roles that the Application Credentials should have in the project, since by default, a newly created set of credentials will inherit all available roles. Below are the available roles which can be assigned to a set of application credentials. When applying these roles to a set of credentials using the --role parameter, please be aware that all role names are case-sensitive: . | Member: The “member” role only has administrative access to the assigned project. | heat_stack_owner: As “heat_stack_owner” you are able to use and execute existing HEAT templates. | load-balancer_member: As a “load-balancer_member” you can use the Octavia LoadBalancer resources. | . Expiration . By default, created Application Credentials will not expire, however, fixed expiration dates/times can be set for credentials upon creation, using the --expires parameter in the command (for example: --expires '2021-07-15T21:00:00'). ",
    "url": "/optimist/specs/application_credentials/#requirements-for-application-credentials",
    
    "relUrl": "/optimist/specs/application_credentials/#requirements-for-application-credentials"
  },"489": {
    "doc": "Application Credentials",
    "title": "Creating Application Credentials via the CLI",
    "content": "A set of Application Credentials can be created in the desired project via the CLI, the example below demonstrates how to create a set of credentials with the following parameters: . | Name: test-credentials | Secret: ZYQZm2k6pk | Roles: Member, heat_stack_owner, load-balancer_member | Expiration Date/Time: 2021-07-12 at 21:00:00 | . The new credentials should appear as follows: . $ openstack application credential create test-credentials --secret ZYQZm2k6pk --role Member --role heat_stack_owner --role load-balancer_member --expires '2021-07-15T21:00:00' +--------------+----------------------------------------------+ | Field | Value | +--------------+----------------------------------------------+ | description | None | expires_at | 2021-07-15T21:00:00.000000 | id | 707d14e835124b4f957938bb5a57d1be | name | test-credentials | project_id | c704ac5a32b84b54a0407d28ad448399 | roles | Member heat_stack_owner load-balancer_member | secret | ZYQZm2k6pk | system | None | unrestricted | False | user_id | 1d9f1ecb5de3607e8982695f72036fa5 | +--------------+----------------------------------------------+ . Note: The secret (whether set by the user or auto-generated) will be displayed upon creation of the credentials. Please take note of the secret at this time. ",
    "url": "/optimist/specs/application_credentials/#creating-application-credentials-via-the-cli",
    
    "relUrl": "/optimist/specs/application_credentials/#creating-application-credentials-via-the-cli"
  },"490": {
    "doc": "Application Credentials",
    "title": "Viewing Application Credentials via the CLI",
    "content": "The list of application credentials belonging to a project can be listed with the following command. $ openstack application credential list +----------------------------------+-------------------+----------------------------------+-------------+------------+ | ID | Name | Project ID | Description | Expires At | +----------------------------------+-------------------+----------------------------------+-------------+------------+ | 707d14e835124b4f957938bb5a57d1be | test-credentials | c704ac5a32b84b54a0407d28ad448399 | None | None | +----------------------------------+-------------------+----------------------------------+-------------+------------+ . Individual credentials can be viewed using the $ openstack application credential show &lt;name&gt; command. ",
    "url": "/optimist/specs/application_credentials/#viewing-application-credentials-via-the-cli",
    
    "relUrl": "/optimist/specs/application_credentials/#viewing-application-credentials-via-the-cli"
  },"491": {
    "doc": "Application Credentials",
    "title": "Deleting Application Credentials via the CLI",
    "content": "Application credentials can be deleted via the CLI with the following command with the name or ID of the specific set of credentials: . openstack application credential delete test-credentials . ",
    "url": "/optimist/specs/application_credentials/#deleting-application-credentials-via-the-cli",
    
    "relUrl": "/optimist/specs/application_credentials/#deleting-application-credentials-via-the-cli"
  },"492": {
    "doc": "Application Credentials",
    "title": "Creating and Deleting Application Credentials via the Optimist Dashboard",
    "content": "Alternatively, Application credentials can also be generated via the Optimist Dashboard under Identity &gt; Application credentials: . Note: Multiple roles can be selected here by holding shift and navigating through the options. Once created, a dialog box will appear to instruct you to capture the ID and secret. Once done, click “Close”. The credentials here can also deleted at any point by using the checkbox to highlight the set of credentials to be deleted and then clicking “DELETE APPLICATION CREDENTIAL” . ",
    "url": "/optimist/specs/application_credentials/#creating-and-deleting-application-credentials-via-the-optimist-dashboard",
    
    "relUrl": "/optimist/specs/application_credentials/#creating-and-deleting-application-credentials-via-the-optimist-dashboard"
  },"493": {
    "doc": "Application Credentials",
    "title": "Testing Application credentials",
    "content": "Once we have created a set of Application Credentials either via the CLI or dashboard, we can test them by using the following curl command to verify that they are working. We need to use our &lt;name&gt; and &lt;secret&gt; in the curl command: . curl -i -H \"Content-Type: application/json\" -d ' { \"auth\": { \"identity\": { \"methods\": [\"application_credential\"], \"application_credential\": { \"id\": “&lt;id&gt;\", \"secret\": “&lt;secret&gt;\"}}}}' https://identity.optimist.gec.io/v3/auth/tokens . A successful curl attempt will output an x-subject-token, unsuccessful attempts where the credentials are incorrect will result in a 401 error. ",
    "url": "/optimist/specs/application_credentials/#testing-application-credentials",
    
    "relUrl": "/optimist/specs/application_credentials/#testing-application-credentials"
  },"494": {
    "doc": "Application Credentials",
    "title": "Application Credentials",
    "content": " ",
    "url": "/optimist/specs/application_credentials/",
    
    "relUrl": "/optimist/specs/application_credentials/"
  },"495": {
    "doc": "Volume Specifications",
    "title": "Volume Specifications",
    "content": "In OpenStack, “volumes” are persistent storage that you can attach to your running OpenStack Compute instances to. In Optimist we have set up three classes of service for volumes. These have different limits, which are detailed below. ",
    "url": "/optimist/specs/volume_specification/",
    
    "relUrl": "/optimist/specs/volume_specification/"
  },"496": {
    "doc": "Volume Specifications",
    "title": "Volume Types",
    "content": "We have three main volume types: . | high-iops | default | low-iops | . ",
    "url": "/optimist/specs/volume_specification/#volume-types",
    
    "relUrl": "/optimist/specs/volume_specification/#volume-types"
  },"497": {
    "doc": "Volume Specifications",
    "title": "Volume Type List",
    "content": "An overview of the three volume types below: . | Name | Read Bytes Sec | Read IOPS Sec | Write Bytes Sec | Write IOPS Sec | . | high-iops | 524288000 | 10000 | 524288000 | 10000 | . | default | 209715200 | 2500 | 209715200 | 2500 | . | low-iops | 52428800 | 300 | 52428800 | 300 | . ",
    "url": "/optimist/specs/volume_specification/#volume-type-list",
    
    "relUrl": "/optimist/specs/volume_specification/#volume-type-list"
  },"498": {
    "doc": "Volume Specifications",
    "title": "Choosing a Volume Type",
    "content": "You can select one of the three volume types upon creation of a volume with the following command (Unless otherwise specified, the type “default” is always used): $ openstack volume create &lt;volume-name&gt; --size 10 --type high-iops . ",
    "url": "/optimist/specs/volume_specification/#choosing-a-volume-type",
    
    "relUrl": "/optimist/specs/volume_specification/#choosing-a-volume-type"
  },"499": {
    "doc": "Images",
    "title": "Images",
    "content": "There are 4 types of images in OpenStack: . | Public Images: These images are maintained by us, available to all users, regularly updated and recommended for use. | Community Images: Previously public images, which have been superseded by newer versions. We’re keeping these images until they’re no longer in use, so as not to compromise your deployments. | Private Images: Images uploaded by you that are only available to your project. | Shared Images: Private images, which are either shared by you, or with you, across multiple different projects. | . Only the first two types are maintained by us. ",
    "url": "/optimist/specs/images/",
    
    "relUrl": "/optimist/specs/images/"
  },"500": {
    "doc": "Images",
    "title": "Public and community images",
    "content": "For your convenience, we’re providing you with a number of selected images. The current list of images is as follows: . | Ubuntu 24.04 LTS (Noble Numbat) | Ubuntu 22.04 LTS (Jammy Jellyfish) | Ubuntu 20.04 LTS (Focal Fossa) | Debian 12 (Bookworm) | Debian 11 (Bullseye) | Debian 10 (Buster) | CentOS 8 | CentOS 7 | CoreOS (stable) | Flatcar Linux | Windows Server 2019 (GUI/Core) | . These images are checked for new releases daily. The latest available version is always a public image, and contains the Latest-suffix. All previous versions of an imags are automatically converted to “community images”, renamed (Latest is replaced by the date of the first upload), and eventially deleted if they are no longer in use at all. OpenStack and many deployment tools support using these images either by name or by their UUID. By using a name, for example Ubuntu 22.04 Jammy Jellyfish - Latest, you can easily stay up to date by redeploying or rebuilding your instances, even if we replace the image in the interim. You can avoid this behaviour by using the UUID instead. This may be useful for cluster deployments, where you want to ensure that all nodes are running the same version of the image. ",
    "url": "/optimist/specs/images/#public-and-community-images",
    
    "relUrl": "/optimist/specs/images/#public-and-community-images"
  },"501": {
    "doc": "Images",
    "title": "Linux Images",
    "content": "All of our provided linux images are unmodified and come directly from their official maintainers. We test them during the upload process to ensure they are deployable. ",
    "url": "/optimist/specs/images/#linux-images",
    
    "relUrl": "/optimist/specs/images/#linux-images"
  },"502": {
    "doc": "Images",
    "title": "Windows Images",
    "content": "What’s inside? . Sadly, there are no prebuilt images for windows deployments, so we built our own. Our changes are minimal, just enough to allow easy use within our instances. Our images are based on a regular installation of Windows Server 2019 standard edition, version 1809 (LTSC). We have added the latest drivers for our virtualization infrastructure, for the network card and storage. Next, we installed the most recent OpenSSH build for windows, and the most recent version of PowerShell. Both are required for the following provisioning steps, and to allow you to initially connect to your instance. We also enabled the RDP service, which is required for remote desktop connections. Don’t forget to add the required security groups for this, and be sure to limit access as much as possible. We have also disabled AutoLogon for security reasons. Our images also come with Spectre and Meltdown mitigations enabled. Additionally we had to disable the random MAC address generator, since our virtual networks enforce fixed MAC addresses. For your convenience and security, we provide these windows images with the latest cumulative updates for Windows and the .NET framework. After booting up an instance, you’ll most likely only have to update the Windows Defender definitions. Finally, we optimized the available DotNetAssemblies, added firewall rules to allow ICMP echo replies and installed cloud-init. The latter is responsible for adding your ssh keys to the new instances. How to use them? . Almost as easy as deploying a linux instance. Add your ssh key to OpenStack (CLI or Dashboard) and deploy the instance. You can then connect to the instance with the following command: . ssh -i ~/.ssh/id_rsa $instanceIP -l Administrator . From here, you can set a password for your Administrator account for use with Remote Desktop: . net user Administrator $password . We strongly discourage using the previous method, adding admin_pass to the instances metadata. This is not encrypted or protected in any way, and is not guaranteed to work due to password security requirements. Be aware: Our Images come without product keys or licenses. You will have to provide your own. ",
    "url": "/optimist/specs/images/#windows-images",
    
    "relUrl": "/optimist/specs/images/#windows-images"
  },"503": {
    "doc": "Images",
    "title": "Uploading your own images",
    "content": "Instead of using the images we provide, you can upload your own images. Easiest way of doing so is to use the OpenStack CLI. openstack image create \\ --property hw_disk_bus=scsi \\ --property hw_qemu_guest_agent=True \\ --property hw_scsi_model=virtio-scsi \\ --property os_require_quiesce=True \\ --private \\ --disk-format qcow2 \\ --container-format bare \\ --file ~/my-image.qcow2 \\ my-image . The command to upload images requires these fields at a minimum: . | --disk-format: qcow2, in this case. This depends on the image format. | --file: The source file on your machine | Name of the Image: my-image for example. | . Additionally, to enable the creation of Snapshots on running Instances, we recommend that you set --property hw_qemu_guest_agent=True on the images you create, and to install the qemu-guest-agent upon creation of the new image. See our FAQ for more details. You can also use the dashboard to upload images. Make sure to use the same properties there. ",
    "url": "/optimist/specs/images/#uploading-your-own-images",
    
    "relUrl": "/optimist/specs/images/#uploading-your-own-images"
  },"504": {
    "doc": "Shelving Instances",
    "title": "Shelving Instances",
    "content": " ",
    "url": "/optimist/specs/shelving_instances/",
    
    "relUrl": "/optimist/specs/shelving_instances/"
  },"505": {
    "doc": "Shelving Instances",
    "title": "Introduction",
    "content": "On the OpenStack platform, you have the ability to shelve an instance. Shelving instances allows you to stop an instance without having it consume resources. A shelved instance, as well as its assigned resources (such as IP address etc), will be retained as a bootable instance. This feature may be used as part of an instance life cycle process or to conserve resources. This does not apply to l1 (localstorage) flavors. For more information please see Storage → Localstorage. ",
    "url": "/optimist/specs/shelving_instances/#introduction",
    
    "relUrl": "/optimist/specs/shelving_instances/#introduction"
  },"506": {
    "doc": "Shelving Instances",
    "title": "Shelve an Instance",
    "content": "Instances can be shelved as follows: $ openstack server shelve &lt;server-id&gt; . ",
    "url": "/optimist/specs/shelving_instances/#shelve-an-instance",
    
    "relUrl": "/optimist/specs/shelving_instances/#shelve-an-instance"
  },"507": {
    "doc": "Shelving Instances",
    "title": "Unshelve an Instance",
    "content": "Instances can be unshelved with the following command: $ openstack server unshelve &lt;server-id&gt; . ",
    "url": "/optimist/specs/shelving_instances/#unshelve-an-instance",
    
    "relUrl": "/optimist/specs/shelving_instances/#unshelve-an-instance"
  },"508": {
    "doc": "Shelving Instances",
    "title": "View Event List for Instances",
    "content": "You can view the shelving / unshelving history of any server by viewing the event list: . $ openstack server event list &lt;server-id&gt; +------------------------------------------+--------------------------------------+--------+----------------------------+ | Request ID | Server ID | Action | Start Time | +------------------------------------------+--------------------------------------+--------+----------------------------+ | req-8d593999-a09b-41a7-8916-1d7c28cd4dc0 | 846112be-d107-4c75-db75-a32eb47a78c5 | shelve | 2022-07-17T15:28:08.000000 | req-076969ee-15a4-470e-8913-051c6f9d4bd3 | 846112be-d107-4c75-db75-a32eb47a78c5 | create | 2022-07-19T16:15:22.000000 | +------------------------------------------+--------------------------------------+--------+----------------------------+ . ",
    "url": "/optimist/specs/shelving_instances/#view-event-list-for-instances",
    
    "relUrl": "/optimist/specs/shelving_instances/#view-event-list-for-instances"
  },"509": {
    "doc": "Shelving Instances",
    "title": "Why Use Shelving?",
    "content": "This feature is useful for archiving instances you are not currently using but do not want to delete. Shelving an instance allows you to you retain the instance data and resource allocations, but frees up the instance memory. When you shelve an instance, the Compute service generates a snapshot image that captures the state of the instance, and uploads it to the Glance image library. If the instance is unshelved, it will be rebuilt using the snapshot. The snapshot image will be deleted if the instance is later unshelved or deleted. ",
    "url": "/optimist/specs/shelving_instances/#why-use-shelving",
    
    "relUrl": "/optimist/specs/shelving_instances/#why-use-shelving"
  },"510": {
    "doc": "Shelving Instances",
    "title": "Billing for Shelved Instances",
    "content": "From a billing perspective, only the root disk of the shelved instance continues to be billed. Once the instance is shelved, CPU and memory resources from the flavor of the instance cease to be billed, however, billing will automatically resume again after unshelving. Shelving has no effect on the utilization of quotas in the project. Shelved resources do not release their quota to ensure sufficient resources for unshelving the instance in the project at all times. ",
    "url": "/optimist/specs/shelving_instances/#billing-for-shelved-instances",
    
    "relUrl": "/optimist/specs/shelving_instances/#billing-for-shelved-instances"
  },"511": {
    "doc": "Changelog",
    "title": "Changelog Optimist",
    "content": "All notable changes to the Optimist Platform are documented on this page. ",
    "url": "/optimist/changelog/#changelog-optimist",
    
    "relUrl": "/optimist/changelog/#changelog-optimist"
  },"512": {
    "doc": "Changelog",
    "title": "Upcoming",
    "content": "Upcoming changes to the Optimist platform are listed here . ",
    "url": "/optimist/changelog/#upcoming",
    
    "relUrl": "/optimist/changelog/#upcoming"
  },"513": {
    "doc": "Changelog",
    "title": "Completed",
    "content": "2022-04-28 . | Optimist Horizon Upgrade (Train) | . 2022-04-27 . | Optimist Heat Upgrade (Train) | . 2022-04-21 . | Optimist Neutron Upgrade (Train) | . 2022-04-05 . | Optimist Nova Upgrade (Train) | . 2022-03-01 . | Optimist Cinder Upgrade (Train) | . 2022-02-23 . | Optimist Designate Upgrade (Train) | . 2022-02-22 . | Optimist Glance Upgrade (Train) | . 2022-02-10 . | Neutron LBaaS removed from Optimist | . 2022-01-25 . | Optimist Keystone Upgrade (Train) | . 2021-08-24 . | Optimist Cinder Upgrade (Stein) | . 2021-08-18 . | Optimist Neutron Feature: . We activated the internal DNS feature. This allows customers to assign dns names to neutron ports. Nova will automatically add the instance name as dns name to the neutron port. | . 2021-07-20 . | Optimist Neutron Upgrade (Stein) | . 2021-06-23 . | Optimist Nova Upgrade (Stein) | . 2021-06-02 . | Optimist Glance upgrade (Stein) | . 2021-06-01 . | Optimist Keystone upgrade (Stein) | . ",
    "url": "/optimist/changelog/#completed",
    
    "relUrl": "/optimist/changelog/#completed"
  },"514": {
    "doc": "Changelog",
    "title": "Changelog",
    "content": " ",
    "url": "/optimist/changelog/",
    
    "relUrl": "/optimist/changelog/"
  }
}
