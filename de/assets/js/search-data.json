{"0": {
    "doc": "Not found",
    "title": "404 Not Found",
    "content": "We cannot find the page you are looking for! . Click here to go back home . ",
    "url": "/404.html#404-not-found",
    
    "relUrl": "/404.html#404-not-found"
  },"1": {
    "doc": "Not found",
    "title": "Not found",
    "content": " ",
    "url": "/404.html",
    
    "relUrl": "/404.html"
  },"2": {
    "doc": "Willkommen!",
    "title": "Willkommen!",
    "content": " ",
    "url": "/",
    
    "relUrl": "/"
  },"3": {
    "doc": "Einführung",
    "title": "Einführung",
    "content": "Elastic Cloud Enterprise by German Edge Cloud (ECE) ist eine von GEC gehostete und betriebene Cloud-Plattform, auf der Sie Ihre Elastic-Workloads in Form sogenannter Deployments betreiben können. Ein Deployment ist ein isoliertes Elastic Cluster, das mit ECE verwaltet wird. Es besteht aus mehreren Komponenten des Elastic Stacks. Für jedes Deployment können Sie folgendes individuell festlegen: . | Welche der unten aufgeführten Elastic Stack Komponenten verwendet werden sollen | Die Größe der jeweiligen Komponenten | Die Anzahl der Availability Zones (1 bis 3) über die sie verteilt werden sollen | Welche der von GEC bereitgestellten Versionen für den Elastic Stack verwendet werden soll | . Jedes Deployment bekommt eigene, aus dem Internet erreichbare URLs für Elasticsearch, Kibana, Fleet und Enterprise Search. ",
    "url": "/ece/intro",
    
    "relUrl": "/ece/intro"
  },"4": {
    "doc": "Einführung",
    "title": "Komponenten des Elastic Stacks",
    "content": "ECE enthält alle aktuell von Elastic angebotenen Server-Komponenten: . | Elasticsearch in den 4 Tiers Hot, Warm, Cold und Frozen sowie Master Nodes und Coordinating/Ingest Nodes | Kibana | Machine Learning | Integration Server (Fleet und APM) | Enterprise Search | . Die Definition der Elasticsearch Data Tier finden Sie hier: https://www.elastic.co/guide/en/elasticsearch/reference/current/data-tiers.html. ",
    "url": "/ece/intro#komponenten-des-elastic-stacks",
    
    "relUrl": "/ece/intro#komponenten-des-elastic-stacks"
  },"5": {
    "doc": "Einführung",
    "title": "Funktionsumfang der Komponenten",
    "content": "Alle ECE Deployments sind mit der Elastic Enterprise Lizenz ausgestattet, d.h. alle Komponenten enthalten alle Funktionalitäten der höchstwertigen Elastic-Lizenz. Dazu gehören Machine Learning, durchsuchbare Snapshots im Frozen Tier (eine sehr kostengünstige Variante für die Speicherung selten abgerufener Daten), Cross-Cluster-Search, alle SIEM (Security Information and Event Management) Funktionen des Elastic Stacks, Watcher und Alerting, der neue AI Assistant u.v.m. Einen Überblick über die Komponenten finden Sie hier https://www.elastic.co/de/pricing/. Eine detaillierte Auflistung erhalten Sie hier: https://www.elastic.co/de/subscriptions. ",
    "url": "/ece/intro#funktionsumfang-der-komponenten",
    
    "relUrl": "/ece/intro#funktionsumfang-der-komponenten"
  },"6": {
    "doc": "Einführung",
    "title": "Berechnung der Größe des Deployments und Preismodell",
    "content": "Die Größe des Deployments wird in GB RAM gemessen. Jeder Elastic Komponente wird nach einem bestimmten Schlüssel zum RAM entsprechend CPU und Storage zugeordnet. Dieser Zuordnungsschlüssel kann pro Komponente verschieden sein. Für die meisten Anwendungsfälle ist der Arbeitsspeicher der bestimmende Faktor für die Performance eines Elastic Workloads. Da die Zuordnung nach festen Schlüsseln erfolgt, reicht ein Parameter (GB RAM) aus, um die anderen Parameter (CPU, Storage) zu berechnen. Dadurch wird auch das Preismodell sehr transparent und leicht verständlich. Sie müssen lediglich den Arbeitsspeicher des Deployments kennen, um Ihre Kosten zu berechnen. Sie zahlen für die genutzten GB RAM Ihres Deployments. Die Erfassung der Nutzung und die Abrechnung erfolgen stundengenau. Zusätzlich fallen Kosten für den genutzten Snapshot-Storage in unserem Object Storage Cluster an. Der (ein- und ausgehende) Datenverkehr ist kostenfrei. ",
    "url": "/ece/intro#berechnung-der-gr%C3%B6%C3%9Fe-des-deployments-und-preismodell",
    
    "relUrl": "/ece/intro#berechnung-der-größe-des-deployments-und-preismodell"
  },"7": {
    "doc": "Einführung",
    "title": "Autoscaling Feature",
    "content": "Die Komponenten Elasticsearch und Machine Learning verfügen über ein sogenanntes Autoscaling. Dieses Feature haben Sie bei einem selbst gehosteten Elastic Stack in dieser Form nicht oder müssten es selbst entwickeln. Mit Autoscaling überwacht ECE den genutzten Festplattenspeicher der Elasticsearch-Komponenten und fügt bei Überschreiten eines Schwellenwertes automatisch weitere Ressourcen hinzu. Dadurch müssen Sie auch bei Wachstum Ihrer Datenmenge keine Ausfälle wegen voller Festplatten o.ä. befürchten. Machine Learning Workloads werden in der notwendigen Größe nur dann gestartet, wenn sie auch benötigt werden. Die Ressourcen in Ihrem Deployment und somit auch Ihre Kosten wachsen mit Ihrer Datenmenge. Dadurch wird eine effiziente Ausnutzung der Ressourcen und eine kosteneffiziente Abbildung Ihrer Workloads auf der ECE Plattform möglich. Für jede Komponente mit Autoscaling lassen sich auch Obergrenzen definieren bis zu denen automatisch hochskaliert wird. Somit gerät z.B. bei Fehlkonfigurationen Ihr Rechnungsbetrag nicht außer Kontrolle. ",
    "url": "/ece/intro#autoscaling-feature",
    
    "relUrl": "/ece/intro#autoscaling-feature"
  },"8": {
    "doc": "Einführung",
    "title": "Ihre Vorteile durch ECE von GEC",
    "content": "Mit der ECE-Lösung von GEC erhalten Sie alle Funktionalitäten der Software von Elastic, dem Marktführer bei Suche, SIEM und Observability, inklusive Enterprise-Lizenz. GEC hostet ausschließlich in hochsicheren deutschen Rechenzentren und leistet alle Betriebsaufgaben von Deutschland aus. Sie erhalten alles aus einer Hand von einem deutschen Vertragspartner. Im Gegensatz zu einem “klassischen” Deployment des Elastic Stacks in einem Cluster mit virtuellen Maschinen, die von der jeweiligen Cloud-Plattform auf vorgegebene Größen beschränkt sind, werden bei ECE Docker-Container verwendet. Diese können bei Bedarf automatisch skalieren. Durch das feingranulare Autoscaling können Sie die Ressourcen effizient ausnutzen und die Workloads kosteneffizient betreiben. ",
    "url": "/ece/intro#ihre-vorteile-durch-ece-von-gec",
    
    "relUrl": "/ece/intro#ihre-vorteile-durch-ece-von-gec"
  },"9": {
    "doc": "Einführung",
    "title": "Leistungen der GEC",
    "content": ". | GEC stellt in seinen sicheren und zertifizierten Rechenzentren in Deutschland die Cloud-Umgebung bereit, auf denen Ihre Elastic Workloads laufen. | GEC gibt Ihnen die Möglichkeit, zwecks hoher Verfügbarkeit Ihre Workloads über 3 Verfügbarkeitszonen (Availability Zones) zu verteilen. | GEC kümmert sich darum, dass genügend Ressourcen für die Skalierung Ihres Deployments zur Verfügung stehen. | GEC überwacht die ECE-Umgebung und sorgt dafür, dass sie verfügbar ist (99,85 %). Auch außerhalb der Bürozeiten wird automatisch eine Rufbereitschaft alarmiert, um eventuelle Probleme an der ECE-Plattform möglichst schnell zu beheben. | GEC sorgt für regelmäßige Updates und Security-Patches der Server. | GEC stellt neue Versionen des Elastic Stacks zur Installation bereit, meist nur wenige Werktage nachdem diese von Elastic veröffentlicht wurden. Wir führen jedoch keine automatischen oder unaufgeforderten Versions-Upgrades Ihres Deployments durch (ausgenommen EOL-Versionen). | GEC stellt aus dem Internet erreichbare URLs für alle Deployments inklusive TLS-Zertifikat für die verschlüsselte Kommunikation zur Verfügung (kundeneigene Zertifikate sind leider nicht möglich). | GEC stellt Backup- und Snapshot-Storage im redundanten GEC Object Storage Cluster zur Verfügung. Für jedes Deployment wird standardmäßig eine (von Ihnen änderbare) Snapshot Policy aktiviert, über die automatisch in regelmäßigen Abständen eine Datensicherung Ihrer Elasticsearch-Daten in den Object Storage erfolgt. Dadurch sind Sie sehr gut gegen Datenverlust geschützt. | All Ihre Deployments in der ECE-Cloud von GEC sind mit der Elastic Enterprise Lizenz ausgestattet. Sie müssen keine Lizenzen bei Elastic einkaufen, Sie erhalten von GEC alles aus einer Hand. | . ",
    "url": "/ece/intro#leistungen-der-gec",
    
    "relUrl": "/ece/intro#leistungen-der-gec"
  },"10": {
    "doc": "Einführung",
    "title": "Ihre Verantwortlichkeit als Kunde",
    "content": "Als Kunde sind Sie für alles verantwortlich was innerhalb Ihres Deployments passiert, insbesondere: . | Das Einsammeln von Daten aus Client-Systemen und das Laden dieser Daten in Elasticsearch | Die Konfiguration von Lifecycle Policies | Das Anlegen von Nutzern und die Vergabe von Berechtigungen | Die Überwachung der Shard-Gesundheit und bei Bedarf deren Reparatur (s. hierzu die Dokumentation von elastic.co) | Konfiguration von Alerts - sofern gewünscht | Wiederherstellung von Snapshots bei Bedarf | Überwachung der Performance Ihres Deployments, sofern Sie dafür spezielle Anforderungen haben. Welche Performance für Ihren Anwendungsfall akzeptabel ist, können wir nicht wissen. | . Weitere Bestimmungen entnehmen Sie den Allgemeinen Geschäftsbedingungen der GEC. ",
    "url": "/ece/intro#ihre-verantwortlichkeit-als-kunde",
    
    "relUrl": "/ece/intro#ihre-verantwortlichkeit-als-kunde"
  },"11": {
    "doc": "Einführung",
    "title": "Service Description",
    "content": "Für detailliertere Informationen kontaktieren Sie bitte unseren Vertrieb. ",
    "url": "/ece/intro#service-description",
    
    "relUrl": "/ece/intro#service-description"
  },"12": {
    "doc": "Einführung",
    "title": "Auszuführende Schritte nach dem ECE-Deployment",
    "content": "Wir empfehlen Ihnen, folgende Schritte durchzuführen, um die Sicherheit Ihres ECE Deployments zu erhöhen und Daten in Elastic zu laden: . Erhöhung der Sicherheit . | Legen Sie in Kibana die benötigten Spaces, Rollen und Nutzer an. Weitere Informationen dazu finden Sie hier: https://www.elastic.co/guide/en/kibana/current/tutorial-secure-access-to-kibana.html. | Verwenden Sie nicht den elastic Superuser für die tägliche Arbeit in Kibana oder um Daten anzuliefern. | Sofern gewünscht, konfigurieren Sie Single Sign-On mittels SAML, LDAP, Active Directory, OpenID Connect oder Kerberos sowie dynamische role mappings, wenn Sie Role-based oder Attribute-based Access Control benötigen. Für die Einstellung der Parameter in Ihrem Deployment wenden Sie sich vorläufig an unseren Support. | Falls gewünscht, lassen Sie durch unseren Support traffic filter konfigurieren. Damit können Sie unerwünschte Zugriffe auf Ihr Deployment verhindern. | Legen Sie ggf. (falls Sie nicht den Agent verwenden, s.u.) Service Accounts für die Datenanlieferung an und erzeugen Sie dafür API Keys. Weitere Informationen finden Sie hier: Grant access using API keys. | . Laden von Daten in Elastic . | Laden Sie Daten in Ihren Cluster. Wir empfehlen hierfür den Elastic Agent in Zusammenarbeit mit dem Fleet Server. Eine detaillierte Anleitung finden Sie hier: Laden von Daten in ECE. | Um Verbindungsprobleme zu ihrem Cluster zu vermeiden, stellen sie sicher, Sniffingin den verwendeten Elasticsearch Clients zu deaktivieren (z.B. fluentd). Elastic Agent, Filebeat etc. verhalten sich automatisch richtig. | Falls Sie Daten aus Ihrem bestehenden Elastic Cluster in Ihr neues Deployment migrieren möchten, finden Sie einige Möglichkeiten in der Elastic Dokumentation: https://www.elastic.co/guide/en/cloud-enterprise/current/ece-migrating-data.html. | Prüfen Sie in Kibana Ihre Index Templates und Lifecycle Policies, damit Sie Ihre Daten den von Ihnen gewünschten Tiers Hot, Warm, Cold, Frozen (und Delete) zuweisen können. Die von Elastic automatisch angelegten Lifecycle Policies haben häufig nur ein Hot Tier ohne Ablaufdatum konfiguriert. Weitere Informationen finden Sie hier: https://www.elastic.co/guide/en/elasticsearch/reference/current/index-lifecycle-management.html. Dies gilt insbesondere für die standardmäßig deployten Lifecycle Policies logs und metrics, die bei Verwendung des Elastic Agents die relevantesten Policies sind. | . Ein Beispiel für die Konfiguration einer Policy, bei der die Daten nach 70 Tagen gelöscht werden, finden Sie hier: Aktualisieren der Standard Lifecycle Policies. ",
    "url": "/ece/intro#auszuf%C3%BChrende-schritte-nach-dem-ece-deployment",
    
    "relUrl": "/ece/intro#auszuführende-schritte-nach-dem-ece-deployment"
  },"13": {
    "doc": "Einführung",
    "title": "Support-Leistungen von GEC",
    "content": "Bei Support-Anfragen können Sie sich an unseren Support wenden, der werktags von 8-18 Uhr erreichbar ist. Der Support von GEC kann Sie bei allen Themen unterstützen, die im Rahmen des ECE-Dienstes in der Verantwortung von GEC liegen: . | Verfügbarkeit und Erreichbarkeit der Plattform | Fragen zur Verwendung von ECE | Fehler in der Elastic Software (wir würden diese entsprechend an Elastic weiterleiten) | . Während wir daran arbeiten, Ihnen in Zukunft eine Self-Service-Oberfläche für die Verwaltung Ihres ECE-Deployments zur Verfügung zu stellen, müssen Sie vorläufig folgende Einstellungen an Ihrem Deployment über unseren Support beauftragen. Während dieser Zeit sind die Leistungen auch im Support-Umfang enthalten: . | Erweiterte Konfiguration des Deployments (d.h. Anpassungen an der elasticsearch.yml bzw. kibana.yml) | Einträge im Elasticsearch Keystore | Upgrade der Elastic-Version Ihres Deployments | Konfiguration von IP-Filtern für Ihr Deployment | Konfiguration von Vertrauensbeziehungen zwischen Deployments, damit Sie Cross-Cluster Search bzw. Cross-Cluster Replication verwenden können | . Nicht durch den Support der GEC abgedeckt sind: . | Generelle Beratung zur Elastic Software oder deren Verwendung in Ihrem Deployment | . Ggf. können wir auf Anfrage einige der nicht abgedeckten Leistungen als vergütete Consulting-Leistung anbieten. Bitte wenden Sie sich hierzu an den Vertrieb. Ebenso können wir keine 24/7-Rufbereitschaft für Kunden anbieten. Dies ist auch nicht nötig, da die ordnungsgemäße Funktionalität der ECE-Plattform von einem automatisierten Monitoring überwacht wird. Wird eine Einschränkung des Service festgestellt, wird unsere interne Rufbereitschaft automatisch alarmiert und beginnt mit der Problembehebung. ",
    "url": "/ece/intro#support-leistungen-von-gec",
    
    "relUrl": "/ece/intro#support-leistungen-von-gec"
  },"14": {
    "doc": "Laden von Daten in Elastic",
    "title": "Laden von Daten in Elastic",
    "content": "Nachdem Ihr Deployment eingerichtet wurde, können Sie mit Elastic Agent und Fleet Daten aus Ihren Logdaten-Quellen (Log Sources) an Ihr Deployment senden. Dieses Dokument beschreibt die nötigen Schritte. ",
    "url": "/ece/shipdata/",
    
    "relUrl": "/ece/shipdata/"
  },"15": {
    "doc": "Laden von Daten in Elastic",
    "title": "Begriffsklärung",
    "content": "Bevor wir beginnen, möchten wir einige Begriffe erklären, die in diesem Dokument verwendet werden. | Begriff | Definition | . | Elastic Agent | Komponente für die Sammlung von Log- und Metrikdaten, die Client-seitig auf den Logdaten-Quellen installiert wird. Es handelt sich um die einzige Komponente, die installiert werden muss, da sie alle Beats mitbringt und diese mit den zugehörigen Konfigurationen auf dem Client (Klienten) verwaltet. | . | Fleet Server | Komponente im Elastic-Deployment, mit der die Agenten verwaltet werden. Alle Einstellungen in Fleet werden in Kibana vorgenommen. | . | Integration | In diesem Zusammenhang handelt es sich um eine (meist durch Elastic selbst) vorkonfigurierte Zusammenstellung von Agenteneinstellungen, Ingest-Pipelines und Kibana-Dashboards für einen bestimmten Logdaten-Quellen-Typ (z. B. Tomcat, Nginx, MySQL, Cisco Komponenten, …). Für die gängigsten Logdaten-Quellen sind Integrationen verfügbar (siehe: https://www.elastic.co/de/integrations/data-integrations). | . | Agent Policy | Eine Sammlung von Einstellungen und Integrationen in Form einer Richtlinie (Policy), die dem Agenten und den darunter liegenden Beats mitteilt, welche Daten wo gesammelt werden sollen. Sie benötigen für jeden Quellentyp eine separate Agentenrichtlinie (Agent Policy). Wenn beispielsweise alle Ihre MySQL-Datenbanken ihre Logdateien im selben lokalen Pfad speichern (was sie ohnehin tun sollten), reicht eine Agentenrichtlinie für MySQL-Datenbanken aus. Eine Agentenrichtlinie kann mehrere Integrationen enthalten. Somit kann ein Agent Logdateien von mehreren Anwendungen auf derselben Quelle sammeln. Sie können nur eine Richtlinie pro Agent haben. Angenommen, Sie haben auf einigen Hosts nur Apache httpd und auf anderen Hosts httpd und Tomcat installiert, dann benötigen Sie mindestens zwei Agentenrichtlinien: eine für httpd und eine für httpd+tomcat. | . | Enrollment Token | Wird vom Agenten beim Start auf dem Client benötigt. Das Token erfüllt zwei Zwecke: Erstens, ermöglicht es die Erstauthentifizierung des Agenten gegenüber den Fleet- und Elastic-Servern. Zweitens, verweist es auf genau eine Agentenrichtlinie, sodass der Fleet-Server dem Agenten mitteilen kann, von welchen Quellen er Daten sammeln soll. Jede Agentenrichtlinie verfügt über mindestens ein Registrierungstoken. Registrierungstokens sind von Natur aus sensible Daten und sollten daher auf sichere Weise gespeichert werden. | . ",
    "url": "/ece/shipdata/#begriffskl%C3%A4rung",
    
    "relUrl": "/ece/shipdata/#begriffsklärung"
  },"16": {
    "doc": "Laden von Daten in Elastic",
    "title": "Erstellen der Agentenrichtlinie",
    "content": "Da die Agentenrichtlinie für den Agenten definiert, welche Daten erfasst werden sollen, muss diese als erstes definiert werden. Definieren Sie am besten eine Agentenrichtlinie pro Quellentyp. Gehen Sie in Kibana zu Fleet → Agent Policies. Erstellen Sie eine neue Agentenrichtlinie und geben Sie ihr einen aussagekräftigen Namen (meist ist es nicht nötig, die Advanced Optionen zu ändern). Klicken Sie dann auf Ihre neue Integration. Sie werden sehen, dass bereits eine Integration, die Systemintegration, vorkonfiguriert ist. Diese sammelt die System-Logs und Metriken vom Client-Computer. Sie müssen sich keine Sorgen wegen des Betriebssystems machen. Egal ob Linux, Windows oder MacOS, der Agent findet das selbst heraus und konfiguriert sich korrekt. Fügen Sie je nach Art der Quelle, von der Sie Daten sammeln möchten, weitere Integrationen hinzu, wie z.B. Apache Produkte: . Durch einen Klick auf die entsprechende Kachel erhalten Sie nicht nur einen Überblick darüber, was die Integration beinhaltet. Sie können, nachdem Sie “Add Integration” ausgewählt haben, auf der nachfolgenden Bildschirmseite auch die Einstellungen anpassen. Der Inhalt der Seite ist vom Typ der Integration abhängig. Wenn Sie Ihre MySQL-Logdaten beispielsweise nicht im Standardpfad speichern, können Sie dies hier ändern. Es wird wahrscheinlich ungefähr wie in dem folgenden Bild aussehen: . ",
    "url": "/ece/shipdata/#erstellen-der-agentenrichtlinie",
    
    "relUrl": "/ece/shipdata/#erstellen-der-agentenrichtlinie"
  },"17": {
    "doc": "Laden von Daten in Elastic",
    "title": "Abrufen des Enrollment Tokens",
    "content": "Für jede Agentenrichtlinie wird automatisch ein Enrollment Token (Registrierungstoken) mit dem Namen Default erstellt. Dieses können Sie jederzeit in Kibana abrufen. ",
    "url": "/ece/shipdata/#abrufen-des-enrollment-tokens",
    
    "relUrl": "/ece/shipdata/#abrufen-des-enrollment-tokens"
  },"18": {
    "doc": "Laden von Daten in Elastic",
    "title": "Installieren des Agenten auf dem Content-Host",
    "content": "In Kibana gibt es einen Wizard, der Sie bei der Installation des Agenten auf den Clients unterstützt. Für den ersten Rollout dieser Art starten Sie die Installation in Kibana → Fleet → Agents → Add Agent. Bei mehreren Agenten mit der selben Agentenrichtlinie können Sie die Installation auch automatisieren, da die Installationsbefehle und -parameter identisch sind. Wählen Sie die richtige Agentenrichtlinie für den Host aus, auf dem Sie die Installation durchführen möchten. Lassen Sie Enroll in Fleet aktiviert. Kibana präsentiert Ihnen dann verschiedene Optionen für die Befehle, die auf dem Client-Computer auszuführen sind: . Der URL-Parameter ist automatisch auf das Deployment eingestellt, in dem Sie sich gerade befinden, und das Enrollment-Token verweist auf die Agentenrichtlinie. Sobald diese Befehle auf dem Client-Computer ausgeführt werden, registriert sich der Agent beim Fleet-Server und erscheint in Kibana in der Agentenliste. Alle nachfolgenden Installationen von Agenten, die dieselbe Agentenrichtlinie verwenden, verfügen über denselben Befehls- und Parametersatz. Sie müssen den Assistenten nicht erneut durchlaufen. Sie können somit diesen Installationsteil im Tool Ihrer Wahl automatisieren. ",
    "url": "/ece/shipdata/#installieren-des-agenten-auf-dem-content-host",
    
    "relUrl": "/ece/shipdata/#installieren-des-agenten-auf-dem-content-host"
  },"19": {
    "doc": "Laden von Daten in Elastic",
    "title": "Auszuführende Schritte nach der Installation",
    "content": "Überprüfen Sie unbedingt die Lifecycle Policies (ILM), einschließlich aller vom System bereitgestellten Richtlinien wie logs und metrics. Standardmäßig sind diese mit einer unbegrenzten Lebensdauer versehen. Wir empfehlen, das zu ändern. Entdecken Sie die ansprechenden neuen Dashboards, die mit jeder zusätzlichen Integration geliefert werden. Machen Sie sich mit dem Fleet-Menü in Kibana vertraut. Suchen Sie nach möglichen Aktualisierungen für die Agentenversionen oder Integrationen. Sie können all dies im Fleet-Menü in Kibana erledigen, dafür müssen Sie sich nicht auf den Client-Computern einloggen. ",
    "url": "/ece/shipdata/#auszuf%C3%BChrende-schritte-nach-der-installation",
    
    "relUrl": "/ece/shipdata/#auszuführende-schritte-nach-der-installation"
  },"20": {
    "doc": "Anpassen von Lifecycle Policies",
    "title": "Anpassen von Lifecycle Policies",
    "content": "Dieses Dokument beschreibt, wie Sie die (Standard) Index Lifecycle Policies (ILM) anpassen können. | Begriffsklärung | Auffinden der zugeordneten ILM-Richtlinie | Erstellen einer neuen ILM Richtlinie . | Verwenden der Benutzeroberfläche (UI) . | Hot Phase | Warm Phase | Cold Phase | Frozen Phase | Delete Phase | . | Verwenden von Dev Tools | . | Zuweisen der Richtlinie zu einem Index | Rollover des Data Streams | . ",
    "url": "/ece/updateilm/",
    
    "relUrl": "/ece/updateilm/"
  },"21": {
    "doc": "Anpassen von Lifecycle Policies",
    "title": "Begriffsklärung",
    "content": "Bevor wir beginnen, möchten wir einige Begriffe erklären, die in diesem Dokument verwendet werden. | Begriff | Definition | . | ILM | Abkürzung für Index Lifecycle Management. Bestimmt den Lebenszyklus Ihrer Daten sowie deren Zuordnung zu den verschiedenen Phasen. Wird in Richtlinien definiert (“Index Lifecycle Policies”). | . | Data stream | Der Datenstrom ist eine Sammlung einzelner Indizes. Man könnte es fast als kleines DNS (Domain Name System) für das Routing von Anfragen an Indizes betrachten. Ein Datenstrom kann mehrere Indizes enthalten. | . | Rollover | Prozess der (automatischen) Erstellung eines neuen Indexes innerhalb eines Datenstroms in der Hot Phase, sodass ein einzelner Index nicht unbegrenzt wächst. Er ist in der Hot Phase der ILM-Richtlinie definiert. | . ",
    "url": "/ece/updateilm/#begriffskl%C3%A4rung",
    
    "relUrl": "/ece/updateilm/#begriffsklärung"
  },"22": {
    "doc": "Anpassen von Lifecycle Policies",
    "title": "Auffinden der zugeordneten ILM-Richtlinie",
    "content": "Bei Verwendung des Elastic Agents und Fleet gelten Standardrichtlinien für die Datenströme. Diese Richtlinien sind einfach gehalten und beinhalten nur wenige Phasen, oft sogar nur die Hot Phase. Die auf die Datenströme angewendete Richtlinie finden Sie in Kibana. | Öffnen Sie Kibana. | Wählen Sie im Menü Stack Management aus. | Öffnen Sie Index Management und gehen Sie zu Data Streams. | Im Popup rechts sehen Sie die zugeordnete ILM Richtlinie. | . ",
    "url": "/ece/updateilm/#auffinden-der-zugeordneten-ilm-richtlinie",
    
    "relUrl": "/ece/updateilm/#auffinden-der-zugeordneten-ilm-richtlinie"
  },"23": {
    "doc": "Anpassen von Lifecycle Policies",
    "title": "Erstellen einer neuen ILM Richtlinie",
    "content": "Verwenden der Benutzeroberfläche (UI) . | Um eine neue ILM-Richtlinie zu erstellen, wählen Sie Stack Management. | Wählen Sie Index Lifecycle Management und klicken Sie auf Create Policy. | Geben Sie einen neuen Namen für die Richtlinie ein. | . Hot Phase . In der Hot Phase werden alle Daten gespeichert, die neu indiziert werden und auf die ständig zugegriffen wird. Sie ist immer die erste Phase und daher verpflichtend. In unserem Beispiel haben wir den Schalter “Use recommended default” deaktiviert, um das maximale Alter der Daten bis zum Rollover vom Standardwert von 30 Tagen auf 7 Tage zu verkürzen. Warm Phase . Nachdem 7 Tage lang Daten gesammelt wurden oder der Index eine Größe von 50 GB erreicht hat, sollen die Indizes unmittelbar in die zweite Phase, die sogenannte Warm Phase, übergehen. In dieser Stufe wird die Priorität der Indizes reduziert und auf “schreibgeschützt” gesetzt. Cold Phase . Nach 7 Tagen in der Warm Phase gehen die Daten in die nächste Phase über, die sogenannte Cold Phase. In dieser Phase können durchsuchbare Snapshots verwendet werden, um den Platzbedarf auf den Elastic Instanzen zu verringern. Die Snapshots sind im Object Storage gespeichert, der selbst schon hochverfügbar ausgelegt ist. Daher können wir die Anzahl der Replikate und die Priorität auf Null setzen. Frozen Phase . In dieser Phase werden die Daten von den Elastic Knoten entfernt und nur ein kleiner Cache verbleibt hier. Wenn auf die Daten zugegriffen wird, werden diese aus dem Snapshot geholt und bereitgestellt. Delete Phase . In dieser Phase werden die Indizes entfernt, die ein bestimmtes Alter erreicht haben. Verwenden von Dev Tools . Alternativ kann auch in den Dev Tools von Kibana eine neue ILM-Richtlinie erstellt werden. Die Konsole finden Sie in Kibana unter Management → Dev Tools. PUT _ilm/policy/&lt;add-some-name-here&gt; { \"policy\": { \"phases\": { \"hot\": { \"min_age\": \"0ms\", \"actions\": { \"rollover\": { \"max_primary_shard_size\": \"50gb\", \"max_age\": \"7d\" }, \"set_priority\": { \"priority\": 100 } } }, \"warm\": { \"min_age\": \"0d\", \"actions\": { \"set_priority\": { \"priority\": 50 }, \"readonly\": {} } }, \"cold\": { \"min_age\": \"7d\", \"actions\": { \"readonly\": {}, \"searchable_snapshot\": { \"snapshot_repository\": \"found-snapshots\", \"force_merge_index\": true }, \"set_priority\": { \"priority\": 0 }, \"allocate\": { \"number_of_replicas\": 0 } } }, \"frozen\": { \"min_age\": \"14d\", \"actions\": { \"searchable_snapshot\": { \"snapshot_repository\": \"found-snapshots\", \"force_merge_index\": true } } }, \"delete\": { \"min_age\": \"70d\", \"actions\": { \"delete\": { \"delete_searchable_snapshot\": true } } } } } } . ",
    "url": "/ece/updateilm/#erstellen-einer-neuen-ilm-richtlinie",
    
    "relUrl": "/ece/updateilm/#erstellen-einer-neuen-ilm-richtlinie"
  },"24": {
    "doc": "Anpassen von Lifecycle Policies",
    "title": "Zuweisen der Richtlinie zu einem Index",
    "content": "Die neu erstellte Richtlinie kann dann über das Menü Index Lifecycle Policies z.B. einem Index Template zugewiesen werden. ",
    "url": "/ece/updateilm/#zuweisen-der-richtlinie-zu-einem-index",
    
    "relUrl": "/ece/updateilm/#zuweisen-der-richtlinie-zu-einem-index"
  },"25": {
    "doc": "Anpassen von Lifecycle Policies",
    "title": "Rollover des Data Streams",
    "content": "Damit die neue ILM-Richtlinie sofort benutzt wird, muss ein Rollover für den Datenstrom angestoßen werden. Dies kann auch in den Dev Tools von Kibana erfolgen, hier am Beispiel für den Datenstrom auditbeat-8.5.0: . POST auditbeat-8.5.0/_rollover . ",
    "url": "/ece/updateilm/#rollover-des-data-streams",
    
    "relUrl": "/ece/updateilm/#rollover-des-data-streams"
  },"26": {
    "doc": "GKS Changelog v2.21",
    "title": "Unterstützte Kubernetes Versionen",
    "content": "Im aktuellen Release werden die folgenden Kubernetes-Versionen unterstützt: . | 1.22.15 | 1.23.12 | 1.24.6 | . ",
    "url": "/gks/about/changelog-v2.21/#unterst%C3%BCtzte-kubernetes-versionen",
    
    "relUrl": "/gks/about/changelog-v2.21/#unterstützte-kubernetes-versionen"
  },"27": {
    "doc": "GKS Changelog v2.21",
    "title": "End of Life Ankündigungen",
    "content": "Wir werden die Unterstützung der Kubernetes Version v1.22 am 18.10.2022 beenden. Führen Sie bei allen bestehenden Clustern mit Kubernetes Version 1.22 ein Update auf mindestens Version 1.23 bis zu diesem Datum durch. ",
    "url": "/gks/about/changelog-v2.21/#end-of-life-ank%C3%BCndigungen",
    
    "relUrl": "/gks/about/changelog-v2.21/#end-of-life-ankündigungen"
  },"28": {
    "doc": "GKS Changelog v2.21",
    "title": "Neue Funktionen",
    "content": "Projektübersichtsseite . Es gibt eine neue Übersichtsseite für Projekte, die all Projektinformationen Übersichtlich anzeigt. Sollten sie das alte Verhalten bevorzugen, können sie sich das Userprofil umstellen. New CNI Versionen . Canal wurde auf v3.23 und Cilium auf v1.12 updated. Bitte updaten sie ihr CNI in Ihren Clustern. ",
    "url": "/gks/about/changelog-v2.21/#neue-funktionen",
    
    "relUrl": "/gks/about/changelog-v2.21/#neue-funktionen"
  },"29": {
    "doc": "GKS Changelog v2.21",
    "title": "Änderungen in Kubernetes",
    "content": "Upgrade-Hinweise für Kubernetes 1.24 . Wenn Sie ein Upgrade auf Kubernetes 1.24 planen, lesen Sie bitte im offiziellen Kubernetes v1.24 Changelog den Abschnitt What’s New und machen Sie sich mit den bevorstehenden Änderungen vertraut. Es werden diesmal sehr viele beta-Kubernetes APIs entfernt, was potenziell Änderungen im Software-Ausroll-Prozess zur Folge hat. Eine Übersicht über die Änderungen finden Sie im Changelog in Abschnitt Changes by Kind. | Wichtige Hinweise zum Upgrade | Deprecations | API-Änderungen | Features | . ",
    "url": "/gks/about/changelog-v2.21/#%C3%A4nderungen-in-kubernetes",
    
    "relUrl": "/gks/about/changelog-v2.21/#änderungen-in-kubernetes"
  },"30": {
    "doc": "GKS Changelog v2.21",
    "title": "GKS Changelog v2.21",
    "content": " ",
    "url": "/gks/about/changelog-v2.21/",
    
    "relUrl": "/gks/about/changelog-v2.21/"
  },"31": {
    "doc": "GKS Changelog v2.20",
    "title": "Neue URL",
    "content": "Das iMKE-Dashboard wird umbenannt in GKS-Dashboard, da der neue Name besser die Zugehörigkeit zur German Edge Cloud widerspiegelt als der vorherige. Auch der Domänenname ändert sich damit. Das Dashboard ist ab dem 1. Juli 2022 unter der URL https://gks.gec.io erreichbar. ",
    "url": "/gks/about/changelog-v2.20/#neue-url",
    
    "relUrl": "/gks/about/changelog-v2.20/#neue-url"
  },"32": {
    "doc": "GKS Changelog v2.20",
    "title": "Unterstützte Kubernetes Versionen",
    "content": "Im aktuellen Release werden die folgenden Kubernetes-Versionen unterstützt: . | 1.21.8 | 1.22.5 | 1.23.6 | . ",
    "url": "/gks/about/changelog-v2.20/#unterst%C3%BCtzte-kubernetes-versionen",
    
    "relUrl": "/gks/about/changelog-v2.20/#unterstützte-kubernetes-versionen"
  },"33": {
    "doc": "GKS Changelog v2.20",
    "title": "End of Life Ankündigungen",
    "content": "Wir werden die Unterstützung der Kubernetes Version v1.21 am 28.06.2022 beenden. Führen Sie bei allen bestehenden Clustern mit Kubernetes Version 1.21 ein Update auf mindestens Version 1.22 bis zu diesem Datum durch. ",
    "url": "/gks/about/changelog-v2.20/#end-of-life-ank%C3%BCndigungen",
    
    "relUrl": "/gks/about/changelog-v2.20/#end-of-life-ankündigungen"
  },"34": {
    "doc": "GKS Changelog v2.20",
    "title": "Neue Funktionen",
    "content": "Das aktuelle Release der Plattform ist ein technisches Release ohne neue Features. ",
    "url": "/gks/about/changelog-v2.20/#neue-funktionen",
    
    "relUrl": "/gks/about/changelog-v2.20/#neue-funktionen"
  },"35": {
    "doc": "GKS Changelog v2.20",
    "title": "Bugfixes",
    "content": ". | Für Kundencluster, die im Backend etcd 3.5 nutzen (Kubernetes 1.22 Cluster), wurden im Backend etcd Korruptionschecks aktiviert um etcd Dateninkonsistenzen zu entdecken. Diese Checks werden beim etcd Startup sowie alle 4 Stunden durchgeführt. (#13766) | . ",
    "url": "/gks/about/changelog-v2.20/#bugfixes",
    
    "relUrl": "/gks/about/changelog-v2.20/#bugfixes"
  },"36": {
    "doc": "GKS Changelog v2.20",
    "title": "Änderungen in Kubernetes",
    "content": "Upgrade-Hinweise für Kubernetes 1.23 . Wenn Sie ein Upgrade auf Kubernetes 1.23 planen, lesen Sie bitte im offiziellen Kubernetes v1.23 Changelog den Abschnitt What’s New und machen Sie sich mit den bevorstehenden Änderungen vertraut. Es werden diesmal sehr viele beta-Kubernetes APIs entfernt, was potenziell Änderungen im Software-Ausroll-Prozess zur Folge hat. Eine Übersicht über die Änderungen finden Sie im Changelog in Abschnitt Changes by Kind. | Wichtige Hinweise zum Upgrade | Deprecations | API-Änderungen | Features | . ",
    "url": "/gks/about/changelog-v2.20/#%C3%A4nderungen-in-kubernetes",
    
    "relUrl": "/gks/about/changelog-v2.20/#änderungen-in-kubernetes"
  },"37": {
    "doc": "GKS Changelog v2.20",
    "title": "GKS Changelog v2.20",
    "content": " ",
    "url": "/gks/about/changelog-v2.20/",
    
    "relUrl": "/gks/about/changelog-v2.20/"
  },"38": {
    "doc": "GKS Changelog v2.19",
    "title": "Unterstützte Kubernetes Versionen",
    "content": "Im Rahmen des aktuellen Release werden die folgenden Kubernetes-Versionen unterstützt: . | 1.21.8 | 1.22.5 | . ",
    "url": "/gks/about/changelog-v2.19/#unterst%C3%BCtzte-kubernetes-versionen",
    
    "relUrl": "/gks/about/changelog-v2.19/#unterstützte-kubernetes-versionen"
  },"39": {
    "doc": "GKS Changelog v2.19",
    "title": "End of Life Ankündigungen",
    "content": "Wir werden die Unterstützung der Kubernetes Version v1.21 am 28.06.2022 beenden. Bitte updaten Sie alle bestehenden Cluster mit Kubernetes Version 1.21 auf mindestens 1.22 bis zu diesem Datum. ",
    "url": "/gks/about/changelog-v2.19/#end-of-life-ank%C3%BCndigungen",
    
    "relUrl": "/gks/about/changelog-v2.19/#end-of-life-ankündigungen"
  },"40": {
    "doc": "GKS Changelog v2.19",
    "title": "Neue Funktionen",
    "content": ". | CNI Update Funktionalität: es ist nun möglich die CNI canal im dashboard upzudaten | . ",
    "url": "/gks/about/changelog-v2.19/#neue-funktionen",
    
    "relUrl": "/gks/about/changelog-v2.19/#neue-funktionen"
  },"41": {
    "doc": "GKS Changelog v2.19",
    "title": "Bugfixes",
    "content": ". | Um eine Reihe von CVEs (CVE-2021-44716, CVE-2021-44717, CVE-2021-3711, CVE-2021-3712, CVE-2021-33910) zu mitigieren wurden die unterstützten Kubernetes-Versionen aktualisiert. Die Controlplanes der Cluster wurden automatisch upgedated. Bitte stellen Sie ein zeitnahes Updates Ihrer Machine Deployments sicher. Bei einem Update der Machine Deployments kann es durch den rollierenden Neustart der Worker-Nodes zu rollierenden Restarts der Deployments und Statefulsets kommen. | . ",
    "url": "/gks/about/changelog-v2.19/#bugfixes",
    
    "relUrl": "/gks/about/changelog-v2.19/#bugfixes"
  },"42": {
    "doc": "GKS Changelog v2.19",
    "title": "Änderungen in Kubernetes",
    "content": "Upgrade-Hinweise für Kubernetes 1.22 . Wenn Sie ein Upgrade auf Kubernetes 1.22 planen, lesen Sie bitte den Abschnitt What’s New des offiziellen Kubernetes v1.22 Changelogs und machen Sie sich mit den bevorstehenden Änderungen vertraut. Es werden diesmal sehr viele beta-Kubernetes APIs entfernt, welches potenziell Änderungen in ihren Software-Ausroll-Prozess zur Folge hat. Eine Übersicht über die Änderungen finden Sie im Abschnitt Changes by Kind des Changelogs. | Wichtige Hinweise zum Upgrade | Deprecations | API-Änderungen | Features | . ",
    "url": "/gks/about/changelog-v2.19/#%C3%A4nderungen-in-kubernetes",
    
    "relUrl": "/gks/about/changelog-v2.19/#änderungen-in-kubernetes"
  },"43": {
    "doc": "GKS Changelog v2.19",
    "title": "GKS Changelog v2.19",
    "content": " ",
    "url": "/gks/about/changelog-v2.19/",
    
    "relUrl": "/gks/about/changelog-v2.19/"
  },"44": {
    "doc": "GKS Changelog v2.18",
    "title": "Unterstützte Kubernetes Versionen",
    "content": "Im Rahmen des aktuellen Release werden die folgenden Kubernetes-Versionen unterstützt: . | 1.19.15 | 1.20.11 | 1.21.5 | 1.22.2 | . ",
    "url": "/gks/about/changelog-v2.18/#unterst%C3%BCtzte-kubernetes-versionen",
    
    "relUrl": "/gks/about/changelog-v2.18/#unterstützte-kubernetes-versionen"
  },"45": {
    "doc": "GKS Changelog v2.18",
    "title": "End of Life Ankündigungen",
    "content": "Wir werden die Unterstützung der Kubernetes Version v1.19 am 03.11.2021 beenden. Bitte updaten Sie alle bestehenden Cluster mit Kubernetes Version 1.19 auf mindestens 1.20 bis zu diesem Datum. ",
    "url": "/gks/about/changelog-v2.18/#end-of-life-ank%C3%BCndigungen",
    
    "relUrl": "/gks/about/changelog-v2.18/#end-of-life-ankündigungen"
  },"46": {
    "doc": "GKS Changelog v2.18",
    "title": "Neue Funktionen",
    "content": ". | Cluster Templates werden ab sofort unterstützt | Ältere Cluster (erstellt vor v1.17) können jetzt auf den externen Cloud Controller Manager migriert werden | containerd wird ab sofort als Container Runtime unterstützt (Wie Sie Ihre Cluster zu containerd umziehen können) | . ",
    "url": "/gks/about/changelog-v2.18/#neue-funktionen",
    
    "relUrl": "/gks/about/changelog-v2.18/#neue-funktionen"
  },"47": {
    "doc": "GKS Changelog v2.18",
    "title": "Bugfixes",
    "content": ". | Um CVE-2021-25741 zu mitigieren wurden die unterstützten Kubernetes-Versionen aktualisiert. Die Controlplanes der Cluster wurden automatisch upgedatet. Bitte stellen Sie ein zeitnahes Updates Ihrer Machine Deployments sicher. Bei einem Update der Machine Deployments kann es durch den rollierenden Neustart der Worker-Nodes zu rollierenden Restarts der Deployments und Statefulsets kommen. | . ",
    "url": "/gks/about/changelog-v2.18/#bugfixes",
    
    "relUrl": "/gks/about/changelog-v2.18/#bugfixes"
  },"48": {
    "doc": "GKS Changelog v2.18",
    "title": "Änderungen in Kubernetes",
    "content": "Upgrade-Hinweise für Kubernetes 1.21 . Wenn Sie ein Upgrade auf Kubernetes 1.21 planen, lesen Sie bitte den Abschnitt What’s New des offiziellen Kubernetes v1.21 Changelogs und machen Sie sich mit den bevorstehenden Änderungen vertraut. Eine Übersicht über die Änderungen finden Sie im Abschnitt Changes by Kind des Changelogs. | Wichtige Hinweise zum Upgrade | Deprecations | API-Änderungen | Features | . Upgrade-Hinweise für Kubernetes 1.22 . Wenn Sie ein Upgrade auf Kubernetes 1.22 planen, lesen Sie bitte den Abschnitt What’s New des offiziellen Kubernetes v1.22 Changelogs und machen Sie sich mit den bevorstehenden Änderungen vertraut. Eine Übersicht über die Änderungen finden Sie im Abschnitt Changes by Kind des Changelogs. | Wichtige Hinweise zum Upgrade | Deprecations | API-Änderungen | Features | . ",
    "url": "/gks/about/changelog-v2.18/#%C3%A4nderungen-in-kubernetes",
    
    "relUrl": "/gks/about/changelog-v2.18/#änderungen-in-kubernetes"
  },"49": {
    "doc": "GKS Changelog v2.18",
    "title": "GKS Changelog v2.18",
    "content": " ",
    "url": "/gks/about/changelog-v2.18/",
    
    "relUrl": "/gks/about/changelog-v2.18/"
  },"50": {
    "doc": "GKS Changelog v2.17",
    "title": "Unterstützte Kubernetes Versionen",
    "content": "Im Rahmen des aktuellen Release werden die folgenden Kubernetes-Versionen unterstützt: . | 1.18.18 | 1.19.10 | 1.19.11 | 1.20.7 | 1.21.2 | . ",
    "url": "/gks/about/changelog-v2.17/#unterst%C3%BCtzte-kubernetes-versionen",
    
    "relUrl": "/gks/about/changelog-v2.17/#unterstützte-kubernetes-versionen"
  },"51": {
    "doc": "GKS Changelog v2.17",
    "title": "End of Life Ankündigungen",
    "content": "Wir werden die Unterstützung der Kubernetes Version v1.18 am 15.09.2021 beenden. Bitte updaten Sie alle bestehenden Cluster mit Kubernetes Version 1.18 auf mindestens 1.19 bis zu diesem Datum. ",
    "url": "/gks/about/changelog-v2.17/#end-of-life-ank%C3%BCndigungen",
    
    "relUrl": "/gks/about/changelog-v2.17/#end-of-life-ankündigungen"
  },"52": {
    "doc": "GKS Changelog v2.17",
    "title": "Neue Funktionen",
    "content": ". | Support für Kubernetes 1.20 hinzugefügt | . ",
    "url": "/gks/about/changelog-v2.17/#neue-funktionen",
    
    "relUrl": "/gks/about/changelog-v2.17/#neue-funktionen"
  },"53": {
    "doc": "GKS Changelog v2.17",
    "title": "Änderungen in Kubernetes",
    "content": "Upgrade-Hinweise für Kubernetes 1.20 . Wenn Sie ein Upgrade auf Kubernetes 1.20 planen, lesen Sie bitte den Abschnitt Upgrade Notes des offiziellen Kubernetes v1.20 Changelogs und machen Sie sich mit den bevorstehenden Änderungen vertraut. Eine Übersicht über die Änderungen finden Sie im Abschnitt Changes by Kind des Changelogs. | Wichtige Hinweise zum Upgrade | Deprecations | API-Änderungen | Features | . Upgrade-Hinweise für Kubernetes 1.21 . Wenn Sie ein Upgrade auf Kubernetes 1.21 planen, lesen Sie bitte den Abschnitt What’s New des offiziellen Kubernetes v1.21 Changelogs und machen Sie sich mit den bevorstehenden Änderungen vertraut. Eine Übersicht über die Änderungen finden Sie im Abschnitt Changes by Kind des Changelogs. | Wichtige Hinweise zum Upgrade | Deprecations | API-Änderungen | Features | . ",
    "url": "/gks/about/changelog-v2.17/#%C3%A4nderungen-in-kubernetes",
    
    "relUrl": "/gks/about/changelog-v2.17/#änderungen-in-kubernetes"
  },"54": {
    "doc": "GKS Changelog v2.17",
    "title": "GKS Changelog v2.17",
    "content": " ",
    "url": "/gks/about/changelog-v2.17/",
    
    "relUrl": "/gks/about/changelog-v2.17/"
  },"55": {
    "doc": "Über GKS",
    "title": "Über GKS",
    "content": " ",
    "url": "/gks/about/#%C3%BCber-gks",
    
    "relUrl": "/gks/about/#über-gks"
  },"56": {
    "doc": "Über GKS",
    "title": "Was ist die GKS Plattform?",
    "content": "Die GKS Plattform stellt Managed Kubernetes Cluster bereit. Als Kunde können Sie mit einem einfachen Klick in unserem Web Interface Kubernetes Cluster erstellen, aktualisieren oder auch wieder löschen. Was ist ein “Managed Kubernetes Cluster”? . Kubernetes Cluster bieten eine verfügbare und skalierbare Umgebung für containerisierte Anwendungen. Bei einem gemanagten Kubernetes Cluster auf unserer GKS Plattform: . | Kümmern wir uns um die zugrundeliegende Server-Infrastruktur und die Kubernetes-Installation. | Stellen wir die Skalierbarkeit Ihres Clusters sicher und geben Ihnen die Möglichkeit, die Anzahl und Größe Ihrer Worker-Nodes nach Ihren Bedürfnissen frei zu konfigurieren. | Übernehmen wir das Management und den Betrieb der Kubernetes-Controlplane. | . Während wir uns um den stabilen Betrieb Ihres Clusters kümmern, haben Sie trotzdem den Vollzugriff auf den Cluster über eine kubeconfig-Datei. Sie haben die volle Kontrolle über Ihre Cluster und können: . | Eine unbegrenzte Anzahl von Clustern über das GKS Web Interface verwalten und neue Cluster schnell erstellen. | Die Kubernetes-Version bestehender Cluster mit einem einfachen Klick aktualisieren und so Ihre Cluster immer aktuell halten. | Nicht mehr benötigte Cluster mit einem einfachen Klick löschen. | Bei Bedarf vollen Root-Zugriff auf Ihre Worker-Nodes bekommen (zum Beispiel, wenn Sie Ihre selbst entwickelten Applikationen intensiv debuggen wollen). | . Bei Problemen im Clusterbetrieb können Sie sich jederzeit an unseren 24/7-Support wenden. Jenseits der Betriebsleistung der GKS Plattform bieten wir mit unserem Professional Service Team auch eine weitergehende Beratung an. Wir helfen Ihnen beispielsweise bei allgemeinen Fragen zum Applikationsbetrieb auf Kubernetes oder auch bei spezifischen Fragen zum optimalen Anwendungs- und Softwaredesign für Ihr konkretes Vorhaben. Wie kann ich einen “Managed Kubernetes Cluster” benutzen? . Sie können einen GKS Cluster so benutzen wie jeden anderen Kubernetes Cluster auch. Da es sich bei einem GKS Cluster um einen gemanagten Kubernetes Cluster handelt, können Sie sich dabei voll und ganz auf den Applikationsbetrieb konzentrieren. Beispielsweise können Sie ein GKS Cluster nutzen, um bereits paketierte, containerisierte Anwendungen mit Hilfe von helm zu installieren, zum Beispiel: . | Eine vollständige WordPress-Installation | Ein Drupal CMS | Eine private nextcloud Filesharing-Umgebung | . Kubernetes ist dabei selbstverständlich nicht auf bereits fertig paketierte, vollständige Anwendungen beschränkt. Vielmehr ist Kubernetes die natürliche Umgebung, um Ihre eigenen, zeitgemäßen “cloud-native”-Anwendungen zu entwickeln und produktiv zu betreiben. Als Entwicklungsumgebung könnten Sie beispielsweise die folgenden Anwendungen in einem Kubernetes Cluster bereitstellen: . | GitLab zur Verwaltung des Sourcecode | Jenkins zur Automatisierung Ihrer Entwicklungsworkflows | Artifactory um Ihre Buildergebnisse versioniert abzulegen | . Weiterhin könnten Sie jeweils einen Entwicklungs-, Staging- und Produktions-Cluster in GKS anlegen und Ihre Anwendungsarchitektur mit einfach zu installierenden und mit Helm paketierten Komponenten unterstützen, zum Beispiel: . | Einem kompletten Kafka-Setup | Einer PostgreSQL-, MySQL- oder MariaDB-Datenbank | Einem kompletten Prometheus-Monitoring-Stack inkl. Prometheus, Alertmanager und Grafana | . Dies sind nur einige Beispiele. Für eine weitergehende Beratung fragen Sie am besten direkt unser Professional Services Team. ",
    "url": "/gks/about/#was-ist-die-gks-plattform",
    
    "relUrl": "/gks/about/#was-ist-die-gks-plattform"
  },"57": {
    "doc": "Über GKS",
    "title": "Wie funktioniert die GKS Plattform?",
    "content": "Die GKS Plattform selbst basiert ebenfalls auf Kubernetes. Intern läuft die Controlplane eines Kundenclusters in einem dedizierten Kubernetes Namespace. Die einzelnen Komponenten der Controlplane sind dabei als Deployments bzw. Statefulsets gestartet: . | Kubernetes API Server | Etcd Datenbank | Controller-Manager | Scheduler | Machine-Controller | … andere Komponenten der Controlplane. | . Die Controlplane in Kubernetes zu betreiben, hat viele Vorteile: einzelne Pods werden automatisch über verschiedene Server verteilt (Scheduling), das gesamte Setup ist skalierbar und weitgehend selbstheilend, da abgestürzte Pods automatisch neugestartet werden. Die Worker-Nodes eines Kundenclusters werden wiederum als selbstständige VMs im Openstack-Tenant des Kunden betrieben. Der Machine-Controller in der Controlplane kümmert sich dabei um die automatische Erstellung der VMs sowie ihre Eingliederung in den bestehenden Cluster. Dabei können mit dem Web Interface jederzeit neue Nodes im Cluster hinzugefügt oder auch gelöscht werden. Komplexere Änderungen werden durch den Machine-Controller in einem “Rolling-Upgrade” durchgeführt - immer eine Worker-Node nach der anderen. So werden Downtimes während eines Upgrades minimiert. ",
    "url": "/gks/about/#wie-funktioniert-die-gks-plattform",
    
    "relUrl": "/gks/about/#wie-funktioniert-die-gks-plattform"
  },"58": {
    "doc": "Über GKS",
    "title": "Zertifizierungen",
    "content": ". GKS ist ein Produkt um effizient Kubernetes Cluster in der GEC Cloud zu betreiben. Kunden können über ein übersichtliches Web Interface mit nur ein paar Klicks komplett funktionale und von der Cloud Native Computing Foundation zertifizierte Cluster hochfahren und für Applikations-Deployments verwenden. Conformance Ergebnisse für unsere Plattform können Sie jederzeit unter CNCF Kubernetes Conformance einsehen. ",
    "url": "/gks/about/#zertifizierungen",
    
    "relUrl": "/gks/about/#zertifizierungen"
  },"59": {
    "doc": "Über GKS",
    "title": "Über GKS",
    "content": " ",
    "url": "/gks/about/",
    
    "relUrl": "/gks/about/"
  },"60": {
    "doc": "Unterstützte Kubernetes Versionen",
    "title": "Unterstützte Kubernetes Versionen",
    "content": "Folgende Kubernetes Versionen unterstützen wir aktuell in GKS. | Version | GKS Deprecation | GKS End-of-Life | . | v1.24 |   |   | . | v1.23 | 31.01.2023 | 18.04.2023 | . | v1.22 | 19.07.2022 | 18.10.2022 | . | v1.21 | 04.03.2022 | 28.06.2022 | . | v1.20 | 22.11.2021 | 01.03.2022 | . | v1.19 | 22.07.2021 | 03.11.2021 | . | v1.18 | 14.06.2021 | 15.09.2021 | . | v1.17 | 20.04.2021 | 31.05.2021 | . | v1.16 | 10.12.2020 | 10.03.2021 | . | v1.15 | 10.12.2020 | 10.03.2021 | . ",
    "url": "/gks/about/kubernetesversions/#unterst%C3%BCtzte-kubernetes-versionen",
    
    "relUrl": "/gks/about/kubernetesversions/#unterstützte-kubernetes-versionen"
  },"61": {
    "doc": "Unterstützte Kubernetes Versionen",
    "title": "Force Upgrade Policy",
    "content": "Falls ein Kundencluster von seinem Eigentümer nicht zum angekündigten End-of-Life Datum aktualisiert wird, wird er automatisch auf die letzte noch unterstützte Version aktualisiert. Weitere Informationen über die GKS Deprecation- und Force-Upgrade-Richtlinie finden Sie hier. ",
    "url": "/gks/about/kubernetesversions/#force-upgrade-policy",
    
    "relUrl": "/gks/about/kubernetesversions/#force-upgrade-policy"
  },"62": {
    "doc": "Unterstützte Kubernetes Versionen",
    "title": "Unterstützte Kubernetes Versionen",
    "content": " ",
    "url": "/gks/about/kubernetesversions/",
    
    "relUrl": "/gks/about/kubernetesversions/"
  },"63": {
    "doc": "Erste Schritte",
    "title": "Erste Schritte",
    "content": "Diese Anleitung beschreibt, wie Sie Ihr erstes GKS-Projekt inkl. einen ersten Kubernetes Cluster anlegen, wie Sie auf den Cluster zugreifen und anschließend die angelegten Ressourcen wieder vollständig löschen können. ",
    "url": "/gks/gettingstarted/#erste-schritte",
    
    "relUrl": "/gks/gettingstarted/#erste-schritte"
  },"64": {
    "doc": "Erste Schritte",
    "title": "Das erste Projekt anlegen",
    "content": "Nach dem Login in GKS erscheint folgendes Fenster. Klicken Sie auf Add Project. Danach öffnet sich ein Fenster, in Sie dem Projekt einen Namen geben. Als Beispiel verwenden Sie hier Team Kubernetes. Klicken Sie dann auf Save Project. Im Anschluss legt GKS das Projekt an und stellt es in der Übersicht dar. Mit einem Klick auf den Eintrag Team Kubernetes sind Sie im Projekt-Umfeld und können den Cluster anlegen. Die folgende Seite zeigt das Projekt. Hier sind alle bereits bestehenden Cluster sowie zugehörige User und weitere Kontroll-Mechanismen sichtbar. Im Augenblick ist diese Liste noch leer, bis Sie Ihren ersten Kubernetes Cluster erstellt haben. ",
    "url": "/gks/gettingstarted/#das-erste-projekt-anlegen",
    
    "relUrl": "/gks/gettingstarted/#das-erste-projekt-anlegen"
  },"65": {
    "doc": "Erste Schritte",
    "title": "Den ersten Cluster erstellen",
    "content": "Um einen Cluster anzulegen, klicken Sie im gewünschten Projekt oben rechts auf Create Cluster. Jetzt öffnet sich die erste Seite für den Prozess des Cluster Anlegens. Wählen Sie den Provider openstack. Wählen Sie anschließend eine der drei Verfügbarkeitszonen aus, in unserem Beispiel nehmen Sie IX2. Im nächsten Schritt konfigurieren Sie die Cluster-Details. Nennen Sie den Cluster first-system und wählen Sie die gewünschte Kubernetes-Version aus. Für den gelegentlichen SSH-Zugriff auf Worker-Nodes können Sie optional einen öffentlichen SSH-Schlüssel hinterlegen. Zum Hinzufügen eines SSH Keys klicken Sie auf Add SSH Key. In dem sich öffnenden Dialog können Sie dann Ihren SSH Public Key eintragen und ihm einen passenden Namen geben. Damit GKS in der OpenStack-Infrastruktur die notwendigen Ressourcen erzeugen kann, geben Sie im nächsten Schritt Ihre Zugangsdaten ein. Danach wird der Inhalt im Feld Project automatisch aktualisiert und Sie können in der Dropdownliste Ihr gewünschtes OpenStack Projekt auswählen. Mit dem Hinzufügen Ihrer Zugangsdaten und dem Auswählen des OpenStack-Projekts sind alle notwendigen Eingaben getätigt und Sie können mit dem nächsten Schritt fortfahren. Wenn Sie das tun, wird automatisch ein eigenes Netzwerk, Subnetz sowie eine Security Gruppe für den neuen Cluster erstellt. Es ist allerdings auch möglich, ein existierendes Netzwerk zu verwenden, um den Cluster zu erstellen. Dazu müssen Sie das Netzwerk und das Subnetz auswählen. Diese müssen allerdings mit einem Router verbunden sein. In unserer OpenStack Dokumentation ist beschrieben, wie Sie einen Router erstellen und mit einem Netzwerk verbinden können. Im nächsten Schritt definieren Sie, wie viele und welche virtuellen Maschinen als Worker-Nodes im Cluster verfügbar sein sollen. Zuerst geben Sie dem sogenannten Machine Deployment einen Namen. Für Ihren Testcluster nutzen Sie dazu den Namensgenerator. Danach spezifizieren Sie die Replicas (Anzahl der Worker-Nodes im Kubernetes Cluster) und den Flavor (Maschinentyp), welcher im Wesentlichen die Anzahl der verfügbaren CPU-Kerne und des RAMs bestimmt. Wählen Sie Flatcar als Betriebssystem für die Worker-Nodes. Über einen Klick auf Next gelangen Sie zum letzten Schritt, wo Sie noch einmal alle Einstellungen verifizieren und mit Create Cluster die Cluster-Erstellung starten können. Nun wird der Cluster erstellt. Um auf die Informationen zugreifen zu können, müssen Sie nun wieder zur Cluster-Übersicht des Projektes gehen und dort Ihren Cluster auswählen. Nachdem Sie Ihren Cluster ausgewählt haben, kommen Sie auf die Seite mit allen Cluster-Details. ",
    "url": "/gks/gettingstarted/#den-ersten-cluster-erstellen",
    
    "relUrl": "/gks/gettingstarted/#den-ersten-cluster-erstellen"
  },"66": {
    "doc": "Erste Schritte",
    "title": "Auf das Cluster zugreifen",
    "content": "Um auf den Cluster zuzugreifen, klicken Sie oben rechts auf die Get Kubeconfig Schaltfläche. Damit laden Sie eine Datei herunter, die sich im Kubernetes-Jargon kubeconfig nennt. In dieser Datei stehen alle Endpunkte, Zertifikate sowie Bereiche des Clusters. Mit dieser Datei ist kubectl in der Lage, sich mit dem Cluster zu verbinden. Um diese Datei zu nutzen, müssen Sie diese auf der Konsole registrieren. Dafür gibt es zwei Möglichkeiten: . | kubectl schaut als Standard in die Datei .kube/config im Heimat-Verzeichnis des Benutzers | Sie können die kubeconfig temporär mit Hilfe einer Umgebungsvariable exportieren | . Der Einfachheit halber und um auf Ihrem System die Standards nicht zu verändern, wählen Sie hier die Variante 2. Dafür benutzen Sie eine Konsole. In den Screenshots wird iTerm2 auf macOS verwendet, es funktioniert jedoch auf Linux und Windows genau so. Als Erstes müssen Sie die heruntergeladene Datei finden. Chrome und Firefox laden diese beide normalerweise in den Downloads-Ordner. Der Dateiname setzt sich jetzt aus zwei Komponenten zusammen. | kubeconfig-admin- | Ihre Cluster ID | . Um diese dann zu registrieren, nutzen Sie folgendes Kommando: . cd Downloads export KUBECONFIG=$(pwd)/kubeconfig-admin-CLUSTERID . Nun können Sie mit Ihrem Cluster kommunizieren. Das einfachste Kommando ist hier, sich alle Nodes seines Clusters anzeigen zu lassen: . kubectl get nodes NAME STATUS ROLES AGE VERSION musing-kalam-XXXXXXXXX-ks4xz Ready &lt;none&gt; 10m v1.21.5 musing-kalam-XXXXXXXXX-txc4w Ready &lt;none&gt; 10m v1.21.5 musing-kalam-XXXXXXXXX-vc4g2 Ready &lt;none&gt; 10m v1.21.5 . ",
    "url": "/gks/gettingstarted/#auf-das-cluster-zugreifen",
    
    "relUrl": "/gks/gettingstarted/#auf-das-cluster-zugreifen"
  },"67": {
    "doc": "Erste Schritte",
    "title": "Aufräumen",
    "content": "Um nach diesem ersten Test den Cluster wieder zu löschen, klicken Sie auf Delete: . In dem sich öffnenden Fenster wird als Sicherheitsfrage der Cluster-Name abgefragt. Da Sie alles löschen wollen, lassen Sie beide Checkboxen angekreuzt. Damit werden auch Volumes und Loadbalancer in OpenStack gelöscht. ",
    "url": "/gks/gettingstarted/#aufr%C3%A4umen",
    
    "relUrl": "/gks/gettingstarted/#aufräumen"
  },"68": {
    "doc": "Erste Schritte",
    "title": "Erste Schritte",
    "content": " ",
    "url": "/gks/gettingstarted/",
    
    "relUrl": "/gks/gettingstarted/"
  },"69": {
    "doc": "GKS-Projekte verwalten",
    "title": "GKS-Projekte verwalten",
    "content": "Ein GKS-Projekt ist eine Abstraktionsschicht, um mehrere Kubernetes Cluster gemeinsam verwalten zu können. Alle Kubernetes Cluster in einem GKS-Projekt: . | Haben die gleichen Benutzer (“Member” des Projekts). Die für das Projekt freigeschalteten Benutzer können sich über das GKS-Frontend anmelden und alle im Projekt vorhandenen Cluster (je nach Zugriffslevel) sehen bzw. administrieren. Sie können sich auch eine kubeconfig-Datei für den direkten Zugriff auf den Cluster herunterladen. | Haben die gleichen Service Accounts, die auf die GKS REST API mit einem API-Token zugreifen können. | Können im Projekt hinterlegte SSH-Keys zugewiesen haben. | . Projekte sind im o.g. Sinn nur für die Verwaltung wichtig und haben technisch weiter keine Auswirkungen auf die enthaltenen Kubernetes Cluster. ",
    "url": "/gks/managingprojects/#gks-projekte-verwalten",
    
    "relUrl": "/gks/managingprojects/#gks-projekte-verwalten"
  },"70": {
    "doc": "GKS-Projekte verwalten",
    "title": "Weiterführende Themen",
    "content": ". | Ein GKS-Projekt anlegen | Benutzer-Management in GKS-Projekten | Service-Accounts in GKS-Projekten | . ",
    "url": "/gks/managingprojects/#weiterf%C3%BChrende-themen",
    
    "relUrl": "/gks/managingprojects/#weiterführende-themen"
  },"71": {
    "doc": "GKS-Projekte verwalten",
    "title": "GKS-Projekte verwalten",
    "content": " ",
    "url": "/gks/managingprojects/",
    
    "relUrl": "/gks/managingprojects/"
  },"72": {
    "doc": "Ein Projekt anlegen",
    "title": "Ein Projekt anlegen",
    "content": "Nach dem Login in GKS erscheint folgendes Fenster. Klicken Sie auf Add Project. Geben Sie in dem sich öffnenden Fenster dem Projekt einen Namen. Als Beispiel verwenden Sie hier Team Kubernetes. Klicken Sie danach auf Save Project. Im Anschluss legt GKS das Projekt an und stellt es in der Übersicht dar. Mit einem Klick auf den Eintrag Team Kubernetes sind Sie im Projekt-Umfeld und können den Cluster anlegen. Die folgende Seite zeigt das Projekt. Hier sind alle bereits bestehenden Cluster sowie zugehörige User und weitere Kontroll-Mechanismen sichtbar. Diese Liste ist solange leer, bis Sie Ihren ersten Kubernetes Cluster erstellt haben. Mit einem Klick auf die Seitenleiste links öffnen Sie die Navigation im Projekt-Umfeld und können die weiteren Bereiche erkunden. ",
    "url": "/gks/managingprojects/creatingaproject/#ein-projekt-anlegen",
    
    "relUrl": "/gks/managingprojects/creatingaproject/#ein-projekt-anlegen"
  },"73": {
    "doc": "Ein Projekt anlegen",
    "title": "Ein Projekt anlegen",
    "content": " ",
    "url": "/gks/managingprojects/creatingaproject/",
    
    "relUrl": "/gks/managingprojects/creatingaproject/"
  },"74": {
    "doc": "Benutzer Management im Projekt",
    "title": "Benutzer Management im Projekt",
    "content": " ",
    "url": "/gks/managingprojects/projectusermanagement/#benutzer-management-im-projekt",
    
    "relUrl": "/gks/managingprojects/projectusermanagement/#benutzer-management-im-projekt"
  },"75": {
    "doc": "Benutzer Management im Projekt",
    "title": "Benutzer hinzufügen",
    "content": "Sie können mit ein paar Klicks einen Benutzer zu einem vorhandenen GKS-Projekt hinzufügen. Hinweis: Der Benutzer muss sich vorher einmalig in GKS anmelden, bevor er verwendet werden kann. Damit Sie den Benutzer hinzufügen können, brauchen Sie: . | Den Projektnamen | Die Benutzer-E-Mail | . Wählen Sie zuerst das korrekte Projekt aus. Klicken Sie dann in der linken Seitenleiste auf Members. Als nächstes klicken Sie oben rechts auf Add Member. Geben Sie zum Schluss die Benutzer-E-Mail-Adresse und die gewünschte Rolle (Owner, Editor, oder Viewer) ein. ",
    "url": "/gks/managingprojects/projectusermanagement/#benutzer-hinzuf%C3%BCgen",
    
    "relUrl": "/gks/managingprojects/projectusermanagement/#benutzer-hinzufügen"
  },"76": {
    "doc": "Benutzer Management im Projekt",
    "title": "Benutzer entfernen",
    "content": "Um einen Benutzer zu entfernen, gehen Sie wieder in den Members-Bereich des Projekts. Wählen Sie zuerst das Projekt aus. Klicken Sie dann in der linken Seitenleiste auf Members. Mit dem Löschen-Symbol können Sie den Benutzer entfernen. ",
    "url": "/gks/managingprojects/projectusermanagement/#benutzer-entfernen",
    
    "relUrl": "/gks/managingprojects/projectusermanagement/#benutzer-entfernen"
  },"77": {
    "doc": "Benutzer Management im Projekt",
    "title": "Benutzer Offboarding",
    "content": "Vor dem endgültigen Offboarding eines Benutzers sollte dieser zuerst aus allen Projekten entfernt werden. Beachten Sie, dass Projekte, in denen der Benutzer alleiniges Mitglied ist, nicht gesehen werden können. Diese Projekte werden bei einem Löschauftrag an den GKS-Support jedoch automatisch entfernt, sofern diese keine aktiven Cluster mehr enthalten. Bei aktiven Clustern erfolgt eine Rückfrage, diese werden nicht automatisch gelöscht. ",
    "url": "/gks/managingprojects/projectusermanagement/#benutzer-offboarding",
    
    "relUrl": "/gks/managingprojects/projectusermanagement/#benutzer-offboarding"
  },"78": {
    "doc": "Benutzer Management im Projekt",
    "title": "Benutzer Management im Projekt",
    "content": " ",
    "url": "/gks/managingprojects/projectusermanagement/",
    
    "relUrl": "/gks/managingprojects/projectusermanagement/"
  },"79": {
    "doc": "Service Account Tokens verwalten",
    "title": "Service Account Tokens verwalten",
    "content": " ",
    "url": "/gks/managingprojects/projectserviceaccounts/#service-account-tokens-verwalten",
    
    "relUrl": "/gks/managingprojects/projectserviceaccounts/#service-account-tokens-verwalten"
  },"80": {
    "doc": "Service Account Tokens verwalten",
    "title": "Service Accounts",
    "content": "Service Accounts ermöglichen die Nutzung eines langlebigen Tokens, welchen man für Authentifizierung mit der GKS API nutzen kann. Ein Service Account ist ein spezielles Userkonto, welches einem GKS Projekt, und nicht einem individuellen Endbenutzer gehört. Die Ressourcen des Projekts nehmen die Identität des Service Accounts an, um die GKS API anzusprechen, so dass die Benutzer nicht direkt involviert sind. Ein Service Account kann einen oder mehrere JTW Tokens haben, welcher für die Authentifizierung bei der GKS API verwendet werden kann. Ein JWT Token ist standardmäßig 3 Jahre lang gültig. ",
    "url": "/gks/managingprojects/projectserviceaccounts/#service-accounts",
    
    "relUrl": "/gks/managingprojects/projectserviceaccounts/#service-accounts"
  },"81": {
    "doc": "Service Account Tokens verwalten",
    "title": "Kernkonzept",
    "content": "Service Accounts sind Ressourcen eines Projekts. Nur der Eigentümer eines Projekts kann einen Service Account anlegen. Es ist nicht notwendig, eine neue Gruppe für die Service Accounts zu erstellen. Wir können die Service Accounts einer bereits existierenden Gruppen: Project Manager, Editor oder Viewer zuweisen. Ein Service Account ist automatisch durch ein UserProjectBinding mit dem Projekt verknüpft, welche eine Bindung zwischen dem Service Account und dem Projekt definiert. Ein Service Account wird nach dem Löschen eines Projekts automatisch entfernt. ",
    "url": "/gks/managingprojects/projectserviceaccounts/#kernkonzept",
    
    "relUrl": "/gks/managingprojects/projectserviceaccounts/#kernkonzept"
  },"82": {
    "doc": "Service Account Tokens verwalten",
    "title": "Erstellen eines Service Accounts mit Token",
    "content": ". | Wählen Sie ein Projekt aus. | Gehen Sie zur Service Accounts Seite. | Benutzen Sie die Add Service Account Schaltfläche. | Geben Sie den Namen des Service Accounts und die Gruppe (Project Manager, Editor oder Viewer) an. | Fügen Sie den SA hinzu mit Add Service Account. | . Nun wurde ein Service Account angelegt. Um einen Token mit dem SA zu verknüpfen, gehen Sie folgendermaßen vor: . | Wählen Sie den Service Account aus. | Benutzen Sie die + Add Token Schaltfläche. | Geben Sie einen Namen an und fügen Sie den Token mit Add Token hinzu. | Nun wird der generierte Token angezeigt. Laden Sie diesen mit dem Pfeil Icon als Datei herunter oder sichern Sie ihn mit Copy&amp;Paste. | . Wichtiger Hinweis: Stellen Sie sicher, dass dieser Token an einem sicheren Ort und nur auf Ihrem eigenen Gerät gesichert wird. Der Token kann nicht nochmal angezeigt werden, nachdem das Dashboard Fenster geschlossen wurde. ",
    "url": "/gks/managingprojects/projectserviceaccounts/#erstellen-eines-service-accounts-mit-token",
    
    "relUrl": "/gks/managingprojects/projectserviceaccounts/#erstellen-eines-service-accounts-mit-token"
  },"83": {
    "doc": "Service Account Tokens verwalten",
    "title": "Zugriff an die API durch einen Service Account Token",
    "content": "Ein Client kann sich bei dem Server authentifizieren, indem er ein Authorization Request Header Feld mit dem SA Token inkludiert. Authorization: Bearer aaa.bbb.ccc . Beispiel: Um eine Liste der Cluster abzufragen, können Sie den folgenden API-Aufruf nutzen: . curl -X GET \"https://gks.gec.io/api/v1/projects?displayAll=true\" -H \"accept: application/json\" -H \"authorization: Bearer eyJhbXxXXxXxX...\" | jq . Das Ergebnis sieht dann etwa so aus: . [ { \"id\": \"q3jXY4ZYx8\", \"name\": \"My-project\", \"creationTimestamp\": \"2020-12-08T21:55:47Z\", \"status\": \"Active\", \"owners\": [ { \"name\": \"your.email@your-company.de\", \"creationTimestamp\": \"0001-01-01T00:00:00Z\", \"email\": \"your.email@your-company.de\" } ], \"clustersNumber\": 1 } ] . ",
    "url": "/gks/managingprojects/projectserviceaccounts/#zugriff-an-die-api-durch-einen-service-account-token",
    
    "relUrl": "/gks/managingprojects/projectserviceaccounts/#zugriff-an-die-api-durch-einen-service-account-token"
  },"84": {
    "doc": "Service Account Tokens verwalten",
    "title": "Service Accounts und Tokens verfolgen",
    "content": "Es können mehrere Service Accounts für ein Projekt erstellt werden. Der Name des SA muss jedoch eindeutig für das Projekt sein. Ein Service Account kann mehrere Tokens mit eindeutigen Namen haben. Der Anzeigename des Service Accounts und Tokens ist eine gute Möglichkeit, um weitere, erklärende Informationen - wie zum Beispiel den Zweck des SAs - zu erfassen. ",
    "url": "/gks/managingprojects/projectserviceaccounts/#service-accounts-und-tokens-verfolgen",
    
    "relUrl": "/gks/managingprojects/projectserviceaccounts/#service-accounts-und-tokens-verfolgen"
  },"85": {
    "doc": "Service Account Tokens verwalten",
    "title": "Service Accounts und Tokens verwalten",
    "content": "Es ist möglich, einen Service Account zu löschen und mit dem gleichen Namen neu zu erstellen. Das Gleiche gilt für den Service Account Token. Der Name eines Service Accounts oder Tokens kann nach dem Anlegen geändert werden. Der Service Account Token ist nur während des Erstellens des Users sichtbar. Der Benutzer kann einen Token erneuern, aber der vorherige Token wird dabei ungültig. ",
    "url": "/gks/managingprojects/projectserviceaccounts/#service-accounts-und-tokens-verwalten",
    
    "relUrl": "/gks/managingprojects/projectserviceaccounts/#service-accounts-und-tokens-verwalten"
  },"86": {
    "doc": "Service Account Tokens verwalten",
    "title": "Service Account Tokens verwalten",
    "content": " ",
    "url": "/gks/managingprojects/projectserviceaccounts/",
    
    "relUrl": "/gks/managingprojects/projectserviceaccounts/"
  },"87": {
    "doc": "Cluster Lebenszyklus",
    "title": "Cluster Lebenszyklus",
    "content": "Das Verwalten von GKS-basierten Kubernetes-Clustern wird weitestgehend von der GKS-Plattform selbst übernommen. Jedoch gibt es einige Aufgaben im Lebenszyklus eines Clusters, die manuell gemacht werden müssen. Neben selbstverständlichen Dingen wie das Erstellen oder Löschen eines nicht mehr benötigten Clusters zählen dazu auch gewisse Cluster-Updates, wie zum Beispiel die Aktualisierung der Kubernetes-Version. Da solche Updates den rollierenden Neustart der Worker-Nodes beinhalten, müssen diese (zum Beispiel in einem Wartungsfenster Ihrer Wahl) manuell vorgenommen bzw. gestartet werden. ",
    "url": "/gks/clusterlifecycle/#cluster-lebenszyklus",
    
    "relUrl": "/gks/clusterlifecycle/#cluster-lebenszyklus"
  },"88": {
    "doc": "Cluster Lebenszyklus",
    "title": "Weiterführende Themen",
    "content": ". | Einen Cluster anlegen | Cluster Templates | Einen Cluster aktualisieren/Kubernetes-Updates | CNI Auswahl | CNI Updates | Anbindung der Controlplane | Openstack-Credentials ändern | Einen Cluster löschen | Deprecation Policy | Migration Container Runtime Engine | . ",
    "url": "/gks/clusterlifecycle/#weiterf%C3%BChrende-themen",
    
    "relUrl": "/gks/clusterlifecycle/#weiterführende-themen"
  },"89": {
    "doc": "Cluster Lebenszyklus",
    "title": "Cluster Lebenszyklus",
    "content": " ",
    "url": "/gks/clusterlifecycle/",
    
    "relUrl": "/gks/clusterlifecycle/"
  },"90": {
    "doc": "Einen Cluster anlegen",
    "title": "Einen Cluster anlegen",
    "content": "Um einen Cluster in GKS anzulegen, benötigen Sie nur ein paar Klicks. Dazu benötigen Sie ein Projekt. Falls Sie noch kein Projekt erstellt haben, erstellen Sie zuerst ein Projekt. Um einen Cluster anzulegen, klicken Sie im gewünschten Projekt oben rechts auf Create Cluster. Jetzt öffnet sich die erste Seite für den Prozess, einen Cluster anzulegen. Wählen Sie den Provider openstack. Wählen Sie anschließend eine der drei Verfügbarkeitszonen aus, in diesem Beispiel nehmen Sie IX2: . Im nächsten Schritt konfigurieren Sie die Cluster-Details. In dem Beispiel nennen Sie den Cluster first-system und wählen die gewünschte Kubernetes-Version aus. Für den gelegentlichen SSH-Zugriff auf Worker-Nodes können Sie optional einen öffentlichen SSH-Schlüssel hinterlegen. Zum Hinzufügen eines SSH-Keys klicken Sie auf Add SSH Key. In dem sich öffnenden Dialog können Sie Ihren SSH Public Key eintragen und ihm einen passenden Namen geben. Damit GKS in der OpenStack-Infrastruktur die notwendigen Ressourcen erzeugen kann, geben Sie im nächsten Schritt Ihre Zugangsdaten ein. Danach wird der Inhalt im Feld Project automatisch aktualisiert und Sie können in der Dropdown-Liste Ihr gewünschtes OpenStack Projekt auswählen. Mit dem Hinzufügen der Credentials und dem Auswählen des OpenStack-Projekts haben Sie alle notwendigen Eingaben getätigt und Sie können mit dem nächsten Schritt fortfahren. Wenn Sie das tun, wird automatisch ein eigenes Netzwerk, Subnetz sowie eine Security Gruppe für das neue Cluster erstellt. Sie können jedoch auch ein existierendes Netzwerk verwenden, um den Cluster zu erstellen. Dazu müssen Sie das Netzwerk und das Subnetz auswählen. Diese müssen allerdings mit einem Router verbunden sein. In unserer OpenStack Dokumentation ist beschrieben, wie Sie einen Router erstellen und mit einem Netzwerk verbinden können. Im nächsten Schritt definieren Sie, wie viele und welche virtuellen Maschinen als Worker-Nodes im Cluster verfügbar sein sollen. Zuerst geben Sie dem sogenannten Machine Deployment einen Namen. Für Ihren Testcluster nutzen Sie dazu den Namensgenerator. Danach spezifizieren Sie die Replicas (Anzahl der Worker-Nodes im Kubernetes-Cluster) und den Flavor (den Maschinentyp), welcher im Wesentlichen die Anzahl der verfügbaren CPU-Kerne und des RAMs bestimmt. Wählen Sie Flatcar als Betriebssystem für die Worker-Nodes. Über einen Klick auf Next gelangen Sie zum letzten Schritt, wo Sie noch einmal alle Einstellungen verifizieren und mit Create Cluster die Cluster-Erstellung starten können. Nun wird das Cluster erstellt. Um auf die Informationen zugreifen zu können, müssen Sie wieder zur Cluster-Übersicht des Projektes gehen und dort Ihren Cluster auswählen. Nach der Auswahl Ihres Clusters kommen Sie auf die Seite mit allen Cluster-Details. ",
    "url": "/gks/clusterlifecycle/creatingacluster/#einen-cluster-anlegen",
    
    "relUrl": "/gks/clusterlifecycle/creatingacluster/#einen-cluster-anlegen"
  },"91": {
    "doc": "Einen Cluster anlegen",
    "title": "Zusammenfassung",
    "content": "Herzlichen Glückwunsch! Sie haben folgende Schritte erfolgreich durchgeführt und gelernt: . | Was ist ein GKS Cluster? | Wie erstellt man einen GKS Cluster? | . In den folgenden Abschnitten finden Sie Beispiele für die Verwendung von Clustern. ",
    "url": "/gks/clusterlifecycle/creatingacluster/#zusammenfassung",
    
    "relUrl": "/gks/clusterlifecycle/creatingacluster/#zusammenfassung"
  },"92": {
    "doc": "Einen Cluster anlegen",
    "title": "Einen Cluster anlegen",
    "content": " ",
    "url": "/gks/clusterlifecycle/creatingacluster/",
    
    "relUrl": "/gks/clusterlifecycle/creatingacluster/"
  },"93": {
    "doc": "Cluster Templates",
    "title": "Cluster Templates",
    "content": " ",
    "url": "/gks/clusterlifecycle/clustertemplates/#cluster-templates",
    
    "relUrl": "/gks/clusterlifecycle/clustertemplates/#cluster-templates"
  },"94": {
    "doc": "Cluster Templates",
    "title": "Was sind Cluster Templates?",
    "content": "Cluster Templates sind Vorlagen, die eine schnelle und einheitliche Erstellung von Kubernetes Clustern ermöglichen. Mit Cluster Templates können Sie Cluster mit wenigen Klicks anlegen, ohne dass Sie Einstellungen wie z.B. Zugangsdaten, Netzwerkeinstellungen und Verfügbarkeitszonen jedes Mal neu eingeben müssen. ",
    "url": "/gks/clusterlifecycle/clustertemplates/#was-sind-cluster-templates",
    
    "relUrl": "/gks/clusterlifecycle/clustertemplates/#was-sind-cluster-templates"
  },"95": {
    "doc": "Cluster Templates",
    "title": "Erstellung von Cluster Templates",
    "content": "Zur Erstellung eines Cluster Templates wählen Sie in der Sidebar den Menüpunkt Cluster Templates und betätigen anschließend die Schaltfläche Create Cluster. Daraufhin öffnet sich der aus dem Abschnitt Einen Cluster anlegen bekannte Cluster-Erstellungsprozess. Geben Sie hier alle Daten ein, die für die Cluster-Erstellung benötigt werden. Klicken Sie im letzten Schritt Summary jedoch nicht auf Create Cluster, sondern auf Save Cluster Template. Nun öffnet sich der Dialog Save Cluster Template. Hier können Sie den gewünschten Namen und Speicherbereich festlegen. Templates können in 2 verschiedenen Bereichen gespeichert werden: . | Auf Projekt-Ebene: Alle User des Projekts können das Template nutzen. | Auf User-Ebene: Das Template kann in allen Projekten genutzt werden, in denen der User Schreibrechte besitzt. Andere User können das Template nicht nutzen. | . Die Auswahl wird durch die Schaltfläche Save Cluster Template bestätigt und das Cluster Template wurde damit gespeichert. ",
    "url": "/gks/clusterlifecycle/clustertemplates/#erstellung-von-cluster-templates",
    
    "relUrl": "/gks/clusterlifecycle/clustertemplates/#erstellung-von-cluster-templates"
  },"96": {
    "doc": "Cluster Templates",
    "title": "Cluster aus Templates erstellen",
    "content": "Mit dem gerade angelegtem Template können Sie nun einfach neue Cluster erstellen. Wählen Sie in der Sidebar den Menüpunkt Cluster Templates aus. Wählen Sie das gewünschte Template aus und betätigen Sie die Schaltfläche Create Cluster from Template. Daraufhin werden Sie gefragt, wie viele Cluster Sie aus diesem Template erstellen wollen. Geben Sie eine Zahl ein und bestätigen Sie die Auswahl mit Create Clusters. Der bzw. die Cluster werden nun angelegt. Cluster, die mit einem Cluster Template erstellt wurden, sind an dem template-instance-id Label erkennbar. Hinweis: Einen weiteren Weg, Cluster aus Templates zu erstellen, finden Sie im Cluster Menü mit der Schaltfläche Create Clusters from Template. Die Funktion unterscheidet sich nicht von der gerade gezeigten und ist nur eine Abkürzung in der Oberfläche. ",
    "url": "/gks/clusterlifecycle/clustertemplates/#cluster-aus-templates-erstellen",
    
    "relUrl": "/gks/clusterlifecycle/clustertemplates/#cluster-aus-templates-erstellen"
  },"97": {
    "doc": "Cluster Templates",
    "title": "Cluster Templates löschen",
    "content": "Um Cluster Templates wieder zu löschen, wählen Sie in der Sidebar den Menüpunkt Cluster Templates und das entsprechende Template aus. Zum Löschen betätigen Sie die Schaltfläche Delete Cluster Template. ",
    "url": "/gks/clusterlifecycle/clustertemplates/#cluster-templates-l%C3%B6schen",
    
    "relUrl": "/gks/clusterlifecycle/clustertemplates/#cluster-templates-löschen"
  },"98": {
    "doc": "Cluster Templates",
    "title": "Cluster Templates",
    "content": " ",
    "url": "/gks/clusterlifecycle/clustertemplates/",
    
    "relUrl": "/gks/clusterlifecycle/clustertemplates/"
  },"99": {
    "doc": "Kubernetes Updates",
    "title": "Kubernetes Updates",
    "content": "Einen Kubernetes Cluster aktuell zu halten dient mehreren Zwecken. Einerseits gewährleistet es die Sicherheit des Clusters. Hinzu kommt, dass Kubernetes auch neue Features mit jedem Release mitbringt. Um hier auf der sicheren Seite zu sein, empfiehlt es sich, regelmäßig Kubernetes-Updates zu installieren. In besonders kritischen Fällen aktualisieren wir die Cluster API automatisch auf die letzte Minor-Version, um auch unsere eigene Infrastruktur sicher zu halten. In diesem Fall können Sie den nächsten Abschnitt “Der Cluster” überspringen. Sie müssen jedoch noch die (Worker-)Nodes selbst updaten. Bevor Sie einen Cluster upgraden, lesen Sie das Changelog der Ziel-Kubernetes-Version, und machen Sie sich mit den bevorstehenden Änderungen vertraut. Das Tool Kubepug kann bei der Vorbereitung eines Updates helfen. Es überprüft die Kompatibilität der benutzten Ressourcen zur neuen Kubernetes-Version und warnt vor veralteten (deprecated) oder nicht mehr unterstützten Versionen. ",
    "url": "/gks/clusterlifecycle/upgradingacluster/#kubernetes-updates",
    
    "relUrl": "/gks/clusterlifecycle/upgradingacluster/#kubernetes-updates"
  },"100": {
    "doc": "Kubernetes Updates",
    "title": "Der Cluster",
    "content": "In Kubernetes teilt sich die Infrastruktur in Master- (= Kubernetes Controlplane) und (Worker)-Nodes. Bei GKS Clustern wird der Master verwaltet. Da mehrere Versionen für den Master angeboten werden, kann man im GKS Web Interface die Version auswählen. Ein Update des Masters ist mit ein paar Mausklicks erledigt. Als Erstes gehen Sie zu dem Cluster, für den Sie ein Update durchführen wollen. Klicken Sie in das Feld Control Plane und wählen Sie eine neue Version für den Master aus. Wir empfehlen, dabei direkt Upgrade Machine Deployments zu aktivieren, um die Worker-Nodes gleich mit upzugraden. Jetzt aktualisiert GKS den Master (und ggf. auch die Nodes) selbstständig und Sie sind mit diesem Schritt fertig. ",
    "url": "/gks/clusterlifecycle/upgradingacluster/#der-cluster",
    
    "relUrl": "/gks/clusterlifecycle/upgradingacluster/#der-cluster"
  },"101": {
    "doc": "Kubernetes Updates",
    "title": "Die Nodes",
    "content": "Sofern der Master ohne Machine Deployments aktualisiert wurde, oder ein Upgrade der GKS-Plattform selbst zu einem impliziten Upgrade des Master geführt hat (i.d.R. nur Patchlevel), müssen Sie nun noch manuell die Nodes updaten. Auch hier hilft Ihnen das GKS Web Interface. Beachten Sie jedoch, dass bei einem Update neue Nodes erzeugt und die alten Nodes gelöscht werden. Dabei werden zwangsweise auch alle Pods neu gestartet. Im ersten Schritt klicken Sie auf das Machine Deployment. Klicken Sie auf das Editier- (Bleistift-)Symbol, um die Update-Ansicht zu öffnen. Bei der kubelet Version wählen Sie die Version aus, die auch bei Ihrem Cluster Controlplane konfiguriert ist (in diesem Beispiel 1.23.6) und bestätigen Sie mit Save Changes. Nun aktualisiert GKS automatisch die Node Group auf die neue Version und Kubernetes sorgt dafür, dass Ihre Applikationen neu auf die aktualisierten Nodes verteilt werden. ",
    "url": "/gks/clusterlifecycle/upgradingacluster/#die-nodes",
    
    "relUrl": "/gks/clusterlifecycle/upgradingacluster/#die-nodes"
  },"102": {
    "doc": "Kubernetes Updates",
    "title": "Zwei-Node Cluster",
    "content": "Aufzupassen ist hier bei Clustern mit zwei Nodes. GKS nutzt ein rollierendes Update als Update-Strategie. Hierbei wird immer ein Node nach dem anderen getauscht. Bei einem Cluster mit zwei oder weniger Nodes bedeutet dies, dass der zuerst aktualisierte Node komplett geplant wird, noch bevor der zweite fertig ist. Als Lösung gibt es ein recht einfaches Bash-Script, welches in einem Namespace alle Pods noch einmal neu erstellen lässt: https://github.com/truongnh1992/playing-with-istio/blob/master/upgrade-sidecar.sh . Dieses nutzen wir, nachdem das Cluster komplett aktualisiert wurde, in einem Terminal mit konfiguriertem kubectl. Im Kapitel Mit einem Cluster verbinden zeigen wir, wie Sie Ihr kubectl mit Ihrem Cluster verbinden können. curl -o upgrade-node.sh https://raw.githubusercontent.com/truongnh1992/playing-with-istio/master/upgrade-sidecar.sh chmod +x upgrade-node.sh echo -e \"#\\!/bin/bash\\n$(cat upgrade-node.sh)\" &gt; upgrade-node.sh . Nun müssen Sie nur noch dieses Script auf alle Ihre Namespaces anwenden. kubectl get namespace NAME STATUS AGE default Active 36m kube-node-lease Active 36m kube-public Active 36m kube-system Active 36m # Wir interessieren uns für default Namespace: ./upgrade-node.sh default Refreshing pods in all Deployments . Nun sind alle Pods sauber auf Ihre Nodes verteilt worden. ",
    "url": "/gks/clusterlifecycle/upgradingacluster/#zwei-node-cluster",
    
    "relUrl": "/gks/clusterlifecycle/upgradingacluster/#zwei-node-cluster"
  },"103": {
    "doc": "Kubernetes Updates",
    "title": "Kubernetes Updates",
    "content": " ",
    "url": "/gks/clusterlifecycle/upgradingacluster/",
    
    "relUrl": "/gks/clusterlifecycle/upgradingacluster/"
  },"104": {
    "doc": "CNI Auswahl",
    "title": "CNI Auswahl",
    "content": "Diese Seite gibt einen Überblick darüber, was eine CNI ist, nach welchen Kriterien man eine Auswahl trifft und wie man sie im Clustererstellungsprozess auswählt. ",
    "url": "/gks/clusterlifecycle/cnichoices/",
    
    "relUrl": "/gks/clusterlifecycle/cnichoices/"
  },"105": {
    "doc": "CNI Auswahl",
    "title": "Was ist eine CNI und wofür wird sie gebraucht?",
    "content": "CNI ist eine Abkürzung für Container Network Interface. Vereinfacht erklärt, ist es eine Beschreibung der in Kubernetes benötigten Netzwerkfunktionalität, um die Kommunikation zwischen Pods und Nodes zu realisieren. Da die Netzwerkumgebungen, in denen Kubernetes läuft, sehr verschieden sind, haben sich die Entwickler von Kubernetes darauf geeinigt, sich auf eine standardisierte Schnittstellenbeschreibung der benötigten Netzwerkfunktionalität zu beschränken und anderen die Implementierung zu überlassen. Dieser Schritt resultierte in der Entstehung vieler verschiedener CNIs mit verschiedenen Funktionen und für verschiedene Einsatzzwecke. Unsere Kubernetes Plattform unterstützte lange Zeit nur eine CNI und hat diese fest als Standard definiert, ohne eine Auswahlmöglichkeit zu bieten. Dies hat sich kürzlich geändert. Wir unterstützen nun zwei verschiedene CNIs, aus denen der Benutzer eine wählen kann, die am Besten zu seinen Bedürfnissen passt. ",
    "url": "/gks/clusterlifecycle/cnichoices/#was-ist-eine-cni-und-wof%C3%BCr-wird-sie-gebraucht",
    
    "relUrl": "/gks/clusterlifecycle/cnichoices/#was-ist-eine-cni-und-wofür-wird-sie-gebraucht"
  },"106": {
    "doc": "CNI Auswahl",
    "title": "Canal",
    "content": "Unser langjähriger Standard in Sachen CNI war bislang Canal. Es handelt sich dabei um die Kombination der beiden CNIs Flannel und Calico. Flannel realisiert dabei die Netzwerkfunktionalität zwischen den Pods, während calico sich um die network policies kümmert. Die CNI Canal gibt es schon sehr lange und besitzt daher auch die nötige Produktionsreife. Canal unterstützt sowohl den proxy-Modus iptables als auch ipvs. Falls Sie sich unsicher sind, welches CNI sie wählen sollen, dann wählen sie Canal. ",
    "url": "/gks/clusterlifecycle/cnichoices/#canal",
    
    "relUrl": "/gks/clusterlifecycle/cnichoices/#canal"
  },"107": {
    "doc": "CNI Auswahl",
    "title": "Cilium",
    "content": "Cilium ist relativ neu auf dem CNI Markt. Es setzt, im Gegensatz zu Canal, für die Konfiguration der Kommunikation zwischen den Pods intern auf neuere Technologien wie beispielsweise eBPF. Cilium bietet neben besseren health-checks außerdem mit Hubble ein Add-on zur genaueren Analyse von Netzwerkverkehr innerhalb des Kubernetes Clusters an. Zudem bietet es ebenfalls Unterstützung für feingranulares Filtern des eingehenden und ausgehenden Netzwerkverkehrs sowie des Verkehrs innerhalb des Clusters. Um alle fortgeschrittenen Funktionen von Cilium nutzen zu können ist es notwendig den proxy Modus eBPF auszuwählen. Dieser erscheint als Wahlmöglichkeit sobald Cilium als CNI gewählt wurde. Außerdem muss das Betriebssystemimage des Kubernetes Workers eine relativ aktuelle Kernelversion anbieten, welches auf unseren Flatcar images immer gegeben ist. An dem Cilium Projekt wird im Moment rege weiterentwickelt so dass regelmäßig neue Features herausgebracht und Fehler behoben werden. Wenn Sie an einem tieferen Verständnis über die Netzwerkflüsse in ihrem Kubernetes Cluster interessiert sind, dann wählen Sie Cilium als CNI. ",
    "url": "/gks/clusterlifecycle/cnichoices/#cilium",
    
    "relUrl": "/gks/clusterlifecycle/cnichoices/#cilium"
  },"108": {
    "doc": "CNI Auswahl",
    "title": "Installation der CNI",
    "content": "Beim Erstellungsprozess Ihres Kubernetes Clusters in Schritt zwei können Sie unter dem Punkt “Network Configuration” Ihre CNI auswählen. Dort sind auch weitere Einstellungen wie proxy-Modus und weiter unten auch der Controlplane Konnektor Konnectivity möglich. Bitte beachten Sie, dass die Wahl von eBPF als proxy-Modus erst zur Verfügung steht, wenn Cilium als CNI ausgewählt wurde. Weitere Informationen zum Controlplane Konnektor Konnectivity finden sie auf einer eigenen Seite hier. ",
    "url": "/gks/clusterlifecycle/cnichoices/#installation-der-cni",
    
    "relUrl": "/gks/clusterlifecycle/cnichoices/#installation-der-cni"
  },"109": {
    "doc": "CNI Auswahl",
    "title": "Installation des Hubble Add-ons",
    "content": "Wenn Cilium als CNI verwendet wurde, dann hat man die Möglichkeit sich den Netzwerkverkehr des Kubernetes Cluster visualisieren zu lassen. Dazu muss die Komponente Hubble über den Addons-Reiter installiert werden, nachdem die Clustererstellung erfolgreich beendet wurde. ",
    "url": "/gks/clusterlifecycle/cnichoices/#installation-des-hubble-add-ons",
    
    "relUrl": "/gks/clusterlifecycle/cnichoices/#installation-des-hubble-add-ons"
  },"110": {
    "doc": "CNI Auswahl",
    "title": "Warnung",
    "content": "Die Auswahl der CNI kann nur zum Zeitpunkt der Erstellung des Kubernetes Clusters erfolgen. Ein nachträgliches Ändern der CNI ist zwar technisch möglich, wird aber von unserer Plattform nicht unterstützt. Im Falle eines manuellen Änderns des CNI im Nachhinein ist die Wahrscheinlichkeit sehr hoch, dass es bei Updates der Kubernetes Version zu Komplikationen kommt. Wir raten daher dringend von der nachträglichen manuellen Einflussnahme ab. Falls Sie generell vorhaben, Änderungen an der CNI vorzunehmen, erstellen Sie den Cluster ohne CNI und installieren und warten sie diese danach selber. ",
    "url": "/gks/clusterlifecycle/cnichoices/#warnung",
    
    "relUrl": "/gks/clusterlifecycle/cnichoices/#warnung"
  },"111": {
    "doc": "CNI Auswahl",
    "title": "Weiterführende Themen",
    "content": ". | Flannel github page | Canal github page | Canal installation docs | Cilium Documentation | . ",
    "url": "/gks/clusterlifecycle/cnichoices/#weiterf%C3%BChrende-themen",
    
    "relUrl": "/gks/clusterlifecycle/cnichoices/#weiterführende-themen"
  },"112": {
    "doc": "CNI Updates",
    "title": "CNI Updates",
    "content": "Auf der Clusterdetailseite kann man sehen, ob ein Update für das CNI verfügbar ist. Wenn ein grüner Pfeil in der CNI Plugin Box angezeigt wird, steht ein Update zur Verfügung. Um das Update zu beginnen, müssen Sie auf die Box klicken. In einem Popup werden die verfügbaren Versionen angezeigt. Mit Change CNI Version wird der Updateprozess im Cluster begonnen. Das Update läuft im Hintergrund. Während die Netzwerk-Pods im Cluster neu starten, kann es kurz zu Paketverlusten auf den einzelnen Workern kommen. Es wird immer nur ein Worker gleichzeitig aktualisiert, daher sollte es nicht zu Ausfällen in Deployments mit mehr als einem Replica kommen. Nach dem Update, wenn alle Netzwerk-Pods im Daemonset restarted wurden, ist der Cluster wieder voll einsatzfähig. Sollte es wider Erwarten zu Netzwerkproblemen kommen, können die folgenden Schritte Abhilfe schaffen: . | Ein erneuter Rolling Restart der Canal Pods: kubectl rollout restart daemonset --namespace kube-system canal | Ein Rolling Recreate der Worker-Nodes kann auch helfen, dies kann im Web Interface gestartet werden. | . CNI Updates werden immer nur auf die nächste Minorversion unterstützt. Sollte mehr als eine neue Version verfügbar sein, muss eine nach der anderen aktualisiert werden. ",
    "url": "/gks/clusterlifecycle/upgradingcni/",
    
    "relUrl": "/gks/clusterlifecycle/upgradingcni/"
  },"113": {
    "doc": "Controlplane Konnektor",
    "title": "Controlplane Konnektor",
    "content": "Diese Seite bietet eine Übersicht darüber, wie die Controlplane eines GKS Kubernetes Clusters mit seinen Worker-Nodes kommuniziert. ",
    "url": "/gks/clusterlifecycle/controlplaneconnector/",
    
    "relUrl": "/gks/clusterlifecycle/controlplaneconnector/"
  },"114": {
    "doc": "Controlplane Konnektor",
    "title": "Cluster Aufbau",
    "content": "Die GKS Plattform stellt verwaltete Kubernetes Dienste für Endkunden zur Verfügung. Aus Gründen der Verfügbarkeit, Sicherheit und Stabilität wird die Controlplane getrennt von den Worker-Nodes betrieben. Der Endkunde hat die vollständige Kontrolle über die Worker-Nodes (bis hinab auf Betriebssystemebene der Kubernetes Worker), welche in deren eigenem Openstack Tenant laufen. Die Controlplane hingegen wird vom Service-Erbringer in dessen eigenem dedizierten Openstack-Tenant betrieben. Trotz dieser Trennung muss weiterhin gewährleistet sein, dass Controlplane und Worker miteinander kommunizieren können. Kommunikation initiiert von den Kubernetes Workern . Diese Richtung der Kommunikation ist einfach realisiert, da die Controlplane ihre Dienste über den Kubernetes API Server anbietet, der über eine öffentliche IPv4 Adresse erreichbar ist, die den Kubernetes Workern bekannt ist. Die Netzwerkpakete verlassen zum Initiieren der Verbindung das private Openstack-Subnetz über den egress-Router des Subnetzes in Richtung Internet. Anschließend betreten sie über einen exponierten Loadbalancer das private Subnetz des Openstack-Tenants vom Servicebetreiber. Allerdings ist es manchmal auch notwendig, dass der Kubernetes API Server die Verbindung zu Diensten auf den Worker-Nodes initiiert (hauptsächlich zu den kubelets), vor allem wenn Befehle wie kubectl port-forward, kubectl exec oder kubectl logs ausgeführt werden sollen. Die Worker-Nodes sind nicht über einen öffentlichen Endpunkt erreichbar (was aus Sicherheitsperspektive sehr sinnvoll ist). Kommunikation initiiert von der Controlplane . Der Trick ist, bereits von Anfang an, einen VPN-Tunnel von den Worker-Nodes aufzubauen und darüber dafür zu sorgen, dass die Controlplane die Worker-Nodes jederzeit erreichen kann. Die Konfiguration dazu entsteht bereits bei der Erstellung des Clusters. Die bisherige von uns benutzte Lösung implementierte diesen Tunnel mittels der openVPN Software. Diese Lösung hatte jedoch einige Nachteile, so dass ein anderes speziell dafür entwickeltes Tool dessen Aufgabe übernahm, die sogenannte Konnectivity. Es ermöglicht einen deutlich stabileren Aufbau mit einfacheren Mitteln und wird von der Kubernetes Community entwickelt und gepflegt. ",
    "url": "/gks/clusterlifecycle/controlplaneconnector/#cluster-aufbau",
    
    "relUrl": "/gks/clusterlifecycle/controlplaneconnector/#cluster-aufbau"
  },"115": {
    "doc": "Controlplane Konnektor",
    "title": "Installation/Migration",
    "content": "Wenn ein neuer Kubernetes Cluster auf der GKS Plattform erstellt wird, enthält er bereits automatisch Konnectivity. Bestehende Cluster müssen einmalig im GKS Dashboard unter dem Menüpunkt “Edit cluster” einen Haken in die Box für Konnectivity setzen und speichern. Danach läuft ein Automatismus los und führt nahtlos die Migration von openVPN hin zu Konnectivity vor. ",
    "url": "/gks/clusterlifecycle/controlplaneconnector/#installationmigration",
    
    "relUrl": "/gks/clusterlifecycle/controlplaneconnector/#installationmigration"
  },"116": {
    "doc": "Controlplane Konnektor",
    "title": "Weiterführende Themen",
    "content": ". | konnectivity | Github repo für die Specifization | . ",
    "url": "/gks/clusterlifecycle/controlplaneconnector/#weiterf%C3%BChrende-themen",
    
    "relUrl": "/gks/clusterlifecycle/controlplaneconnector/#weiterführende-themen"
  },"117": {
    "doc": "Openstack Applikation Credentials benutzen",
    "title": "Openstack Application Credentials benutzen",
    "content": "Während der Erstellung eines GKS-Clusters ist es notwendig sich bei der Openstack API anzumelden. Dies wurde in der bisherigen Dokumentation mit dem Benutzenamen und dem Passwort des Benutzers des Openstack Tenants beschrieben. Da jedoch sowohl der openstack-cloud-controller als auch der machine-controller beide Zugriff auf die Openstack-Ressourcen benötigen (Provisionieren von Compute-Instanzen für neue Kubernetes-worker Knoten oder Provisionierung von Netzwerk-Loadbalancern wenn Kubernetes Services des Typ Loadbalancer eingerichtet werden sollen) ist es notwendig die bei der Erstellung des Clusters verwendeten Benutzernamen und Passwort im GKS-cluster selbst als Kubernetes-Secret zu hinterlegen. Dies resultiert darin, dass alle Benutzer mit der Rolle ClusterAdmin Rechte haben dieses Secret zu lesen. Bei erhöhtem Sicherheitsbedarf ist dieser Zustand nicht wünschenswert. Es gibt eine Openstack-Funktionalität welche dieses verhindert: Openstack Application Credentials. Openstack erlaubt das Erzeugen weiterer Applikationsuser, welches ihr eigenes Passwort haben und dessen Rechte eingeschränkt werden können. Eine weitere Eigenschaft dieser Applikationsuser ist, dass sie nicht Rechte auf den gesamten Openstack Tenant haben können, sondern ihre Reichweite auf einzelne Projekte innerhalb des Openstack Tenants eingeschränkt sind. Dies ermöglicht die kontrollierte Separierung von Umgebungen. Es wäre zum Beispiel möglich innerhalb eines Openstack Tenants drei Projekte dev, test und prod zu erstellen und für jedes Projekt einen Applikationsuser anzulegen und nur diesen bei der Erstellung der jeweiligen GKS-Cluster zu benutzen. Dies würde dem Benutzer das Managen der Openstack-Ressourcen aller Umgebungen (Projekte) ermöglichen und trotzdem die größtmögliche Isolation zwischen den Umgebungen gewährleisten. ",
    "url": "/gks/clusterlifecycle/applicationcredentials/#openstack-application-credentials-benutzen",
    
    "relUrl": "/gks/clusterlifecycle/applicationcredentials/#openstack-application-credentials-benutzen"
  },"118": {
    "doc": "Openstack Applikation Credentials benutzen",
    "title": "Openstack Applikations-User anlegen",
    "content": "Eine detaillierte Beschreibung dazu was Openstack Applikations-User genau sind und wie sie angelegt werden findet man in unserer Openstack-Dokumentation hier. ",
    "url": "/gks/clusterlifecycle/applicationcredentials/#openstack-applikations-user-anlegen",
    
    "relUrl": "/gks/clusterlifecycle/applicationcredentials/#openstack-applikations-user-anlegen"
  },"119": {
    "doc": "Openstack Applikation Credentials benutzen",
    "title": "Erstellen von GKS-Clustern mit Openstack Applikations-Usern",
    "content": "Beim Erstellen eines GKS-Clusters wird in Schritt 3 (Settings) nach Openstack Authentifizierungsdaten gefragt. Standardmäßig ist hierbei die HTML-Form Basic Credentials ausgewählt in der Benutzernamen, Passwort, Projektnamen und ProjektID für Openstack eingegeben werden soll. Es gibt jedoch einen weiteren Tab mit dem Namen Application Credentials wo nur noch in zwei Feldern die ID und das Secret desselben erfragt wird. Projektname und -ID entfallen da Application Credentials immer einem Projekt eindeutig zugeordnet sind und sich dies auch nicht im Nachhinein ändern lässt. Nach der Eingabe der Application Credentials beginnt im Hintergrund die Erstellung des Kubernetes Clusters wie bereits zuvor in der Dokumentation beschrieben. Zuvor gibt es nochmal die Möglichkeit die Eingabe auf der Zusammenfassungsseite in Schritt 5 zu überprüfen. Hier findet sich nun nur noch die applicationID anstelle von Domäne, Benutzer- und Projektname des Openstack Projekts. ",
    "url": "/gks/clusterlifecycle/applicationcredentials/#erstellen-von-gks-clustern-mit-openstack-applikations-usern",
    
    "relUrl": "/gks/clusterlifecycle/applicationcredentials/#erstellen-von-gks-clustern-mit-openstack-applikations-usern"
  },"120": {
    "doc": "Openstack Applikation Credentials benutzen",
    "title": "Austauschen von Openstack Applikations-Usern in bestehenden GKS-Clustern",
    "content": "Eine weitere Besonderheit von Openstack Applikationsusern ist, dass man Sie mit einem Ablaufdatum versehen kann. Dies erhöht die Sicherheit des Setups und garantiert dass bei unentdeckter Enthüllung des Passworts dieses trotzdem nur eine begrenzte Zeit unerlaubt genutzt werden kann. In diesem Fall sollte der Openstack-Tenant Besitzer rechtzeitig einen neuen Applikationsuser anlegen und den verfallenden damit ersetzen. Hierzu wird die Clusteransicht des betreffenden Clusters im GKS-Dashboard geöffnet. Um den Applikationsuser im laufenden Betrieb reibungslos auszutauschen drückt der GKS-Benutzer auf den Knopf mit den drei vertikalen Punkten in der oberen rechten Ecke der Clusteransicht und wählt den Menüpunkt Edit Provider aus. Im nächsten Schritt wird ein Dialog zum austauschen der Openstack Authentifizierungsdaten angezeigt. Wie zuvor gibt es auch hier zwei Tabs, wobei standardmäßig die User Credentials ausgewählt sind. Auch hier wechselt der Benutzer auf den Application Credentials Tab und bekommt nun die Möglichkeit eine neue ApplikationsID und -Passwort einzugeben. Nach der Eingabe der korrekten Werte für ID und Passwort müssen die Eingaben durch den Druck auf das Save Settings Feld bestätigt werden. Danach startet im Hintergrund der automatische Austauschprozess für die neuen Zugangsdaten. ",
    "url": "/gks/clusterlifecycle/applicationcredentials/#austauschen-von-openstack-applikations-usern-in-bestehenden-gks-clustern",
    
    "relUrl": "/gks/clusterlifecycle/applicationcredentials/#austauschen-von-openstack-applikations-usern-in-bestehenden-gks-clustern"
  },"121": {
    "doc": "Openstack Applikation Credentials benutzen",
    "title": "Glossar",
    "content": "Auf dieser Seite werden die Worte Applikationsuser, Applikationsbenutzer und Application Credentials im Kontext von Openstack synonym verwendet. ",
    "url": "/gks/clusterlifecycle/applicationcredentials/#glossar",
    
    "relUrl": "/gks/clusterlifecycle/applicationcredentials/#glossar"
  },"122": {
    "doc": "Openstack Applikation Credentials benutzen",
    "title": "Openstack Applikation Credentials benutzen",
    "content": " ",
    "url": "/gks/clusterlifecycle/applicationcredentials/",
    
    "relUrl": "/gks/clusterlifecycle/applicationcredentials/"
  },"123": {
    "doc": "Openstack Credentials ändern",
    "title": "Openstack Credentials ändern",
    "content": "Es ist möglich, die Credentials für einen Cluster zu ändern. Das wird vor allem nach dem Ändern eines Passwortes, oder beim Wechsel des Users benötigt. ",
    "url": "/gks/clusterlifecycle/openstackcredentials/#openstack-credentials-%C3%A4ndern",
    
    "relUrl": "/gks/clusterlifecycle/openstackcredentials/#openstack-credentials-ändern"
  },"124": {
    "doc": "Openstack Credentials ändern",
    "title": "Credentials ändern",
    "content": "Mit den folgenden einfachen Schritten, können Sie die Credentials ändern: . | Wählen Sie den gewünschten Cluster aus. | Klicken Sie auf die drei Punkte, um das Untermenü zu öffnen. | Wählen Sie Edit Provider. | Ändern Sie die Credentials. | . Kurz nach dem Sie Ihre Eingaben gespeichert haben, erscheint ein Pop-up mit der Bestätigung, dass die Credentials erfolgreich geändert wurden. ",
    "url": "/gks/clusterlifecycle/openstackcredentials/#credentials-%C3%A4ndern",
    
    "relUrl": "/gks/clusterlifecycle/openstackcredentials/#credentials-ändern"
  },"125": {
    "doc": "Openstack Credentials ändern",
    "title": "Openstack Credentials ändern",
    "content": " ",
    "url": "/gks/clusterlifecycle/openstackcredentials/",
    
    "relUrl": "/gks/clusterlifecycle/openstackcredentials/"
  },"126": {
    "doc": "Einen Cluster löschen",
    "title": "Einen Cluster löschen",
    "content": "Einen Cluster in GKS zu löschen ist sehr schnell machbar. Die Voraussetzung dafür ist ein existierender Cluster in einem Projekt. ",
    "url": "/gks/clusterlifecycle/deletingacluster/#einen-cluster-l%C3%B6schen",
    
    "relUrl": "/gks/clusterlifecycle/deletingacluster/#einen-cluster-löschen"
  },"127": {
    "doc": "Einen Cluster löschen",
    "title": "Einen Cluster finden",
    "content": "Um einen Cluster zu löschen, müssen Sie in die Detailansicht des Clusters gehen. Hierfür klicken Sie auf den Eintrag first-system. Für die folgenden Schritte müssen Sie sich den Cluster-Namen merken. Um diesen in die Zwischenablage zu kopieren, klicken Sie auf den Namen. ",
    "url": "/gks/clusterlifecycle/deletingacluster/#einen-cluster-finden",
    
    "relUrl": "/gks/clusterlifecycle/deletingacluster/#einen-cluster-finden"
  },"128": {
    "doc": "Einen Cluster löschen",
    "title": "Einen Cluster löschen",
    "content": "Klicken Sie dazu auf Delete. In dem sich öffnenden Fenster wird als Sicherheitsfrage der Cluster-Name abgefragt. Da Sie diesen vorher schon in die Zwischenablage kopiert haben, müssen Sie diesen nur noch einfügen. Da Sie alles löschen wollen, lassen Sie die beiden Checkboxen angekreuzt. Damit werden auch die Volumes und der Loadbalancer in OpenStack gelöscht. ",
    "url": "/gks/clusterlifecycle/deletingacluster/#einen-cluster-l%C3%B6schen-1",
    
    "relUrl": "/gks/clusterlifecycle/deletingacluster/#einen-cluster-löschen-1"
  },"129": {
    "doc": "Einen Cluster löschen",
    "title": "Zusammenfassung",
    "content": "Herzlichen Glückwunsch! Sie haben folgende Schritte erfolgreich durchgeführt und gelernt: . | Wie lösche ich einen Cluster? | Wie lösche ich auch in OpenStack alle Ressourcen? | . ",
    "url": "/gks/clusterlifecycle/deletingacluster/#zusammenfassung",
    
    "relUrl": "/gks/clusterlifecycle/deletingacluster/#zusammenfassung"
  },"130": {
    "doc": "Einen Cluster löschen",
    "title": "Einen Cluster löschen",
    "content": " ",
    "url": "/gks/clusterlifecycle/deletingacluster/",
    
    "relUrl": "/gks/clusterlifecycle/deletingacluster/"
  },"131": {
    "doc": "Deprecation Policy",
    "title": "Deprecation Policy",
    "content": " ",
    "url": "/gks/clusterlifecycle/deprecationpolicy/#deprecation-policy",
    
    "relUrl": "/gks/clusterlifecycle/deprecationpolicy/#deprecation-policy"
  },"132": {
    "doc": "Deprecation Policy",
    "title": "Deprecation Policy in GKS",
    "content": "Das offizielle Kubernetes Project veröffentlicht ungefähr vier Kubernetes Versionen pro Jahr und markiert die gleiche Anzahl an alten Versionen als veraltet. Bis einschließlich Version 1.18 hatte Kubernetes eine N-2 Supportrichtlinie, sprich die drei neuesten Versionen haben Sicherheits- und Fehlerkorrekturen erhalten. Ab Version v1.19 folgt Kubernetes einer N-3 Supportrichtlinie. Dies bedeutet, dass das Support-Fenster auf ein Jahr verlängert wurde. Eine gute Visualisierung des Zeitraums, für den die einzelnen Versionen unterstützt werden/wurden sehnen Sie auf dem folgenden Bild: . GKS folgt diesem Lebenszyklus, indem fortlaufend neue Versionen eingeführt und ältere aus dem Support genommen werden. Nachdem eine bestimmte Kubernetes-Version das End-of-Life erreicht hat, werden keine Bugfixes oder Sicherheitsupdates mehr veröffentlicht. Daher können wir diese auch nicht mehr unterstützen und müssen sie aus dem Support nehmen. ",
    "url": "/gks/clusterlifecycle/deprecationpolicy/#deprecation-policy-in-gks",
    
    "relUrl": "/gks/clusterlifecycle/deprecationpolicy/#deprecation-policy-in-gks"
  },"133": {
    "doc": "Deprecation Policy",
    "title": "Der Deprecation Prozess",
    "content": "Wenn wir eine Kubernetes-Version aus dem Support nehmen, informieren wir die Kunden im Voraus über die Deprecation (“End-of-Life Ankündigung”), damit sie genügend Zeit haben, das Kubernetes-Upgrade selbst einzuplanen, vorzubereiten und durchzuführen. Zu dem Zeitpunkt des angekündigten End-of-Life Datums wird dann die genannte Kubernetes-Version aus GKS entfernt. Die Liste der unterstützten Kubernetes-Versionen und ihre geplanten End-of-Life Daten finden Sie hier. Eine ausführliche Dokumentation zum Ausführen von Cluster-Upgrades finden Sie hier. Was bedeutet eine End-of-Life Ankündigung für mich? . Wenn für eine bestimmte Kubernetes-Version eine End-of-Life-Ankündigung erfolgt ist, werden Kunden gebeten, ihre Cluster auf eine neuere Version (vorzugsweise die neueste) zu aktualisieren. Was passiert, wenn ich nicht bis zum EOL-Datum aktualisiere? . Wenn ein Kundencluster vor dem Entfernen einer veralteten Kubernetes-Version nicht aktualisiert wird, geschieht zunächst nichts. Mit der entfernten Version können keine neuen Cluster mehr erstellt werden, aber vorhandene Cluster werden weiterhin funktionieren. Kann ich für immer auf einer EOL-Version bleiben? . Nein, da dies in der Zukunft möglicherweise zu schwerwiegenden Sicherheitsproblemen führen könnte. ",
    "url": "/gks/clusterlifecycle/deprecationpolicy/#der-deprecation-prozess",
    
    "relUrl": "/gks/clusterlifecycle/deprecationpolicy/#der-deprecation-prozess"
  },"134": {
    "doc": "Deprecation Policy",
    "title": "GKS Force-Upgrade Richtlinie",
    "content": "Wenn eine Kubernetes-Version das End-of-Life erreicht, müssen wir ihre Unterstützung von GKS entfernen, da sie keine Bugfixes oder Sicherheitsupdates mehr erhält. Ab diesem Zeitpunkt ist es nicht mehr möglich, neue Cluster mit dieser Version zu erstellen. Hinweis: Beachten Sie die folgenden technischen Einschränkungen in Kubernetes: . | Ein Kubernetes-Cluster (bzw. seine Controlplane) kann jeweils um eine Version aktualisiert werden, z. B. von v1.21 → v1.22. | Es ist nicht möglich, mehrere Versionen in einem Schritt zu aktualisieren. | Es ist nicht möglich, einen Cluster downzugraden. | . Für unsere Kunden bedeutet dies Folgendes: Wenn sie ihre Cluster vor dem Entfernen der nächsten EOL-Version nicht aktualisieren, gehen sie das Risiko ein, bei dem Entfernen der nächsten Version kein Upgrade durchführen zu können. Dies würde ein ernstes Problem bedeuten, da ihre einzige Alternative darin besteht, einen neuen Cluster zu erstellen und den Workload von dem alten zu migrieren, da ein Upgrade nicht möglich wäre (da hierfür zwei Versionen in einem Schritt aktualisiert werden müssten). Um solche Situationen zu vermeiden, müssen wir für Cluster, die mit einer bereits entfernten Kubernetes-Version ausgeführt werden, ein Upgrade aktiv erzwingen, bevor wir die nächste veraltete Version entfernen. Was passiert mit meinen Clustern während des Force-Upgrades? . Wir werden ein automatisiertes Kubernetes-Upgrade für die Controlplane und für die Machine Deployments initiieren. Dies sollte zwar funktionieren, aber ein erfolgreiches Upgrade kann aufgrund der Vielfalt der Anwendungen und Anwendungsfälle nicht garantiert werden. Änderungen in der Kubernetes API können zu fehlerhaften/inkompatiblen Anwendungen innerhalb des Kubernetes-Clusters führen. Wir können die Verantwortung für solche Probleme nicht übernehmen. Aus diesem Grund empfehlen wir jedem Kunden, das Kubernetes-Upgrade selbst einzuplanen und durchzuführen. ",
    "url": "/gks/clusterlifecycle/deprecationpolicy/#gks-force-upgrade-richtlinie",
    
    "relUrl": "/gks/clusterlifecycle/deprecationpolicy/#gks-force-upgrade-richtlinie"
  },"135": {
    "doc": "Deprecation Policy",
    "title": "Deprecation Policy",
    "content": " ",
    "url": "/gks/clusterlifecycle/deprecationpolicy/",
    
    "relUrl": "/gks/clusterlifecycle/deprecationpolicy/"
  },"136": {
    "doc": "Migration Container Runtime Engine",
    "title": "Migration Container Runtime Engine",
    "content": " ",
    "url": "/gks/clusterlifecycle/clustermigrations/containerruntimeengine/#migration-container-runtime-engine",
    
    "relUrl": "/gks/clusterlifecycle/clustermigrations/containerruntimeengine/#migration-container-runtime-engine"
  },"137": {
    "doc": "Migration Container Runtime Engine",
    "title": "Informationen zur Container Runtime Engine",
    "content": "Lange Zeit wurde im Unterbau von Kubernetes Docker (genauer gesagt der dockershim) als Container “Runtime Engine” genutzt. Diese Softwarebasis und Integration des dockershim zu pflegen und aktuell zu halten wurde mit der Zeit jedoch immer aufwendiger, so dass die Kubernetes Maintainer sich dazu entschieden, einen generischeren Standard zu wählen, der weniger Aufwand bedeutet. Heraus kam dabei der CRI Standard, für den es auch recht schnell Implementierungen wie containerd oder cri-o gab. Da Docker diesen Standard selbst nicht implementiert, musste der dockershim entwickelt werden. Mit dem Kubernetes v1.20 Release wurde der Einsatz von dockershim ab v1.24 abgekündigt. Um GKS Kubernetes Cluster auf die Version v1.24 updaten zu können, muss vorher die Container Runtime Engine auf eine CRI-Standard konforme Implementierung migriert werden. Für GKS Cluster ist dies containerd. ",
    "url": "/gks/clusterlifecycle/clustermigrations/containerruntimeengine/#informationen-zur-container-runtime-engine",
    
    "relUrl": "/gks/clusterlifecycle/clustermigrations/containerruntimeengine/#informationen-zur-container-runtime-engine"
  },"138": {
    "doc": "Migration Container Runtime Engine",
    "title": "Änderung der Container Runtime Engine Konfiguration",
    "content": "Die Migration wird vorbereitet, indem die aktuelle Container Runtime Engine des Kubernetes Clusters von docker auf containerd gestellt wird. Dazu müssen Sie folgende Schritte ausführen: . | Editieren Sie die Cluster Konfiguration. | Ändern Sie den Wert des Feldes Container Runtime von docker nach containerd und speichern Sie anschließend die Änderungen. | . ",
    "url": "/gks/clusterlifecycle/clustermigrations/containerruntimeengine/#%C3%A4nderung-der-container-runtime-engine-konfiguration",
    
    "relUrl": "/gks/clusterlifecycle/clustermigrations/containerruntimeengine/#änderung-der-container-runtime-engine-konfiguration"
  },"139": {
    "doc": "Migration Container Runtime Engine",
    "title": "Ausführung der Migration",
    "content": "Um die Migration auszuführen, müssen die Worker-Nodes einmal durchgestartet werden. Dies kann entweder im Rahmen eines Kubernetes Upgrades geschehen, oder durch einen Neustart des Machine Deployments. Da Ersteres bereits hier dokumentiert ist, wird im Folgenden der Neustart im Detail beschrieben. Wichtig ist hierbei zu beachten, dass ein Neustart des Machine Deployments unweigerlich alle PODs mindestens einmal neu startet, da alle Worker-Nodes einmal ausgetauscht werden. | Prüfen Sie die aktuelle Container Runtime Engine auf der Konsole mit dem kubectl-Kommando. $ kubectl describe node | grep \"Container Runtime Version\" Container Runtime Version: docker://19.3.15 Container Runtime Version: docker://19.3.15 Container Runtime Version: docker://19.3.15 . Die Ausgabe des Befehls zeigt, dass aktuell der dockershim verwendet wird. | Starten Sie das Machine Deployment neu: . | Wählen Sie das Machine Deployment im Cluster-Bildschirm. | Drücken Sie die Restart-Schaltfläche im Machine Deployment Bildschirm. | Bestätigen Sie, dass der Neustart des Machine Deployments wirklich durchgeführt werden soll. | . Nun werden alle Worker-Nodes nacheinander aus dem Cluster rotiert und durch frische Nodes ersetzt. Nachdem der letzte Node durchgetauscht wurde, ist der Neustart des Machine Deployments abgeschlossen. | Prüfen Sie erneut, welche Container Runtime Engine nach dem Neustart läuft. $ kubectl describe node | grep \"Container Runtime Version\" Container Runtime Version: containerd://1.5.4 Container Runtime Version: containerd://1.5.4 Container Runtime Version: containerd://1.5.4 . | . Damit ist die Migration der Container Runtime Engine von docker nach containerd abgeschlossen. ",
    "url": "/gks/clusterlifecycle/clustermigrations/containerruntimeengine/#ausf%C3%BChrung-der-migration",
    
    "relUrl": "/gks/clusterlifecycle/clustermigrations/containerruntimeengine/#ausführung-der-migration"
  },"140": {
    "doc": "Migration Container Runtime Engine",
    "title": "Weiterführende Themen",
    "content": ". | https://kubernetes.io/blog/2020/12/02/dockershim-faq/ | https://kubernetes.io/blog/2020/12/02/dont-panic-kubernetes-and-docker/ | . ",
    "url": "/gks/clusterlifecycle/clustermigrations/containerruntimeengine/#weiterf%C3%BChrende-themen",
    
    "relUrl": "/gks/clusterlifecycle/clustermigrations/containerruntimeengine/#weiterführende-themen"
  },"141": {
    "doc": "Migration Container Runtime Engine",
    "title": "Migration Container Runtime Engine",
    "content": " ",
    "url": "/gks/clusterlifecycle/clustermigrations/containerruntimeengine/",
    
    "relUrl": "/gks/clusterlifecycle/clustermigrations/containerruntimeengine/"
  },"142": {
    "doc": "Cluster Backups",
    "title": "Cluster Backups",
    "content": " ",
    "url": "/gks/clusterbackups/#cluster-backups",
    
    "relUrl": "/gks/clusterbackups/#cluster-backups"
  },"143": {
    "doc": "Cluster Backups",
    "title": "Weiterführende Themen",
    "content": ". | Etcd Backup und Wiederherstellung | Ein PVC von einem existierenden Openstack Volume wiederherstellen | . ",
    "url": "/gks/clusterbackups/#weiterf%C3%BChrende-themen",
    
    "relUrl": "/gks/clusterbackups/#weiterführende-themen"
  },"144": {
    "doc": "Cluster Backups",
    "title": "Cluster Backups",
    "content": " ",
    "url": "/gks/clusterbackups/",
    
    "relUrl": "/gks/clusterbackups/"
  },"145": {
    "doc": "Etcd Backup und Wiederherstellung",
    "title": "Etcd Backup und Wiederherstellung",
    "content": "GKS hat eine etcd Backup- und Wiederherstellungsfunktion, die standardmäßig für alle Cluster aktiviert ist. Die Standardkonfiguration des Backups wird mit einem Standardintervall von @alle 20m und einer Aufbewahrung der letzten 20 Backups erstellt. Es ist jedoch möglich, bei Bedarf zusätzliche Backup-Schedules zu erstellen. Hinweis: Bedenken Sie, dass es sich nur um ein etcd-Backup und -Wiederherstellung handelt. Das einzige, was wiederhergestellt wird, ist der etcd-Status, nicht die PVC-Volumes mit Anwendungsdaten oder ähnlichem. ",
    "url": "/gks/clusterbackups/etcdbackups/#etcd-backup-und-wiederherstellung",
    
    "relUrl": "/gks/clusterbackups/etcdbackups/#etcd-backup-und-wiederherstellung"
  },"146": {
    "doc": "Etcd Backup und Wiederherstellung",
    "title": "Etcd Backup",
    "content": "Etcd Backup-Schedules anlegen . Etcd Backups und Wiederherstellungen sind an ein Projekt gebundene Ressourcen, die Sie in der Projektansicht verwalten können. Um eine neue Sicherung zu erstellen, müssen Sie auf die Schaltfläche Add Automatic Backup hinzufügen klicken. Sie haben die Wahl zwischen voreingestellten täglichen, wöchentlichen oder monatlichen Backups, oder einem Backup mit einem benutzerdefinierten Intervall und einer bestimmten Zeit. Um alle verfügbaren Backups zu sehen, klicken Sie auf ein Backup, das Sie interessiert. Sie sehen dann die Liste der abgeschlossenen Backups. Etcd Backup-Snapshot anlegen . Sie können auch einmalige Backup-Snapshots erstellen, die ähnlich wie die automatischen Snapshots eingerichtet sind, mit dem Unterschied, dass für sie kein Zeitplan und keine Aufbewahrungszeit festgelegt sind. ",
    "url": "/gks/clusterbackups/etcdbackups/#etcd-backup",
    
    "relUrl": "/gks/clusterbackups/etcdbackups/#etcd-backup"
  },"147": {
    "doc": "Etcd Backup und Wiederherstellung",
    "title": "Etcd Backups wiederherstellen",
    "content": "Wenn Sie ein Backup wiederherstellen möchten, müssen Sie auf das Wiederherstellungssymbol für das Backup in der Benutzeroberfläche klicken (Restore Cluster from Snapshot). Wiederherstellung von Etcd Backup-Schedules . Wiederherstellung von Etcd Backup-Snapshot . Danach wird der Cluster angehalten, etcd wird gelöscht und dann aus dem Backup neu erstellt. Danach wird die Pause des Clusters wieder aufgehoben. In der Zwischenzeit wird ein Etcd-Wiederherstellungsobjekt im Projekt erstellt, dessen Fortschritt Sie in der Etcd-Wiederherstellungsliste beobachten können. In der Cluster-Ansicht werden Sie feststellen, dass sich Ihr Cluster in einem Wiederherstellungszustand befindet und Sie nicht mit ihm interagieren können, bis der Vorgang abgeschlossen ist. Wenn die Wiederherstellung abgeschlossen ist, wird der Cluster wieder freigegeben, so dass Sie ihn verwenden können. Die etcd-Wiederherstellung geht in den Zustand “Completed” über. ",
    "url": "/gks/clusterbackups/etcdbackups/#etcd-backups-wiederherstellen",
    
    "relUrl": "/gks/clusterbackups/etcdbackups/#etcd-backups-wiederherstellen"
  },"148": {
    "doc": "Etcd Backup und Wiederherstellung",
    "title": "Etcd Backup und Wiederherstellung",
    "content": " ",
    "url": "/gks/clusterbackups/etcdbackups/",
    
    "relUrl": "/gks/clusterbackups/etcdbackups/"
  },"149": {
    "doc": "Ein PVC von einem existierenden Openstack Volume wiederherstellen",
    "title": "Ein PVC von einem existierenden Openstack Volume wiederherstellen",
    "content": "Wird ein PersistentVolumeClaim (PVC) in einem Ihrer Cluster angelegt, wird daraufhin normalerweise automatisch ein neues PersistentVolume (PV) in Kubernetes sowie ein entsprechendes neues Volume in Openstack angelegt. Das neue Volume ist leer und kann direkt genutzt werden. Es ist allerdings auch möglich, ein bereits existierendes Openstack-Volume in einem Kubernetes Cluster zu benutzen. Hier beschreiben wir eine Möglichkeit, wie dies erreicht werden kann. ",
    "url": "/gks/clusterbackups/restorepvcfromvolume/#ein-pvc-von-einem-existierenden-openstack-volume-wiederherstellen",
    
    "relUrl": "/gks/clusterbackups/restorepvcfromvolume/#ein-pvc-von-einem-existierenden-openstack-volume-wiederherstellen"
  },"150": {
    "doc": "Ein PVC von einem existierenden Openstack Volume wiederherstellen",
    "title": "Voraussetzungen",
    "content": "Die wichtigste Voraussetzung ist ein freies, aktuell nicht benutztes Openstack-Volume. Dies könnte zum Beispiel der Fall sein, wenn ein alter Cluster gelöscht, aber im Löschen-Dialog die Option explizit selektiert wurde, dass verbundene Volumes nicht gelöscht werden sollen oder wenn ein Volume von einem Cluster in ein anderes umgezogen werden soll. In jedem Fall wird es nicht um das Volume an sich gehen, sondern um die darauf vorhandenen Daten, die dem Kubernetes Cluster jetzt als PVC zur Verfügung gestellt werden sollen. Um ein existierendes Openstack-Volume als PVC in ein Kubernetes-Cluster einzubinden, benötigen Sie die ID des Volumes. Um diese herauszufinden, müssen Sie sich zuerst in das Openstack/Optimist Dashboard einloggen. Die Zugangsdaten sind identisch mit den Zugangsdaten, die Sie für das GKS/GKS-Dashboard benutzen. Navigieren Sie nach dem Login zu Volumes und suchen Sie das Volume, welches Sie anbinden wollen. Das Volume darf aktuell an keine andere Instanz/VM gebunden sein und muss den Status “Available” haben. Da ein Volume immer nur an eine Instanz gebunden sein kann, muss das Volume ggf. erst von der alten Instanz getrennt (detached) werden, bevor es im Kubernetes-Cluster verwendet werden kann. Nachdem Sie das Volume gefunden haben, klicken Sie auf den Namen. Auf der Detailseite finden Sie dann die ID des Volumes, die Sie für den nächsten Schritt benötigen. ",
    "url": "/gks/clusterbackups/restorepvcfromvolume/#voraussetzungen",
    
    "relUrl": "/gks/clusterbackups/restorepvcfromvolume/#voraussetzungen"
  },"151": {
    "doc": "Ein PVC von einem existierenden Openstack Volume wiederherstellen",
    "title": "Erstellen eines PV auf Basis eines existierenden Openstack-Volumes",
    "content": "Um das Volume im Cluster verfügbar zu machen, müssen Sie im ersten Schritt ein PersistentVolume-Manifest vorbereiten, welches die ID des Openstack-Volumes im spec.csi.volumeHandle-Schlüssel enthält. apiVersion: v1 kind: PersistentVolume metadata: name: test-pv-restore spec: accessModes: - ReadWriteOnce capacity: storage: 3Gi csi: driver: cinder.csi.openstack.org volumeHandle: 6515d33b-287d-43e1-a3c5-e347d2fc8135 persistentVolumeReclaimPolicy: Delete storageClassName: cinder-csi volumeMode: Filesystem . Anschließend erstellen Sie das PV mit kubectl und überprüfen, ob das PV erfolgreich erstellt wurde. # kubectl apply -f restore-pv.yaml persistentvolume/test-pv-restore created # kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE test-pv-restore 3Gi RWO Delete Available cinder-csi 3s . In diesem Beispiel haben Sie das PV test-pv-restore erstellt, das ein bereits existierendes Openstack-Volume referenziert. ",
    "url": "/gks/clusterbackups/restorepvcfromvolume/#erstellen-eines-pv-auf-basis-eines-existierenden-openstack-volumes",
    
    "relUrl": "/gks/clusterbackups/restorepvcfromvolume/#erstellen-eines-pv-auf-basis-eines-existierenden-openstack-volumes"
  },"152": {
    "doc": "Ein PVC von einem existierenden Openstack Volume wiederherstellen",
    "title": "Erstellen eines PVC, das auf das korrekte PV verweist",
    "content": "Im nächsten Schritt müssen Sie ein PVC erstellen, welches das eben erzeugte PV referenziert. Dazu müssen Sie im PVC-Manifest den Schlüssel spec.volumeName auf den Namen des gerade erstellten PVs setzen. apiVersion: v1 kind: PersistentVolumeClaim metadata: name: test-pvc spec: accessModes: - ReadWriteOnce resources: requests: storage: 3Gi volumeName: test-pv-restore . Nach dem Erstellen des PVCs mit kubectl sollte sich das PVC im Status “Bound” befinden. # kubectl apply -f restore-pvc.yaml persistentvolumeclaim/test-pvc created # kubectl get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE test-pvc Bound test-pv-restore 3Gi RWO cinder-csi 2s . Das PVC “test-pvc” ist damit zur Verwendung bereit. ",
    "url": "/gks/clusterbackups/restorepvcfromvolume/#erstellen-eines-pvc-das-auf-das-korrekte-pv-verweist",
    
    "relUrl": "/gks/clusterbackups/restorepvcfromvolume/#erstellen-eines-pvc-das-auf-das-korrekte-pv-verweist"
  },"153": {
    "doc": "Ein PVC von einem existierenden Openstack Volume wiederherstellen",
    "title": "Erstellen eines Test-Pods, der das PVC benutzt",
    "content": "Um die Daten vor der tatsächlichen Verwendung zu überprüfen, können Sie einen Test-Pod erstellen, der das PVC benutzt. apiVersion: v1 kind: Pod metadata: name: test-pod spec: containers: - name: test-pod image: ubuntu command: - \"sleep\" - \"604800\" volumeMounts: - mountPath: \"/restore\" name: test-pvc volumes: - name: test-pvc persistentVolumeClaim: claimName: test-pvc . Der wichtige Teil des Manifests ist dabei, dass der claimName korrekt auf den Namen des eben erstellten PVCs gesetzt ist. Nachdem der Pod mit kubectl erzeugt wurde, können Sie sich mit dem Pod verbinden. Das Volume ist im o.g. Beispiel im Pfad /restore gemounted, so dass Sie die Daten des Volumes dort finden. # kubectl apply -f pvc-example/test-pod.yaml pod/test-pod created # kubectl get pod -w NAME READY STATUS RESTARTS AGE test-pod 0/1 ContainerCreating 0 5s test-pod 0/1 ContainerCreating 0 17s test-pod 1/1 Running 0 22s ^C # kubectl exec -ti test-pod -- /bin/bash root@test-pod:/# ls /restore/ lost+found my_data.txt root@test-pod:/# exit exit . Damit haben Sie erfolgreich ein bereits existierendes Openstack-Volume an einen Pod in Ihrem Kubernetes Cluster angebunden. ",
    "url": "/gks/clusterbackups/restorepvcfromvolume/#erstellen-eines-test-pods-der-das-pvc-benutzt",
    
    "relUrl": "/gks/clusterbackups/restorepvcfromvolume/#erstellen-eines-test-pods-der-das-pvc-benutzt"
  },"154": {
    "doc": "Ein PVC von einem existierenden Openstack Volume wiederherstellen",
    "title": "Ein PVC von einem existierenden Openstack Volume wiederherstellen",
    "content": " ",
    "url": "/gks/clusterbackups/restorepvcfromvolume/",
    
    "relUrl": "/gks/clusterbackups/restorepvcfromvolume/"
  },"155": {
    "doc": "Machine Deployments",
    "title": "Machine Deployments",
    "content": "Ein Kubernetes-Cluster besteht vereinfacht gesagt aus der Controlplane (dort läuft u. a. die Kubernetes-API, die etcd Datenbank und andere Steuerungskomponenten) und sogenannten Worker-Nodes - (virtuellen) Servern, auf denen die eigentliche Applikations-Lasten gestartet werden. Die Controlplane eines GKS Clusters wird dabei von der Plattform selbst verwaltet, Kunden können nicht direkt auf diese zugreifen. Dagegen können die Worker-Nodes sehr wohl auf verschiedenste Arten konfiguriert werden. Der Kunde kann dabei u.a. das Folgende konfigurieren: . | Node-Flavors: Die Maschinentypen, die als Worker-Nodes benutzt werden. Diese Typen unterscheiden sich typischerweise in ihrer Ausstattung bzgl. CPU und RAM. | Anzahl der Worker-Nodes: Hier kann konfiguriert werden, wie viele Server der o.g. Größe den Cluster bilden sollen. | SSH-Keys: Diese können für den Cluster aktiviert werden, so dass Sie sich auf den Worker-Nodes einloggen können. Dafür ist es weiterhin notwendig, dass die Nodes eine Public IP (Floating IP) besitzen, damit Sie diese erreichen können. Ein Login per SSH kann beispielsweise im Rahmen eines intensiven Debuggings der Applikationen hilfreich sein. | Betriebssystem der Worker-Nodes: Auch wenn die Wahl des Betriebssystem irrelevant für die auf Kubernetes laufenden Applikationen ist, könnte es im Kontext des Debuggings eine Rolle spielen. | . ",
    "url": "/gks/machinedeployments/#machine-deployments",
    
    "relUrl": "/gks/machinedeployments/#machine-deployments"
  },"156": {
    "doc": "Machine Deployments",
    "title": "Weiterführende Themen",
    "content": ". | Sizing: Node Flavors konfigurieren | Debugging: Auslastung der Cluster Nodes | Debugging: SSH-Keys hinzufügen | Aktualisierung des Betriebssystems auf Worker-Nodes | Cluster Maschinen in multiplen Verfügbarkeitszonen | . ",
    "url": "/gks/machinedeployments/#weiterf%C3%BChrende-themen",
    
    "relUrl": "/gks/machinedeployments/#weiterführende-themen"
  },"157": {
    "doc": "Machine Deployments",
    "title": "Machine Deployments",
    "content": " ",
    "url": "/gks/machinedeployments/",
    
    "relUrl": "/gks/machinedeployments/"
  },"158": {
    "doc": "Machine Deployments",
    "title": "Machine Deployments",
    "content": " ",
    "url": "/gks/machinedeployments/machinedeployment/#machine-deployments",
    
    "relUrl": "/gks/machinedeployments/machinedeployment/#machine-deployments"
  },"159": {
    "doc": "Machine Deployments",
    "title": "Machine Deployment hinzufügen",
    "content": "Um ein Machine Deployment hinzuzufügen, klicken Sie auf die Add Machine Deployment Schaltfläche in der oberen rechten Ecke. Der Add Machine Deployment Dialog erscheint. Fügen Sie Ihre Daten hinzu und klicken Sie auf Add Machine Deployment. Die neuen Nodes werden jetzt angelegt. Den aktuellen Status bekommen Sie in den Machine Deployment Details. Wählen Sie das neue Machine Deployment aus. Warten Sie bis alle Nodes grün sind. ",
    "url": "/gks/machinedeployments/machinedeployment/#machine-deployment-hinzuf%C3%BCgen",
    
    "relUrl": "/gks/machinedeployments/machinedeployment/#machine-deployment-hinzufügen"
  },"160": {
    "doc": "Machine Deployments",
    "title": "Machine Deployments löschen",
    "content": "Um ein Machine Deployment zu löschen, können Sie das Löschsymbol in der Machine Deployment Liste benutzen. Sie können auch das Löschsymbol auf der Detailseite verwenden. ",
    "url": "/gks/machinedeployments/machinedeployment/#machine-deployments-l%C3%B6schen",
    
    "relUrl": "/gks/machinedeployments/machinedeployment/#machine-deployments-löschen"
  },"161": {
    "doc": "Machine Deployments",
    "title": "Machine Deployments umbenennen",
    "content": "Machine Deployments können nicht umbenannt werden. Es muss daher ein neues Machine Deployment angelegt werden und anschließend das alte gelöscht werden. Dabei kann es aber, je nach Replica Einstellung und Anzahl der Nodes, zu kurzen Ausfällen kommen. Sicherer ist es, wenn das alte Machine Deployment langsam reduziert wird - 1 Replica nach dem anderen – bis keine Node mehr übrig ist und dann erst löscht. Bei der Prozedur kann es passieren, dass Pods nicht nur auf die neuen Nodes umgezogen werden, sondern auch auf noch aktive alte. Dann wird ein Pod ggf. mehrfach umgezogen. Das können Sie umgehen, indem Sie vor dem Reduzieren erst alle alten Nodes mit kubectl cordon &lt;node name&gt; aus dem Scheduler entfernen. ",
    "url": "/gks/machinedeployments/machinedeployment/#machine-deployments-umbenennen",
    
    "relUrl": "/gks/machinedeployments/machinedeployment/#machine-deployments-umbenennen"
  },"162": {
    "doc": "Machine Deployments",
    "title": "Machine Deployments",
    "content": " ",
    "url": "/gks/machinedeployments/machinedeployment/",
    
    "relUrl": "/gks/machinedeployments/machinedeployment/"
  },"163": {
    "doc": "Node-Flavors konfigurieren",
    "title": "Node-Flavors konfigurieren",
    "content": "Während der Erstellung von Nodes in einem Cluster können Sie zwischen diversen sogenannten “Flavors” wählen. Flavors bestimmen die Anzahl der Prozessorkerne (“CPUs”), den Speicher (“GB RAM”) und die Festplattengröße (“GB Disk”) der entsprechenden Node. Hilfreich bei der Auswahl des richtigen Flavors sind die folgenden, wiederholbaren Schritte: . | Zunächst starten Sie mit kleinen Flavors. | Dann evaluieren Sie die Performance des geplanten Projektes mit diesem Flavor. | Treten Probleme auf, wechseln Sie zu einem größeren Flavor. | . ",
    "url": "/gks/machinedeployments/nodeflavors/#node-flavors-konfigurieren",
    
    "relUrl": "/gks/machinedeployments/nodeflavors/#node-flavors-konfigurieren"
  },"164": {
    "doc": "Node-Flavors konfigurieren",
    "title": "Flavors ändern",
    "content": "Der Wechsel auf einen anderen Flavor ist denkbar einfach: . | Zuerst navigieren Sie zu dem gewünschten Cluster. | Nun klicken Sie auf das Editieren-Icon des Machine Deployments, dessen Flavor Sie ändern möchten. Das Icon erscheint erst, wenn Sie mit der Maus über die entsprechende Machine Deployment fahren. | In dem jetzt geöffneten Fenster selektieren Sie den gewünschten Flavor und klicken zuletzt auf Save Changes, um den Prozess abzuschließen. | Kurz danach erscheint eine Bestätigungsnachricht auf Ihrem Bildschirm, dass der Flavor erfolgreich geändert wurde. | . ",
    "url": "/gks/machinedeployments/nodeflavors/#flavors-%C3%A4ndern",
    
    "relUrl": "/gks/machinedeployments/nodeflavors/#flavors-ändern"
  },"165": {
    "doc": "Node-Flavors konfigurieren",
    "title": "Node-Flavors konfigurieren",
    "content": " ",
    "url": "/gks/machinedeployments/nodeflavors/",
    
    "relUrl": "/gks/machinedeployments/nodeflavors/"
  },"166": {
    "doc": "Nutzungsrate der Cluster Nodes",
    "title": "Nutzungsrate der Cluster Nodes",
    "content": "Bei der Überprüfung des Clusters ist uns eine ungewöhnlich hohe Speicher-Auslastung aufgefallen. Wir sehen, dass ein Node komplett ausgelastet ist, während der andere nur wenig Last zeigt. Dies passiert oft, wenn bei einem Cluster mit weniger als drei Nodes ein Upgrade auf den Nodes durchgeführt wird. Hier erklären wir, wie es dazu kommt. ",
    "url": "/gks/machinedeployments/clusternodesusagerate/#nutzungsrate-der-cluster-nodes",
    
    "relUrl": "/gks/machinedeployments/clusternodesusagerate/#nutzungsrate-der-cluster-nodes"
  },"167": {
    "doc": "Nutzungsrate der Cluster Nodes",
    "title": "Node Auslastung",
    "content": "Als Erstes schauen wir, wie viele Nodes im Cluster sind, und wie ihre Auslastung ist. Der Befehl kubectl top node zeigt die aktuelle Node-Auslastung. Im Beispiel haben wie zwei laufende (running) Nodes. Erst wird der Node “cordoned” - also deaktiviert, damit keine neuen Pods auf dem Node gestartet werden. Dann wird der Node “drained” - also komplett leer gemacht und die bisher auf dem gedrainten Node laufenden Pods auf alle anderen Nodes des Clusters verteilt. Bei nur zwei Nodes wird also alles immer auf den anderen Node platziert. Wenn der zweite Node nach dem Update wieder läuft, werden die Pods nicht automatisch auf beide Nodes verteilt. Dadurch kommt es zu dem eingangs beobachteten Ungleichgewicht. ",
    "url": "/gks/machinedeployments/clusternodesusagerate/#node-auslastung",
    
    "relUrl": "/gks/machinedeployments/clusternodesusagerate/#node-auslastung"
  },"168": {
    "doc": "Nutzungsrate der Cluster Nodes",
    "title": "Ein paar Tipps",
    "content": ". | Wir empfehlen mindestens drei Nodes zu verwenden, da so auch bei Upgrades die Last besser verteilt werden kann. | Das Tool popeye analysiert Cluster und macht Verbesserungsvorschläge auf Grundlage von Best Practices. | Führen Sie auf den Nodes ein Upgrade auf die neueste Version durch und beachten Sie den letzten Schritt in Kubernetes Updates. | . ",
    "url": "/gks/machinedeployments/clusternodesusagerate/#ein-paar-tipps",
    
    "relUrl": "/gks/machinedeployments/clusternodesusagerate/#ein-paar-tipps"
  },"169": {
    "doc": "Nutzungsrate der Cluster Nodes",
    "title": "Nutzungsrate der Cluster Nodes",
    "content": " ",
    "url": "/gks/machinedeployments/clusternodesusagerate/",
    
    "relUrl": "/gks/machinedeployments/clusternodesusagerate/"
  },"170": {
    "doc": "SSH-Key einem Cluster hinzufügen",
    "title": "SSH-Key einem Cluster hinzufügen",
    "content": "Mit der GKS-Plattform können Sie einen SSH-ey auf Worker-Nodes installieren. Dies kann zum Beispiel hilfreich sein, wenn Sie den Cluster oder eine eigene Applikation unmittelbar auf den Worker-Nodes debuggen möchten. Dafür müssen Sie folgende Schritte ausführen: . | Einen SSH-Key erstellen | Den User SSH Key Agent im Cluster aktivieren | Den SSH-Key zum Projekt hinzufügen | Den SSH-Key im Cluster aktivieren | . In der Regel muss den Worker-Nodes noch eine Floating IP zugewiesen werden, damit der Zugriff auf die Worker-Nodes auch Netzwerkseitig funktioniert. ",
    "url": "/gks/machinedeployments/add_ssh_key/#ssh-key-einem-cluster-hinzuf%C3%BCgen",
    
    "relUrl": "/gks/machinedeployments/add_ssh_key/#ssh-key-einem-cluster-hinzufügen"
  },"171": {
    "doc": "SSH-Key einem Cluster hinzufügen",
    "title": "User SSH Key Agent",
    "content": "Um SSH-Keys verwalten zu können, muss der User SSH Key Agent im Rahmen der Erstellung des Clusters aktiviert worden sein. Ist der Agent nicht bei der Erstellung des Clusters aktiviert worden, können Sie nachträglich keine SSH-Keys zum Cluster hinzufügen bzw. ändern. Weiterhin kann auch der User SSH Key Agent nur während der Erstellung des Clusters aktiviert werden, eine nachträgliche Aktivierung ist nicht möglich. Den Status des User SSH Key Agents überprüfen . Sie können den aktuellen Status des User SSH Key Agents auf der Übersichtsseite des Clusters einsehen. Klappen Sie dazu die erweiterte Clusteransicht auf. Nun können Sie im unteren Abschnitt den Status des User SSH Key Agents ablesen. Wenn der Agent aktiviert ist, können Sie jederzeit SSH-Keys zum Cluster hinzufügen oder ändern. Alternativen zur Verwendung des User SSH Key Agents . Cluster können auch ohne aktivierten User SSH Key Agent angelegt werden. In diesem Fall wird während der gesamten Lebenszeit des Clusters jede neue Worker-Node ohne SSH-Keys erstellt. Eine Änderung dieser Eigenschaft ist auch nachträglich nicht möglich. Sie könnten dann die SSH-Keys der Worker-Nodes mit anderen Tools wie beispielsweise Saltstack, Puppet oder Chef verwalten, wenn Worker-Nodes mit einem eigenen Image erstellt werden, die diese Konfiguration unterstützt. Es ist nicht möglich, den User SSH Key Agent nachträglich zu aktivieren, um solche Setups nicht negativ in ihrer Funktion zu beeinflussen. ",
    "url": "/gks/machinedeployments/add_ssh_key/#user-ssh-key-agent",
    
    "relUrl": "/gks/machinedeployments/add_ssh_key/#user-ssh-key-agent"
  },"172": {
    "doc": "SSH-Key einem Cluster hinzufügen",
    "title": "Einen SSH-Key zu einem bestehenden Cluster hinzufügen",
    "content": "Die folgenden Schritte beschreiben, wie Sie einen SSH-Key zu einem bestehenden Cluster hinzufügen können, welcher den User SSH Key Agent aktiviert hat. Einen SSH-Key erstellen . Sie können ein Schlüsselpaar am einfachsten mit dem Tool ssh-keygen erzeugen. ssh-keygen . Die erzeugten Schlüssel (öffentlich und privat) werden standardmäßig in .ssh/id_rsa.pub abgelegt. Den SSH-Key dem Projekt hinzufügen . | Wählen Sie zuerst das richtige Projekt aus. | Gehen Sie danach zur SSH-Key Seite. | Benutzen Sie die “Add SSH Key”-Schaltfläche. | Um den Schlüssel leichter identifizieren zu können, geben Sie dem Key einen Namen und fügen Sie den öffentlichen (nicht den privaten!) Schlüssel in das dafür vorgesehene Feld ein. | . Jetzt können Sie den Key in jedem Cluster des Projekts benutzen. Dies gilt auch für die Erstellung eines neuen Clusters im gleichen Projekt. Den SSH-Key einem Cluster hinzufügen . | Wählen Sie den Cluster aus. | Um das Cluster-Menü zu öffnen, klicken Sie auf die drei Punkte. | Wählen Sie im Menü Manage SSH keys aus. | Nun können Sie den eben erstellten SSH-Key aus einer Dropdown Liste auswählen. | Nach der Auswahl wird der Key in der Liste angezeigt und kann dort bei Bedarf auch wieder gelöscht werden. | . Der Key wird nun allen Worker-Nodes in allen Machine Deployments hinzugefügt. ",
    "url": "/gks/machinedeployments/add_ssh_key/#einen-ssh-key-zu-einem-bestehenden-cluster-hinzuf%C3%BCgen",
    
    "relUrl": "/gks/machinedeployments/add_ssh_key/#einen-ssh-key-zu-einem-bestehenden-cluster-hinzufügen"
  },"173": {
    "doc": "SSH-Key einem Cluster hinzufügen",
    "title": "Einen SSH-Key während der Cluster-Erstellung hinzufügen",
    "content": "Es ist auch möglich, einen SSH-Key bereits zum Zeitpunkt der Cluster Erstellung zu konfigurieren. Das genaue Vorgehen dazu ist im Abschnitt “Einen Cluster anlegen” beschrieben. ",
    "url": "/gks/machinedeployments/add_ssh_key/#einen-ssh-key-w%C3%A4hrend-der-cluster-erstellung-hinzuf%C3%BCgen",
    
    "relUrl": "/gks/machinedeployments/add_ssh_key/#einen-ssh-key-während-der-cluster-erstellung-hinzufügen"
  },"174": {
    "doc": "SSH-Key einem Cluster hinzufügen",
    "title": "Am Worker anmelden",
    "content": "Sobald der oder die SSH-Keys zum Cluster hinzugefügt wurden, muss den Worker-Nodes noch eine öffentliche (Floating) IP zugewiesen werden, damit diese netzwerkseitig erreichbar sind. Dazu Editieren Sie die Machine Deployments. Dort sollten Sie sicherstellen, dass Allocate Floating IP aktiviert ist. Wenn sich hier ein Setting ändert, werden alle Worker neu erstellt. Danach kann man sich über den SSH-Key einloggen. Der Standarduser für Flatcar heisst core. ssh -A core@PUBLIC_IP . ",
    "url": "/gks/machinedeployments/add_ssh_key/#am-worker-anmelden",
    
    "relUrl": "/gks/machinedeployments/add_ssh_key/#am-worker-anmelden"
  },"175": {
    "doc": "SSH-Key einem Cluster hinzufügen",
    "title": "SSH-Key einem Cluster hinzufügen",
    "content": " ",
    "url": "/gks/machinedeployments/add_ssh_key/",
    
    "relUrl": "/gks/machinedeployments/add_ssh_key/"
  },"176": {
    "doc": "Aktualisierung des Betriebssystems auf Worker-Nodes",
    "title": "Aktualisierung des Betriebssystems auf Worker-Nodes",
    "content": " ",
    "url": "/gks/machinedeployments/updatingnodeos/#aktualisierung-des-betriebssystems-auf-worker-nodes",
    
    "relUrl": "/gks/machinedeployments/updatingnodeos/#aktualisierung-des-betriebssystems-auf-worker-nodes"
  },"177": {
    "doc": "Aktualisierung des Betriebssystems auf Worker-Nodes",
    "title": "Flatcar",
    "content": "Flatcar Worker-Nodes automatisch aktualisieren . GKS bietet die Funktionalität, um das Betriebssystem von Flatcar-basierten Worker-Nodes auf dem neuesten Stand zu halten. Diese Funktion installiert automatisch alle Updates auf den Worker-Nodes, die vom Upstream-Anbieter (Kinvolk) für Flatcar veröffentlicht werden. Die Auto-Update-Funktion verwendet FLUO, den Flatcar Linux Update Operator im Hintergrund. Wenn nach dem Aktualisieren des Systems ein Neustart erforderlich ist, wird der Node vor dem Neustart evakuiert. Der Operator koordiniert den Neustart mehrerer Nodes im Cluster, und stellt sicher, dass immer nur ein Node gleichzeitig neu gestartet wird. Die Verwendung der Auto-Update-Funktion ist standardmäßig aktiviert. Der folgende Screenshot zeigt die Erstellung eines Machine Deployments mit aktiviertem Auto-Updater: . Wenn Sie sich selbst um Betriebssystemaktualisierungen (und Neustarts) kümmern möchten, können Sie die automatischen Aktualisierungen der Worker-Nodes deaktivieren, indem Sie das Kontrollkästchen Disable auto-update aktivieren: . Wir empfehlen unseren Benutzern dringend, die Auto-Updater-Funktion zu verwenden, um die Sicherheit ihrer Infrastruktur zu gewährleisten. Prüfen der Auto-Updater-Einstellungen eines Clusters . Um zu prüfen, ob Ihre Nodes automatische Betriebssystemaktualisierungen erhalten, klicken Sie auf das Machine-Deployment: . Kontrollieren Sie, ob vor dem Kästchen Disable auto-update ein grünes Häkchen angezeigt wird (die automatische Aktualisierung ist deaktiviert): . Oder prüfen Sie, ob es ausgegraut ist (die automatische Aktualisierung ist an): . Aktivieren / Deaktivieren des automatischen Updaters für ein vorhandenes Machine-Deployment . Um den Status des automatischen Updaters zu ändern, klicken Sie auf die Schaltfläche Edit Machine Deployment der Machine-Deployment. (De)aktivieren Sie das Kontrollkästchen entsprechend. Nachdem Sie auf Save Changes geklickt haben, führen alle Worker-Nodes ein rollierendes Update durch und starten neu. Flatcar Worker-Nodes manuell aktualisieren . Um einen Flatcar Worker-Node manuell aktualisieren zu können, wird ein SSH-Zugriff benötigt. Die aktuelle OS-Version finden Sie unter /etc/os-release. $ grep VERSION_ID /etc/os-release VERSION_ID=2765.2.2 . Im nächsten Schritt müssen Sie den Dienst update-engine demaskieren und starten. $ sudo systemctl unmask update-engine.service Removed /etc/systemd/system/update-engine.service. $ sudo systemctl start update-engine.service . Nun können Sie nach verfügbaren Updates suchen und sie installieren lassen. sudo update_engine_client -check_for_update sudo update_engine_client -status . Der Update-Engine-Client lädt jetzt die letzte verfügbare Version von Flatcar herunter und passt automatisch die Boot-Reihenfolge so an, dass beim nächsten Boot schon die neue Version gebootet wird. Wenn der Status sich von UPDATE_STATUS_UPDATE_AVAILABLE in UPDATE_STATUS_DOWNLOADING, und danach in UPDATE_STATUS_UPDATED_NEED_REBOOT geändert hat, können Sie den Worker-Node rebooten und den Vorgang für alle Worker-Nodes durchführen. sudo systemctl reboot . Wir empfehlen unseren Benutzern dringend, die Auto-Updater-Funktion zu verwenden, um die Sicherheit ihrer Infrastruktur zu gewährleisten. ",
    "url": "/gks/machinedeployments/updatingnodeos/#flatcar",
    
    "relUrl": "/gks/machinedeployments/updatingnodeos/#flatcar"
  },"178": {
    "doc": "Aktualisierung des Betriebssystems auf Worker-Nodes",
    "title": "Ubuntu",
    "content": "Ubuntu wurde im Juli 2021 aus dem Support für GKS genommen, aktualisieren Sie daher Ihre bestehenden Machine Deployments. Auf Flatcar aktualisieren . Um auf Flatcar zu aktualisieren, klicken Sie auf die Machine Deployment Edit Schaltfläche. Dann klicken Sie auf das Flatcar Logo. Es wurde das Image gewechselt und die Autoupdate Option angezeigt. Die Nodes werden nun neu gebaut und der Cluster ist wieder aktuell. ",
    "url": "/gks/machinedeployments/updatingnodeos/#ubuntu",
    
    "relUrl": "/gks/machinedeployments/updatingnodeos/#ubuntu"
  },"179": {
    "doc": "Aktualisierung des Betriebssystems auf Worker-Nodes",
    "title": "Zusammenfassung",
    "content": "In dieser Anleitung haben Sie Folgendes gelernt: . | Was ist die Auto-Update-Funktion? | Wie aktiviert oder deaktiviert man die Auto-Update-Funktion für ein Flatcar Machine-Deployment? | Wie aktualisiert man manuell einen Flatcar Worker-Node? | Wie aktiviert man die automatische Reboots der Ubuntu Worker-Nodes? | . ",
    "url": "/gks/machinedeployments/updatingnodeos/#zusammenfassung",
    
    "relUrl": "/gks/machinedeployments/updatingnodeos/#zusammenfassung"
  },"180": {
    "doc": "Aktualisierung des Betriebssystems auf Worker-Nodes",
    "title": "Weiterführende Themen",
    "content": ". | Auto-Updating Flatcar Container Linux | FLUO on Github | The Flatcar partitioning scheme | . ",
    "url": "/gks/machinedeployments/updatingnodeos/#weiterf%C3%BChrende-themen",
    
    "relUrl": "/gks/machinedeployments/updatingnodeos/#weiterführende-themen"
  },"181": {
    "doc": "Aktualisierung des Betriebssystems auf Worker-Nodes",
    "title": "Aktualisierung des Betriebssystems auf Worker-Nodes",
    "content": " ",
    "url": "/gks/machinedeployments/updatingnodeos/",
    
    "relUrl": "/gks/machinedeployments/updatingnodeos/"
  },"182": {
    "doc": "Mehrere Availability-Zones",
    "title": "Mehrere Availability-Zones",
    "content": " ",
    "url": "/gks/machinedeployments/multipleavailabilityzones/#mehrere-availability-zones",
    
    "relUrl": "/gks/machinedeployments/multipleavailabilityzones/#mehrere-availability-zones"
  },"183": {
    "doc": "Mehrere Availability-Zones",
    "title": "Einleitung",
    "content": "Ein machine deployment ist immer an eine Availability-Zone (im Weiteren AZ abgekürzt) gebunden, da eine Maschine zu jedem Zeitpunkt immer nur in einer Availability-Zone existieren kann und machine deployments generell nicht aus Maschinen verschiedener Availability-Zones bestehen. Dies beschränkt den nutzbaren persistenten Speicher für Pods auf die jeweilige Availability-Zone in der der Pod das erste mal ge-schedule’d wurde. ",
    "url": "/gks/machinedeployments/multipleavailabilityzones/#einleitung",
    
    "relUrl": "/gks/machinedeployments/multipleavailabilityzones/#einleitung"
  },"184": {
    "doc": "Mehrere Availability-Zones",
    "title": "Setups in einer Availability-Zone",
    "content": "Im Normalfall hat ein Kubernetes Cluster zum Erstellungszeitpunkt nur ein machine deployment, welches in genau einer Availability-Zone liegt und somit keinerlei Probleme bereitet. Wenn ein Pod auf einer Maschine dieses machine deployments laufen soll und persistenten Speicher braucht, fordert er einfach Speicher aus dieser Availability-Zone an. Sollte der Pod von seiner Maschine verdrängt werden, wählt Kubernetes automatisch eine passende Maschine mit hinreichend Ressourcen aus und fährt den Pod auf seinem neuen Ziel wieder hoch. Der dabei an die alte Maschine gebundene persistente Speicher wird nahtlos an der neuen Maschine zur Verfügung gestellt und der Pod kann wie erwartet normal hochfahren. ",
    "url": "/gks/machinedeployments/multipleavailabilityzones/#setups-in-einer-availability-zone",
    
    "relUrl": "/gks/machinedeployments/multipleavailabilityzones/#setups-in-einer-availability-zone"
  },"185": {
    "doc": "Mehrere Availability-Zones",
    "title": "Setups in mehreren Availability-Zones",
    "content": "Setups beschränkt auf einer von mehreren Availability-Zones . Man kann mehr als ein machine deployment haben. Wenn man sich für ein Zweites (oder N-tes) entscheidet kann man wählen, in welcher Availability-Zone dieses neue machine deployment liegen soll. Es ist möglich das neue machine deployment in dieselbe Availability-Zone zu stellen wie das Erste oder es in eine der anderen AZ (dies ist abhängig vom geplanten Verwendungszweck; beides kann sinnvoll sein). Existieren zwei machine deployments in zwei verschiedenen Availability-Zones so kann es passieren dass ein Pod von seiner Maschine in AZ A verdrängt wird und der Kubernetes Scheduler einen neuen Pod in der AZ B einplant und laufen lassen will. Der Pod wird jedoch im Status pending verbleiben und in der Beschreibung einen Fehler hinterlassen dass es nicht möglich ist die Voraussetzungen für das Hochfahren dieses Pods zu erfüllen. Der Grund hierfür liegt im bereits oben beschriebenen Verhalten dass der persistente Speicher ebenfalls an eine Availability-Zone gebunden ist und nicht automatisch migriert werden kann. Eine Lösung ist es Kubernetes beizubringen diese Art von Pod (Applikation) explizit nur in einer bestimmten Availability-Zone zu platzieren. Dies kann sowohl mit einem Deployment, StatefulSet oder Daemonset erreicht werden. Dieses Kubernetes Feature heißt affinity und kann Pods sowohl an (oder gegen) seine eigenen oder andere Pods als auch gegen Nodes verwendet werden. Hier ein Beispiel für ein Deployment welches in der AZ ES1 konfiguriert wird: . apiVersion: apps/v1 kind: Deployment metadata: labels: app: myapp name: myapp namespace: default spec: replicas: 1 ... template: ... spec: affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: topology.kubernetes.io/zone: operator: In values: - es1 ... (Die drei aufeinander folgenden Punkte symbolisieren dass hier Zeilen aus Gründen der einfacheren Lesbarkeit unterschlagen wurden. Das Deployment wird ohne diese unterschlagenen Zeilen nicht funktionieren!) . Setups verteilt über mehrere Availability-Zones . Um die Verfügbarkeit der eigenen Applikation zu erhöhen ist es gängig, diese in zwei verschiedenen Availability-Zones zu platzieren, um den Verlust einer ganzen Availability-Zone mitigieren zu können (natürlich muss die Applikation dies auch unterstützen aber das setzen wir hier mal als gegeben voraus). Um dies zu Erreichen muss man Kubernetes so konfigurieren dass es zwei Repliken einer Applikation immer in zwei verschiedenen Availability-Zones hochfährt. Zusätzlich muss die jeweilige Applikation in derselben AZ bleiben in der der persistente Speicher liegt. Dies kann mit der podAntiAffinity aus dem folgenden Beispiel erreicht werden. In diesem Fall wählen wir ein StatefulSet welches uns die Zuordnung von den Pods zu den jeweils richtigen persistenten Speicherobjekten erleichtert: . apiVersion: apps/v1 kind: StatefulSet metadata: labels: app: myapp name: myapp namespace: default spec: replicas: 2 ... template: ... spec: affinity: podAntiAffinity: requiredDuringSchedulingIgnoredDuringExecution: - labelSelector: matchExpressions: - key: app operator: In values: - myapp topologyKey: topology.kubernetes.io/zone ... (Wieder symbolisieren die drei aufeinander folgenden Punkte dass hier Zeilen aus Gründen der einfacheren Lesbarkeit unterschlagen wurden. Das StatefulSet wird ohne diese unterschlagenen Zeilen nicht funktionieren!) . Um mehr über die Möglichkeiten des Platzierens von Pods in Kubernetes zu lernen folgen sie bitte den unten stehenden Links. ",
    "url": "/gks/machinedeployments/multipleavailabilityzones/#setups-in-mehreren-availability-zones",
    
    "relUrl": "/gks/machinedeployments/multipleavailabilityzones/#setups-in-mehreren-availability-zones"
  },"186": {
    "doc": "Mehrere Availability-Zones",
    "title": "Weiterführende Themen",
    "content": ". | einfache Affinity | Affinity mit mehr Möglichkeiten | . ",
    "url": "/gks/machinedeployments/multipleavailabilityzones/#weiterf%C3%BChrende-themen",
    
    "relUrl": "/gks/machinedeployments/multipleavailabilityzones/#weiterführende-themen"
  },"187": {
    "doc": "Mehrere Availability-Zones",
    "title": "Mehrere Availability-Zones",
    "content": " ",
    "url": "/gks/machinedeployments/multipleavailabilityzones/",
    
    "relUrl": "/gks/machinedeployments/multipleavailabilityzones/"
  },"188": {
    "doc": "Access Management",
    "title": "Access Management",
    "content": "Prinzipiell gibt es zwei Möglichkeiten, Benutzern Zugriff auf einen GKS Cluster zu gewähren: . | Zugriff auf ein GKS-Projekt und damit aller enthaltenen Cluster | Zugriff über “Role-Based Access Control” (RBAC) mit direktem Zugriff auf den Cluster bzw. einzelner Namespaces | . ",
    "url": "/gks/accessmanagement/#access-management",
    
    "relUrl": "/gks/accessmanagement/#access-management"
  },"189": {
    "doc": "Access Management",
    "title": "Weiterführende Themen",
    "content": ". | Projekt-Basierter Zugriff: Mit einem Cluster verbinden | Role-Based Access Control (RBAC) | Revoking Tokens | . ",
    "url": "/gks/accessmanagement/#weiterf%C3%BChrende-themen",
    
    "relUrl": "/gks/accessmanagement/#weiterführende-themen"
  },"190": {
    "doc": "Access Management",
    "title": "Access Management",
    "content": " ",
    "url": "/gks/accessmanagement/",
    
    "relUrl": "/gks/accessmanagement/"
  },"191": {
    "doc": "Projekt-Basierter Zugriff",
    "title": "Projekt-Basierter Zugriff",
    "content": "Hinweis: Dies ist die empfohlene Methode, anderen Benutzern Zugriff auf einen Cluster zu gewähren. Erteilt man Benutzern Zugriff auf ein gesamtes GKS-Projekt, bekommen Benutzer automatisch Zugriff auf alle Cluster innerhalb dieses Projektes. Die Benutzer können sich in GKS einloggen und nach Auswahl des Projektes alle Cluster sehen bzw. je nach gewährten Zugriffsrechten auch bestehende Cluster editieren oder neue Cluster anlegen. Dabei teilen sich alle Benutzer mit den gleichen Zugriffsrechten eine kubeconfig-Datei. Diese Datei nutzt eine Token-basierte Authentifizierung und abhängig vom Access-Level (read-only/admin access) wird ein Token erstellt. ",
    "url": "/gks/accessmanagement/connectingtoacluster/#projekt-basierter-zugriff",
    
    "relUrl": "/gks/accessmanagement/connectingtoacluster/#projekt-basierter-zugriff"
  },"192": {
    "doc": "Projekt-Basierter Zugriff",
    "title": "Mit einem Cluster verbinden",
    "content": "Nachdem Sie in GKS einen Cluster angelegt haben, müssen Sie sich mit diesem verbinden. Das ist notwendig, um Applikationen zu deployen und zu managen. Um einen Cluster zu finden, müssen Sie in die Detailansicht des Clusters gehen. Hierfür klicken Sie auf den Eintrag first-system. Klicken Sie anschließend rechts oben auf Get Kubeconfic. Damit laden Sie kubeconfig-Datei herunter. In dieser Datei stehen alle Endpunkte, Zertifikate sowie Bereiche des Clusters. Mit dieser Datei kann kubectl sich mit dem Cluster verbinden. Um diese Datei zu nutzen, müssen Sie diese auf der Konsole registrieren. Dafür gibt es zwei Möglichkeiten: . | kubectl schaut als Standard in die Datei .kube/config im Heimat-Verzeichnis des Benutzers | Sie können die kubeconfig temporär mittels einer Umgebungsvariable exportieren | . Der Einfachheit halber und um auf Ihrem System die Standards nicht zu verändern, nehmen Sie hier Variante 2. Dafür benutzen Sie eine Konsole. In den Screenshots wurde iTerm2 auf macOS verwendet, es funktioniert jedoch auf Linux und Windows bash genau so. Als Erstes müssen Sie die heruntergeladene Datei finden. Chrome und Firefox laden diese normalerweise in den Downloads-Ordner. Der Dateiname setzt sich aus den zwei folgenden Komponenten zusammen: . | kubeconfig-admin- | Ihre Cluster ID | . Um diese dann zu registrieren, nutzen Sie folgendes Kommando: . cd Downloads export KUBECONFIG=$(pwd)/kubeconfig-admin-CLUSTERID . Nun können Sie mit Ihrem Cluster kommunizieren. Das einfachste Kommando ist hier: “zeige mir alle Nodes meines Clusters”: . kubectl get nodes NAME STATUS ROLES AGE VERSION musing-kalam-XXXXXXXXX-ks4xz Ready &lt;none&gt; 10m v1.20.7 musing-kalam-XXXXXXXXX-txc4w Ready &lt;none&gt; 10m v1.20.7 musing-kalam-XXXXXXXXX-vc4g2 Ready &lt;none&gt; 10m v1.20.7 . ",
    "url": "/gks/accessmanagement/connectingtoacluster/#mit-einem-cluster-verbinden",
    
    "relUrl": "/gks/accessmanagement/connectingtoacluster/#mit-einem-cluster-verbinden"
  },"193": {
    "doc": "Projekt-Basierter Zugriff",
    "title": "Kubernetes Dashboard",
    "content": "In GKS können Sie mit einem Klick auf das Kubernetes Dashboard zugreifen. Um dies im Browser zu öffnen, klicken Sie oben rechts auf die Schaltfläche Open Dashboard. Nun sehen Sie das Kubernetes Dashboard und können Ihren Cluster grafisch erkunden. ",
    "url": "/gks/accessmanagement/connectingtoacluster/#kubernetes-dashboard",
    
    "relUrl": "/gks/accessmanagement/connectingtoacluster/#kubernetes-dashboard"
  },"194": {
    "doc": "Projekt-Basierter Zugriff",
    "title": "Weiterführende Informationen",
    "content": ". | GKS-Projekte verwalten | Revoking Tokens | . ",
    "url": "/gks/accessmanagement/connectingtoacluster/#weiterf%C3%BChrende-informationen",
    
    "relUrl": "/gks/accessmanagement/connectingtoacluster/#weiterführende-informationen"
  },"195": {
    "doc": "Projekt-Basierter Zugriff",
    "title": "Projekt-Basierter Zugriff",
    "content": " ",
    "url": "/gks/accessmanagement/connectingtoacluster/",
    
    "relUrl": "/gks/accessmanagement/connectingtoacluster/"
  },"196": {
    "doc": "Role-Based Access Control (RBAC)",
    "title": "Role-Based Access Control (RBAC)",
    "content": "Die RBAC-Funktion erlaubt es einem GKS-Projekt-Admin, einen feingranularen Zugriff basierend auf vordefinierten ClusterRoles und Roles zu gewähren. Über das GKS Dashboard kann ein Admin dafür clusterweit gültige ClusterRoleBindings und nur in einem bestimmten Namespace gültige RoleBindings anlegen. Benutzer mit diesem Zugriffslevel haben keinen Zugriff auf das GKS Dashboard, können sich jedoch über einen Link (siehe unten) ihre eigene kubeconfig-Datei herunterladen und damit Zugriff auf das Cluster bekommen. Mehr Informationen über Kubernetes RBAC finden Sie hier. ",
    "url": "/gks/accessmanagement/usingrbac/#role-based-access-control-rbac",
    
    "relUrl": "/gks/accessmanagement/usingrbac/#role-based-access-control-rbac"
  },"197": {
    "doc": "Role-Based Access Control (RBAC)",
    "title": "Benutzer-basierter Zugriff",
    "content": "Um einem Benutzer einen RBAC-basierten Zugriff auf einem Cluster einzurichten, klicken Sie im RBAC-Widget auf Add Binding. ",
    "url": "/gks/accessmanagement/usingrbac/#benutzer-basierter-zugriff",
    
    "relUrl": "/gks/accessmanagement/usingrbac/#benutzer-basierter-zugriff"
  },"198": {
    "doc": "Role-Based Access Control (RBAC)",
    "title": "Cluster-weiter Zugriff",
    "content": "Um einen Cluster-weiten Zugriff zu gewähren, wählen Sie im folgenden Fenster Cluster aus, tragen die E-Mail-Adresse des Benutzers ein und wählen die entsprechende Rolle aus. Dabei sollten Sie jedoch beachten, dass der Benutzer prinzipiell für GKS autorisiert ist. Dieser Zugriff ist notwendig, um die kubeconfig-Datei herunterladen zu können. Die auswählbaren Rollen sind als ClusterRoles angelegt und können mit kubectl betrachtet werden. kubectl get clusterrole $NAME_OF_CLUSTERROLE -o yaml . ",
    "url": "/gks/accessmanagement/usingrbac/#cluster-weiter-zugriff",
    
    "relUrl": "/gks/accessmanagement/usingrbac/#cluster-weiter-zugriff"
  },"199": {
    "doc": "Role-Based Access Control (RBAC)",
    "title": "Namespace-weiter Zugriff",
    "content": "Wenn der Zugriff auf einen Namespace beschränkt werden soll, müssen Sie im Add Binding-Dialog zu Namespace wechseln und die E-Mail-Adresse angeben. Im nächsten Schritt müssen Sie die Rolle auswählen, die dem Benutzer zugewiesen werden soll. Zuletzt müssen Sie noch den Namespace auswählen, in dem diese Berechtigung gelten soll. Die für diese Rollen gewährten Zugriffsberechtigungen können Sie ebenfalls mit kubectl abfragen. Da Roles im Gegensatz zu ClusterRoles auf einen bestimmten Namespace bezogen sind, müssen Sie in diesem Fall auch den Namespace mit angeben. kubectl get role $NAME_OF_ROLE -n $NAMESPACE -o yaml . Nachdem Sie den Zugriff entsprechend gewährt haben, sollten die gewährten Rechte im Dashboard sichtbar sein. ",
    "url": "/gks/accessmanagement/usingrbac/#namespace-weiter-zugriff",
    
    "relUrl": "/gks/accessmanagement/usingrbac/#namespace-weiter-zugriff"
  },"200": {
    "doc": "Role-Based Access Control (RBAC)",
    "title": "Den Benutzern die kubeconfig-Datei zur Verfügung stellen",
    "content": "Sobald einem Benutzer RBAC-Rechte zugewiesen wurden, können Sie diesem seine persönliche kubeconfig-Datei über einen speziellen Link zur Verfügung stellen. Dazu müssen Sie den Share kubeconfig Link im GKS Dashboard öffnen. Im nächsten Schritt müssen Sie den angezeigten Link kopieren und an den Nutzer schicken. Der Link zeigt zu einer Login-Seite. Dort muss sich der Benutzer authentifizieren und kann danach direkt seine kubeconfig-Datei herunterladen: . Sobald ein Benutzer seine kubeconfig-Datei heruntergeladen hat, werden alle eventuellen weiteren Änderungen an den RBAC-Rechten sofort aktiv. Insbesondere ein Entziehen der Rechte ist sofort umgesetzt, ein Revoke Token ist damit nicht notwendig. ",
    "url": "/gks/accessmanagement/usingrbac/#den-benutzern-die-kubeconfig-datei-zur-verf%C3%BCgung-stellen",
    
    "relUrl": "/gks/accessmanagement/usingrbac/#den-benutzern-die-kubeconfig-datei-zur-verfügung-stellen"
  },"201": {
    "doc": "Role-Based Access Control (RBAC)",
    "title": "Role-Based Access Control (RBAC)",
    "content": " ",
    "url": "/gks/accessmanagement/usingrbac/",
    
    "relUrl": "/gks/accessmanagement/usingrbac/"
  },"202": {
    "doc": "Revoking Tokens",
    "title": "Revoking Tokens",
    "content": "Alle Benutzer mit den gleichen Zugriffsrechten auf ein gesamtes GKS-Projekt teilen sich die gleiche kubeconfig-Datei. Die kubeconfig-Datei, nutzt eine Token-basierte Authentifizierung, und abhängig vom Access-Level wird ein Token erstellt. Wenn der Zugriff einem Benutzer mit projektweitem Zugriff entzogen werden soll, muss daher die Revoke Token-Funktion des Clusters genutzt werden. Bestehende Token werden dann ungültig und alle verbliebenen Benutzer müssen sich eine neuekubeconfig-Datei herunterladen. ",
    "url": "/gks/accessmanagement/revokingtoken/",
    
    "relUrl": "/gks/accessmanagement/revokingtoken/"
  },"203": {
    "doc": "Revoking Tokens",
    "title": "Login Token Resetten",
    "content": "Zum Rotieren des Login-Tokens führen Sie folgende Schritte aus: . | Wählen Sie Ihren Cluster aus. | Klicken Sie auf der Cluster-Detailseite auf die drei Punkte, um das Cluster-Untermenü zu öffnen. Wählen Sie anschließend Revoke Token. | Wählen Sie den Token aus und klicken Sie auf Revoke Token. | Laden Sie anschließend eine neue kubeconfig-Datei herunter, da die vorherige nun ungültig ist. | . ",
    "url": "/gks/accessmanagement/revokingtoken/#login-token-resetten",
    
    "relUrl": "/gks/accessmanagement/revokingtoken/#login-token-resetten"
  },"204": {
    "doc": "Anwendungen in Kubernetes",
    "title": "Anwendungen in Kubernetes",
    "content": "Applikationen auf Kubernetes zu betreiben ist ein komplexes Thema. In diesem Abschnitt haben wir einige Artikel aufgeführt, die sich auf den Betrieb von Applikationen auf der GKS-Plattform beziehen. ",
    "url": "/gks/k8sapplications/#anwendungen-in-kubernetes",
    
    "relUrl": "/gks/k8sapplications/#anwendungen-in-kubernetes"
  },"205": {
    "doc": "Anwendungen in Kubernetes",
    "title": "Weiterführende Themen",
    "content": ". | Eine Anwendung in Kubernetes starten | Kubernetes Service Accounts verwalten | External-DNS mit Designate | Verwaltung der Zeitzonen | StorageClass Setup | . ",
    "url": "/gks/k8sapplications/#weiterf%C3%BChrende-themen",
    
    "relUrl": "/gks/k8sapplications/#weiterführende-themen"
  },"206": {
    "doc": "Anwendungen in Kubernetes",
    "title": "Anwendungen in Kubernetes",
    "content": " ",
    "url": "/gks/k8sapplications/",
    
    "relUrl": "/gks/k8sapplications/"
  },"207": {
    "doc": "Eine Anwendung in Kubernetes starten",
    "title": "Eine Anwendung in Kubernetes starten",
    "content": "Der Cluster läuft und Sie wollen eine Applikation betreiben. Als Beispiel verwenden wir einen NGINX, der mittels eines Load Balancers auch außerhalb des Clusters zugreifbar gemacht wird. ",
    "url": "/gks/k8sapplications/runningapplications/#eine-anwendung-in-kubernetes-starten",
    
    "relUrl": "/gks/k8sapplications/runningapplications/#eine-anwendung-in-kubernetes-starten"
  },"208": {
    "doc": "Eine Anwendung in Kubernetes starten",
    "title": "Voraussetzungen",
    "content": "Damit Sie die folgenden Schritte erfolgreich abschließen können, brauchen Sie Folgendes: . | kubectl die neueste Version | Einen laufenden Kubernetes Cluster, von GKS erstellt mit laufender Machine Deployment. | Siehe hierzu auch: Einen Cluster anlegen | . | Eine valide Konfigdatei kubeconfig für den Cluster. | Siehe hierzu auch: Mit einem Cluster verbinden. | . | . ",
    "url": "/gks/k8sapplications/runningapplications/#voraussetzungen",
    
    "relUrl": "/gks/k8sapplications/runningapplications/#voraussetzungen"
  },"209": {
    "doc": "Eine Anwendung in Kubernetes starten",
    "title": "Datentypen",
    "content": "Kubernetes ist im Prinzip eine große Datenbank. Alle Dinge, die es betreibt, speichert der API-Server und dementsprechend lassen sich Applikationen in Kubernetes auch betreiben. Deployment . Der Datentyp Deployment kümmert sich darum, dass Applikationen in Kubernetes laufen und nach einigen Methoden wie zum Beispiel Rolling aktualisiert werden können. Ein Deployment müssen Sie auch in Ihrem NGINX Beispiel anlegen, damit Kubernetes Ihnen das Docker Image NGINX im Cluster platziert. Service . Ein Service in Kubernetes ist eine Zusammenfassung diverser Container, die im Cluster laufen. Das Matching geschieht auf Basis von Labels, die Sie an die Deployments hängen. Ein Service kann mehrere Typen haben. In diesem Beispiel wählen Sie LoadBalancer, damit unser Service extern über eine öffentliche IP-Adresse erreichbar ist. ",
    "url": "/gks/k8sapplications/runningapplications/#datentypen",
    
    "relUrl": "/gks/k8sapplications/runningapplications/#datentypen"
  },"210": {
    "doc": "Eine Anwendung in Kubernetes starten",
    "title": "Manifeste",
    "content": "Um NGINX auf Kubernetes laufen zu lassen, brauchen Sie zunächst ein Objekt vom Typ Deployment. Dies lässt sich mit kubectl leicht generieren. kubectl create deployment --dry-run -o yaml --image nginx nginx apiVersion: apps/v1 kind: Deployment metadata: creationTimestamp: null labels: app: nginx name: nginx spec: replicas: 1 selector: matchLabels: app: nginx strategy: {} template: metadata: creationTimestamp: null labels: app: nginx spec: containers: - image: nginx name: nginx resources: {} status: {} . Dies speichern Sie nun in eine Datei deployment.yaml. kubectl create deployment --dry-run -o yaml --image nginx nginx &gt; deployment.yaml . Als Nächstes benötigen Sie einen Service, der die Applikation von der Öffentlichkeit aus zugänglich macht. Als Typ wählen Sie LoadBalancer womit in OpenStack direkt ein fertig konfigurierter Loadbalancer als Einstieg in den Cluster erstellt wird. kubectl create service loadbalancer --dry-run --tcp=80 -o yaml nginx apiVersion: v1 kind: Service metadata: creationTimestamp: null labels: app: nginx name: nginx spec: ports: - name: \"80\" port: 80 protocol: TCP targetPort: 80 selector: app: nginx type: LoadBalancer status: loadBalancer: {} . Speichern Sie dies nun wieder in eine Datei, dieses Mal in service.yaml. kubectl create service loadbalancer --dry-run --tcp=80 -o yaml nginx &gt; service.yaml . Diese beiden Dateien sind die Grundlage für ein öffentliches NGINX in Kubernetes. Zusammen gehören diese zwei Manifeste nicht. Die einzige Verbindung ist das Label app: nginx, welches das Deployment in den Metadaten und der Service als Selektor definiert hat. ",
    "url": "/gks/k8sapplications/runningapplications/#manifeste",
    
    "relUrl": "/gks/k8sapplications/runningapplications/#manifeste"
  },"211": {
    "doc": "Eine Anwendung in Kubernetes starten",
    "title": "Erstellen der Applikation",
    "content": "Nun müssen Sie die zwei Dateien an die Kubernetes API schicken. kubectl apply -f deployment.yaml deployment.apps/nginx created kubectl apply -f service.yaml service/nginx created . Nun können Sie sich die zwei erstellten Objekte anschauen. kubectl get deployment,service NAME READY UP-TO-DATE AVAILABLE AGE deployment.extensions/nginx 1/1 1 1 55s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/kubernetes NodePort 10.10.10.1 &lt;none&gt; 443:31630/TCP 2d23h service/nginx LoadBalancer 10.10.10.86 &lt;pending&gt; 80:31762/TCP 46s . Wie Sie in der Ausgabe sehen, wurde das Deployment angelegt und ist im Zustand READY. Der Service NGINX wurde auch angelegt, die EXTERNAL-IP ist jedoch noch pending. Hier müssen Sie ein bisschen warten, bis der Loadbalancer provisioniert wurde. Nach etwa 1–2 Minuten können Sie das Kommando erneut ausführen und bekommen eine IP angezeigt: . kubectl get deployment,svc NAME READY UP-TO-DATE AVAILABLE AGE deployment.extensions/nginx 1/1 1 1 2m8s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/kubernetes NodePort 10.10.10.1 &lt;none&gt; 443:31630/TCP 2d23h service/nginx LoadBalancer 10.10.10.86 185.116.245.169 80:31762/TCP 119s . Die External IP 185.116.245.169 aus unserem Beispiel ist nun öffentlich erreichbar und zeigt Ihre NGINX-Instanz an. ",
    "url": "/gks/k8sapplications/runningapplications/#erstellen-der-applikation",
    
    "relUrl": "/gks/k8sapplications/runningapplications/#erstellen-der-applikation"
  },"212": {
    "doc": "Eine Anwendung in Kubernetes starten",
    "title": "Aufräumen",
    "content": "Sie können das ganze sehr einfach nun auch wieder löschen. kubectl delete -f service.yaml service \"nginx\" deleted kubectl delete -f deployment.yaml deployment.apps \"nginx\" deleted kubectl get deployment,svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/kubernetes NodePort 10.10.10.1 &lt;none&gt; 443:31630/TCP 2d23h . Wie Sie sehen, wurde alles entfernt. Wenn Sie die IP-Adresse im Browser noch einmal aufrufen, wird ein Fehler angezeigt: Die Applikation läuft nicht mehr. ",
    "url": "/gks/k8sapplications/runningapplications/#aufr%C3%A4umen",
    
    "relUrl": "/gks/k8sapplications/runningapplications/#aufräumen"
  },"213": {
    "doc": "Eine Anwendung in Kubernetes starten",
    "title": "Zusammenfassung",
    "content": "Herzlichen Glückwunsch! Sie haben folgende Schritte erfolgreich durchgeführt und gelernt: . | Wie spreche ich mit Kubernetes? | Was ist ein Deployment? | Wie lege ich ein Deployment an? | Was ist ein Service? | Wie lege ich einen Service an | Wie lösche ich die Applikation wieder? | . Dies sind alle notwendigen Schritte, um in Kubernetes Applikationen zu starten. ",
    "url": "/gks/k8sapplications/runningapplications/#zusammenfassung",
    
    "relUrl": "/gks/k8sapplications/runningapplications/#zusammenfassung"
  },"214": {
    "doc": "Eine Anwendung in Kubernetes starten",
    "title": "Eine Anwendung in Kubernetes starten",
    "content": " ",
    "url": "/gks/k8sapplications/runningapplications/",
    
    "relUrl": "/gks/k8sapplications/runningapplications/"
  },"215": {
    "doc": "Kubernetes Service Accounts verwalten",
    "title": "Kubernetes Service Accounts verwalten",
    "content": "Sie können eingeschränkten Zugriff auf Ihre Cluster mithilfe von Kubernetes Service Accounts und dem Kubernetes RBAC Feature umsetzen. Dafür müssen Sie: . | Einen Kubernetes Service Account anlegen | Eine Rolle mit beschränktem Zugriff definieren | Dem Kubernetes Service Account diese Rolle zuordnen | . Die Authentifizierung in Kubernetes Clustern, die mit GKS erzeugt werden, geschieht über sogenannte Bearer Token. Wenn Sie einen neuen Kubernetes Service Account anlegen, wird ein solches Token oder Secret im jeweiligen Namespace hinterlegt. Dieses Secret wird gelöscht, wenn der Kubernetes Service Account gelöscht wird. ",
    "url": "/gks/k8sapplications/serviceaccounts/#kubernetes-service-accounts-verwalten",
    
    "relUrl": "/gks/k8sapplications/serviceaccounts/#kubernetes-service-accounts-verwalten"
  },"216": {
    "doc": "Kubernetes Service Accounts verwalten",
    "title": "Anlegen eines Kubernetes Service Accounts",
    "content": "Um einen Kubernetes Service Account anzulegen, benutzen Sie das folgende Kommando. Ersetzen Sie dabei my-serviceaccount mit dem Namen, den Sie dem Kubernetes Service Account geben wollen. kubectl apply -f - &lt;&lt;EOF apiVersion: v1 kind: ServiceAccount metadata: name: my-serviceaccount namespace: my-namespace EOF . Dadurch wird im Cluster automatisch ein neues Access Token angelegt. Dieses Token trägt einen Namen my-serviceaccount-token-#### wobei die ‘#’ zufällige alphanumerische Zeichen sind. Um die Token in einem Namespace zu sehen, verwenden Sie das folgende Kommando: . kubectl get secrets --namespace=my-namespace . Sie können sich dann das eigentliche Token mit dem folgenden Kommando ausgeben lassen (vergessen Sie nicht, ‘$SECRETNAME’ durch den Namen zu ersetzen, der für Ihren Kubernetes Service Account gilt): . kubectl get secret $SECRETNAME -o jsonpath='{.data.token}' --namespace=my-namespace . Das angezeigte Token können Sie zusammen mit dem Namen des Serviceaccounts an Dritte übermitteln, um ihnen den Zugriff zu dem Cluster zu ermöglichen. Jetzt haben Sie einen Kubernetes Service Account, der sich an Ihrem Cluster authentifizieren kann, aber er hat noch keine Rechte. Im nächsten Schritt definieren Sie eine Rolle und weisen diese Rolle dem Kubernetes Service Account zu, damit er die entsprechenden Rechte erhält. ",
    "url": "/gks/k8sapplications/serviceaccounts/#anlegen-eines-kubernetes-service-accounts",
    
    "relUrl": "/gks/k8sapplications/serviceaccounts/#anlegen-eines-kubernetes-service-accounts"
  },"217": {
    "doc": "Kubernetes Service Accounts verwalten",
    "title": "Definition einer Rolle mit ihren Rechten",
    "content": "Grundsätzlich gibt es zwei Wege, einem Kubernetes Service Account Rechte zuzuweisen: Cluster-weite Rollen oder Rollen, die auf einen jeweiligen Namespace beschränkt sind. Da die Cluster-weiten Rollen Zugriff auf alle Namespaces geben, empfehlen wir, diese nur zu verwenden, wenn es absolut notwendig ist. Unsere Beispiele definieren Rollen, die auf einen jeweiligen Namespace beschränkt sind. Alle Rechte einer Rolle werden explizit freigegeben – das gilt sowohl für persönliche Zugänge als auch für Kubernetes Service Accounts. Wenn ein Account über mehrere Rollen verfügt, bekommt er die kombinierten Rechte aller Rollen. Um eine Rolle zu definieren, mit der ein Account die Informationen über Pods im Namespace my-namespace auslesen kann, verwenden Sie den folgenden Befehl: . kubectl apply -f - &lt;&lt;EOF apiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata: namespace: my-namespace name: pod-reader rules: - apiGroups: [\"\"] # \"\" indicates the core API group resources: [\"pods\"] verbs: [\"get\", \"watch\", \"list\"] EOF . Jetzt wollen Sie dem zuvor angelegten Kubernetes Service Account diese Rolle zuweisen. Für dieses role binding verwenden Sie das folgende Kommando: . kubectl create rolebinding read-pods \\ --role=pod-reader \\ --serviceaccount=my-namespace:my-serviceaccount \\ --namespace=my-namespace . Um alle Ressourcen aufzulisten, rufen Sie das folgende Kommando auf: . kubectl api-resources . Für die meisten Ressourcen sind folgende Verben definiert: . | get | list | watch | create | edit | update | delete | exec | . ",
    "url": "/gks/k8sapplications/serviceaccounts/#definition-einer-rolle-mit-ihren-rechten",
    
    "relUrl": "/gks/k8sapplications/serviceaccounts/#definition-einer-rolle-mit-ihren-rechten"
  },"218": {
    "doc": "Kubernetes Service Accounts verwalten",
    "title": "Weiterführende Themen",
    "content": "In der offiziellen Kubernetes-Dokumentation wird das Thema ausführlich behandelt: . | Zugriffskontrollen | Using roles and role bindings in RBAC | . ",
    "url": "/gks/k8sapplications/serviceaccounts/#weiterf%C3%BChrende-themen",
    
    "relUrl": "/gks/k8sapplications/serviceaccounts/#weiterführende-themen"
  },"219": {
    "doc": "Kubernetes Service Accounts verwalten",
    "title": "Zusammenfassung",
    "content": "In diesem Abschnitt haben Sie gelernt, wie Sie über die Kommandozeile: . | Kubernetes Service Accounts anlegen | Das automatisch generierte Bearer Token für einen Kubernetes Service Accounts auslesen | Eine Rolle mithilfe des Kubernetes RBAC Features definieren | Mithilfe eines role bindings solche Rollen einem Kubernetes Service Account zuordnen können | . ",
    "url": "/gks/k8sapplications/serviceaccounts/#zusammenfassung",
    
    "relUrl": "/gks/k8sapplications/serviceaccounts/#zusammenfassung"
  },"220": {
    "doc": "Kubernetes Service Accounts verwalten",
    "title": "Kubernetes Service Accounts verwalten",
    "content": " ",
    "url": "/gks/k8sapplications/serviceaccounts/",
    
    "relUrl": "/gks/k8sapplications/serviceaccounts/"
  },"221": {
    "doc": "ExternalDNS mit Designate",
    "title": "ExternalDNS mit Designate",
    "content": "Um den Aufwand zu reduzieren, und die manuelle Verwaltung Ihrer DNS-Zone zu minimieren, können Sie ExternalDNS verwenden. ExternalDNS ermöglicht Ihnen, DNS-Einträge dynamisch über Kubernetes je nach DNS-Anbieter zu steuern. ExternalDNS ist kein eigenständiger DNS-Server, sondern konfiguriert lediglich DNS-Ressourcen in externen DNS-Providern. Beispielsweise (OpenStack Designate, Amazon Route53, Google Cloud DNS, usw.) . ",
    "url": "/gks/k8sapplications/externaldnsanddesignate/#externaldns-mit-designate",
    
    "relUrl": "/gks/k8sapplications/externaldnsanddesignate/#externaldns-mit-designate"
  },"222": {
    "doc": "ExternalDNS mit Designate",
    "title": "Voraussetzungen",
    "content": "Um diesen Guide erfolgreich abzuschließen brauchen Sie Folgendes: . | kubectl die neueste Version | Einen laufenden Kubernetes Cluster, von GKS erstellt mit laufender Machine Deployment . | Siehe hierzu auch: Einen Cluster anlegen | . | Eine valide Konfigdatei kubeconfig für den Cluster . | Siehe hierzu auch: Mit einem Cluster verbinden | . | Installierte OpenStack-CLI Werkzeuge | Einen OpenStack-API-Zugang | . Konfigurieren Sie Ihre Domäne zur Verwendung von Designate . Delegieren Sie Ihre Domains von Ihrem externen DNS-Anbieter auf unsere folgenden DNS-Nameserver, damit Designate die DNS-Ressourcen Ihrer Domaine steuern kann. dns1.ddns.innovo.cloud dns2.ddns.innovo.cloud . ",
    "url": "/gks/k8sapplications/externaldnsanddesignate/#voraussetzungen",
    
    "relUrl": "/gks/k8sapplications/externaldnsanddesignate/#voraussetzungen"
  },"223": {
    "doc": "ExternalDNS mit Designate",
    "title": "DNS-Zone in OpenStack-Designate anlegen",
    "content": "Bevor Sie ExternalDNS nutzen, müssen Sie Ihre DNS-Zone manuell in Ihrem DNS-Provider anlegen. In unserem Beispiel wird die Zone foobar.cloud. in OpenStack-Designate mithilfe der OpenStack-CLI-Tools wie folgt angelegt: . Hinweis: Fügen Sie immer am Ende Ihrer Zone einen . an. $ openstack zone create --email webmaster@foobar.cloud foobar.cloud. +----------------+--------------------------------------+ | Field | Value | +----------------+--------------------------------------+ | action | CREATE | attributes | | created_at | 2018-08-15T06:45:24.000000 | description | None | email | webmaster@foobar.cloud | id | 036ae6e6-6318-47e1-920f-be518d845fb5 | masters | | name | foobar.cloud. | pool_id | bb031d0d-b8ca-455a-8963-50ec70fe57cf | project_id | 2b62bc8ff48445f394d0318dbd058967 | serial | 1534315524 | status | PENDING | transferred_at | None | ttl | 3600 | type | PRIMARY | updated_at | None | version | 1 | +----------------+--------------------------------------+ . Prüfen Sie, dass die Zone tatsächlich angelegt wurde. $ openstack zone list +--------------------------------------+-----------------------+---------+------------+--------+--------+ | id | name | type | serial | status | action | +--------------------------------------+-----------------------+---------+------------+--------+--------+ | 036ae6e6-6318-47e1-920f-be518d845fb5 | foobar.cloud. | PRIMARY | 1534315524 | ACTIVE | NONE | +--------------------------------------+-----------------------+---------+------------+--------+--------+ $ openstack zone show foobar.cloud. +----------------+--------------------------------------+ | Field | Value | +----------------+--------------------------------------+ | action | NONE | attributes | | created_at | 2018-08-15T06:45:24.000000 | description | None | email | webmaster@foobar.cloud | id | 036ae6e6-6318-47e1-920f-be518d845fb5 | masters | | name | foobar.cloud. | pool_id | bb031d0d-b8ca-455a-8963-50ec70fe57cf | project_id | 2b62bc8ff48445f394d0318dbd058967 | serial | 1534315524 | status | ACTIVE | transferred_at | None | ttl | 3600 | type | PRIMARY | updated_at | 2018-08-15T06:45:30.000000 | version | 2 | +----------------+--------------------------------------+ . ",
    "url": "/gks/k8sapplications/externaldnsanddesignate/#dns-zone-in-openstack-designate-anlegen",
    
    "relUrl": "/gks/k8sapplications/externaldnsanddesignate/#dns-zone-in-openstack-designate-anlegen"
  },"224": {
    "doc": "ExternalDNS mit Designate",
    "title": "Die Installation vom ExternalDNS über Helm",
    "content": "Installieren Sie ExternalDNS in Ihrem Cluster. In unserem Beispiel nutzen wir folgendes Helm für die Installation: . | Helm-Installation | $ helm repo add stable https://kubernetes-charts.storage.googleapis.com/ | $ helm repo update | Legen Sie die Datei values.yaml an und passen Sie die Werte der Datei an. Für weitere Optionen, siehe: values-external-dns. | . ## K8s resources type to be observed for new DNS entries by ExternalDNS ## sources: - service - ingress ## DNS provider where the DNS records will be created. Available providers are: ## - aws, azure, cloudflare, coredns, designate, digitalocoean, google, infoblox, rfc2136, transip ## provider: designate ## Adjust the interval for DNS updates ## interval: \"1m\" ## Registry Type. Available types are: txt, noop ## ref: https://github.com/kubernetes-incubator/external-dns/blob/master/docs/proposal/registry.md ## registry: \"txt\" ## TXT Registry Identifier, a name that identifies this instance of External-DNS ## This value should not change, while the cluter exists ## txtOwnerId: \"external-dns\" ## Modify how DNS records are sychronized between sources and providers (options: sync, upsert-only ) ## policy: sync ## Configure resource requests and limits ## ref: http://kubernetes.io/docs/user-guide/compute-resources/ ## resources: limits: memory: 50Mi cpu: 10m requests: memory: 50Mi cpu: 10m ## Configure your OS Access ## extraEnv: - name: OS_AUTH_URL value: https://identity.optimist.gec.io/v3 - name: OS_REGION_NAME value: fra - name: OS_USERNAME value: \"%YOUR_OPENSTACK_USERNAME%\" - name: OS_PASSWORD value: \"%YOUR_OPENSTACK_PASSWORD%\" - name: OS_PROJECT_NAME value: \"%YOUR_OPENSTACK_PROJECT_NAME%\" - name: OS_USER_DOMAIN_NAME value: Default . | $ kubectl create namespace external-dns | $ helm upgrade --install external-dns -f values.yaml stable/external-dns --namespace=external-dns | . ",
    "url": "/gks/k8sapplications/externaldnsanddesignate/#die-installation-vom-externaldns-%C3%BCber-helm",
    
    "relUrl": "/gks/k8sapplications/externaldnsanddesignate/#die-installation-vom-externaldns-über-helm"
  },"225": {
    "doc": "ExternalDNS mit Designate",
    "title": "NGINX Deployment starten",
    "content": "Um den Fully Qualified Domain Name (FQDN) bzw. die Domain zu testen, legen Sie beispielsweise dieses Deployment als nginx.yaml an: . apiVersion: apps/v1 kind: Deployment metadata: name: nginx spec: selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - image: nginx name: nginx ports: - containerPort: 80 --- apiVersion: v1 kind: Service metadata: name: nginx annotations: external-dns.alpha.kubernetes.io/hostname: nginx.foobar.cloud external-dns.alpha.kubernetes.io/ttl: \"60\" spec: selector: app: nginx type: LoadBalancer ports: - protocol: TCP port: 80 targetPort: 80 . Nachdem Sie diese Deklarationsdatei angelegt haben, wenden Sie diese an. kubectl apply -f nginx.yaml . Verifizieren Sie die Ergebnisse . Überprüfen Sie Ihre DNS-Ressourcen in OpenStack. Sie sollten eine ähnliche Liste mit den entsprechenden DNS-Einträgen sehen. openstack recordset list foobar.cloud. Warten Sie ein paar Minuten und testen Sie dann die Verfügbarkeit über das Internet. Rufen Sie Ihre Webseite auf. Dort sollten Sie Folgendes in Ihrem Browser sehen. ",
    "url": "/gks/k8sapplications/externaldnsanddesignate/#nginx-deployment-starten",
    
    "relUrl": "/gks/k8sapplications/externaldnsanddesignate/#nginx-deployment-starten"
  },"226": {
    "doc": "ExternalDNS mit Designate",
    "title": "Zusammenfassung",
    "content": "Sie haben folgende Inhaltspunkte erfolgreich abgeschlossen: . | Was ist ExternalDNS und wie ist diese im Kubernetes-Cluster zu installieren | Die Konfiguration vom ExternalDNS über Helm, um OpenStack Designate anzusprechen | Die Erstellung eines NGINX-Deployments und das Testen der Konnektivität | . Herzlichen Glückwunsch! Sie kennen nun alle notwendigen Schritte, um in Kubernetes dynamische DNS-Ressourcen zu steuern. ",
    "url": "/gks/k8sapplications/externaldnsanddesignate/#zusammenfassung",
    
    "relUrl": "/gks/k8sapplications/externaldnsanddesignate/#zusammenfassung"
  },"227": {
    "doc": "ExternalDNS mit Designate",
    "title": "ExternalDNS mit Designate",
    "content": " ",
    "url": "/gks/k8sapplications/externaldnsanddesignate/",
    
    "relUrl": "/gks/k8sapplications/externaldnsanddesignate/"
  },"228": {
    "doc": "Verwaltung der Zeitzonen",
    "title": "Verwaltung der Zeitzonen",
    "content": "Falls Ihre Anwendung Zeitzonenanpassung benötigt, können Sie diesen Guide befolgen, um diese in Kubernetes zu steuern. ",
    "url": "/gks/k8sapplications/timezones/#verwaltung-der-zeitzonen",
    
    "relUrl": "/gks/k8sapplications/timezones/#verwaltung-der-zeitzonen"
  },"229": {
    "doc": "Verwaltung der Zeitzonen",
    "title": "Voraussetzungen",
    "content": "Um diesen Guide erfolgreich abzuschließen brauchen Sie Folgendes: . | kubectl die neueste Version | Ein laufender Kubernetes Cluster, von GKS erstellt mit laufender Machine Deployment . | Siehe hierzu auch: Einen Cluster anlegen | . | Eine valide Konfigdatei kubeconfig für den Cluster . | Siehe hierzu auch: Mit einem Cluster verbinden | . | . ",
    "url": "/gks/k8sapplications/timezones/#voraussetzungen",
    
    "relUrl": "/gks/k8sapplications/timezones/#voraussetzungen"
  },"230": {
    "doc": "Verwaltung der Zeitzonen",
    "title": "Zeitzonen in einer Kubernetes-Umgebung",
    "content": "Das Standardverhalten im Kubernetes-Cluster . Kubernetes-Cluster erben die Zeitzonenkonfiguration von dem Workernode, so dass die Standardzeitzone im Kubernetes-Cluster die gleiche ist, wie die des Workernodes – diese wird also vom Kernel gesteuert. Pod &amp; Container . Während es nicht trivial ist, die Zeitzone auf Clusterebene zu ändern, es gibt eine einfache Möglichkeit, dies auf Pod- und Containerebene zu bestimmen. Um dieses Ziel zu erreichen, sollten Sie das folgende Szenario befolgen: . Szenario . Starten Sie einen Testcontainer . Starten Sie einen minimalen Container z. B. ein Container mit Busybox, wie folgendes: . apiVersion: v1 kind: Pod metadata: name: busybox-sleep spec: containers: - name: busybox image: busybox args: - sleep - \"100000\" . Legen Sie die oben genannte Datei z. B. als busybox.yaml an und wenden Sie diese Pod-Deklaration an: . kubectl apply -f busybox.yaml . Warten Sie einen Augenblick, bis Ihr Deployment gestartet ist und fragen Sie die Zeit vom Container ab: . kubectl exec busybox-sleep -it -- date . Sie sollten einen ähnlichen Output erhalten, in dem z. B. die Zeitzone UTC zu sehen ist: . Wed Feb 26 10:57:53 UTC 2020 . Die Zeitzone auf dem Testcontainer ändern . Folgendermaßen können Sie die Zeitzone auf Pod bzw. Containerebene ändern: . Teilen Sie Ihre gewünschte zoneinfo Datei dem Pod bzw. Container mit, indem Sie diese von der Workernode in den Container einbinden. Beispielhaft folgt die Umstellung auf Berliner Zeitzone: . apiVersion: v1 kind: Pod metadata: name: busybox-sleep-2 spec: containers: - name: busybox image: busybox args: - sleep - \"100000\" volumeMounts: - name: timezone-config mountPath: /etc/localtime volumes: - name: timezone-config hostPath: path: /usr/share/zoneinfo/Europe/Berlin . Grundsätzlich ist es die selbe Deklarationsdatei, aber ergänzt um ein Volume namens timezone-config, welches die gewünschte Zonendatei dem Container bereitstellt. Speichern Sie die oben genannte Datei als busybox-2.yaml und wenden Sie diese Pod-Deklaration in Kubernetes an: . kubectl apply -f busybox-2.yaml . Nachdem Sie gewartet haben, bis das Deployment gestartet ist, fragen Sie die Zeit vom neuen Container ab: . kubectl exec busybox-sleep-2 -it -- date . Sie sollten einen Output erhalten, in dem die Berliner Zeitzone zu sehen ist: . Wed Feb 26 11:01:04 CET 2020 . ",
    "url": "/gks/k8sapplications/timezones/#zeitzonen-in-einer-kubernetes-umgebung",
    
    "relUrl": "/gks/k8sapplications/timezones/#zeitzonen-in-einer-kubernetes-umgebung"
  },"231": {
    "doc": "Verwaltung der Zeitzonen",
    "title": "Zusammenfassung",
    "content": "Folgende Schritte wurden erfolgreich durchgeführt und gelernt: . | Wie man die Zeit aus dem Container abfragt. | Wie man die zoneinfo bzw. localtime in den Container einbinden kann, so dass die Zone geändert wird. | . Herzlichen Glückwunsch! Dies sind alle notwendigen Schritte, um in Kubernetes die Zeitzone auf Pod bzw. Containerebene zu ändern. ",
    "url": "/gks/k8sapplications/timezones/#zusammenfassung",
    
    "relUrl": "/gks/k8sapplications/timezones/#zusammenfassung"
  },"232": {
    "doc": "Verwaltung der Zeitzonen",
    "title": "Verwaltung der Zeitzonen",
    "content": " ",
    "url": "/gks/k8sapplications/timezones/",
    
    "relUrl": "/gks/k8sapplications/timezones/"
  },"233": {
    "doc": "StorageClass Setup",
    "title": "StorageClass Setup",
    "content": "Es gibt eine vorinstalliere Default Storage Class pro Cluster. Achtung: Diese wird von GKS verwaltet und kann jederzeit überschrieben werden. Bitte erstellen Sie für Änderungen eine eigene Storage Class. kubectl get storageclasses.storage.k8s.io NAME PROVISIONER AGE standard (default) kubernetes.io/cinder 268d . kubectl get storageclasses.storage.k8s.io NAME PROVISIONER AGE cinder-csi (default) cinder.csi.openstack.org 6h45m . Der Provisioner ist abhängig vom Erstellungszeitpunkt des Clusters und der Kubernetes Version. | kubernetes.io/cinder alle Kubernetes Cluster kleiner 1.16 und vor dem 29.10 angelegt. | cinder.csi.openstack.org alle Kubernetes Cluster 1.16+ und nach dem 19.10 angelegt. | . ",
    "url": "/gks/k8sapplications/storageclasses/#storageclass-setup",
    
    "relUrl": "/gks/k8sapplications/storageclasses/#storageclass-setup"
  },"234": {
    "doc": "StorageClass Setup",
    "title": "Openstack Volume Types",
    "content": "Die Openstack Volume Types nach maximal möglichen IOPS sortiert: . | low-iops | default &lt;- wird in der Default Class verwendet | high-iops | . Eine eigene Klasse anlegen . Wenn eine der beiden anderen Typen benötigt wird, können Sie sich eigene Definitionen anlegen. Beispiel: . apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: my-high-iops-class provisioner: cinder.csi.openstack.org parameters: type: high-iops . Zum Anlegen verwenden Sie das Kommando: kubectl apply -f storage-class.yaml . | name: Hier muss ein eigener Name verwendet werden, damit es nicht mit den Standardklassen kollidiert. | provisioner: Benutzt immer den aktuellen Provisioner. Man findet ihn in der Standardklasse. | type: Benutzt immer eine offiziell vom Optimist unterstützten Disk-Type (aktuell low-iops and high-iops). | . Um die neue Klasse zu verwenden, passen Sie die Volume Definition mit dem neuen Namen an. ",
    "url": "/gks/k8sapplications/storageclasses/#openstack-volume-types",
    
    "relUrl": "/gks/k8sapplications/storageclasses/#openstack-volume-types"
  },"235": {
    "doc": "StorageClass Setup",
    "title": "StorageClass Setup",
    "content": " ",
    "url": "/gks/k8sapplications/storageclasses/",
    
    "relUrl": "/gks/k8sapplications/storageclasses/"
  },"236": {
    "doc": "Add-On",
    "title": "Add-On",
    "content": "Innerhalb von GKS können spezielle, kuratierte Applikationen als Add-ons betrieben werden. In diesem Abschnitt zeigen wir, wie Sie auf der GKS Plattform diese Add-ons installieren und verwalten können. ",
    "url": "/gks/addons/#add-on",
    
    "relUrl": "/gks/addons/#add-on"
  },"237": {
    "doc": "Add-On",
    "title": "Weiterführende Themen",
    "content": ". | Cluster Autoscaler | . ",
    "url": "/gks/addons/#weiterf%C3%BChrende-themen",
    
    "relUrl": "/gks/addons/#weiterführende-themen"
  },"238": {
    "doc": "Add-On",
    "title": "Add-On",
    "content": " ",
    "url": "/gks/addons/",
    
    "relUrl": "/gks/addons/"
  },"239": {
    "doc": "Cluster Autoscaler",
    "title": "Cluster Autoscaler",
    "content": " ",
    "url": "/gks/addons/cluster-autoscaler/#cluster-autoscaler",
    
    "relUrl": "/gks/addons/cluster-autoscaler/#cluster-autoscaler"
  },"240": {
    "doc": "Cluster Autoscaler",
    "title": "Was ist ein Cluster Autoscaler in Kubernetes?",
    "content": "Der Kubernetes Cluster Autoscaler ist ein Tool, das die Anzahl der Worker-Nodes je nach Verbrauch automatisch nach oben oder unten anpasst. Das bedeutet, dass der Autoscaler zum Beispiel einen Cluster automatisch hochskaliert, indem er die Anzahl der Nodes erhöht, wenn nicht genügend Node-Ressourcen für das Cluster-Workload-Scheduling vorhanden sind. Aber auch herunterskaliert, wenn die Node-Ressourcen ständig im Leerlauf sind oder mehr als genügend Node-Ressourcen für das Cluster-Workload-Scheduling vorhanden sind. Kurz gesagt handelt es sich um eine Komponente, die die Größe eines Kubernetes-Clusters automatisch so anpasst, dass alle Pods einen Platz zum Ausführen haben und keine überflüssigen Nodes vorhanden sind. ",
    "url": "/gks/addons/cluster-autoscaler/#was-ist-ein-cluster-autoscaler-in-kubernetes",
    
    "relUrl": "/gks/addons/cluster-autoscaler/#was-ist-ein-cluster-autoscaler-in-kubernetes"
  },"241": {
    "doc": "Cluster Autoscaler",
    "title": "Cluster-Autoscaler-Verwendung",
    "content": "Der Kubernetes-Autoscaler im GKS Cluster skaliert automatisch nach oben oder unten, wenn eine der folgenden Bedingungen erfüllt ist: . | Einige Pods konnten im Cluster aufgrund unzureichender Ressourcen nicht ausgeführt werden. | Es gibt Nodes im Cluster, die über einen längeren Zeitraum (standardmäßig 10 Minuten) nicht ausgelastet waren und ihre Pods auf anderen vorhandenen Nodes platzieren können. | . ",
    "url": "/gks/addons/cluster-autoscaler/#cluster-autoscaler-verwendung",
    
    "relUrl": "/gks/addons/cluster-autoscaler/#cluster-autoscaler-verwendung"
  },"242": {
    "doc": "Cluster Autoscaler",
    "title": "Anforderungen",
    "content": "Die Verwendung eines Kubernetes Cluster Autoscalers im GKS Cluster muss bestimmte Mindestanforderungen erfüllen: . | Kubernetes-Cluster mit Kubernetes v1.18 oder neuer ist erforderlich | . ",
    "url": "/gks/addons/cluster-autoscaler/#anforderungen",
    
    "relUrl": "/gks/addons/cluster-autoscaler/#anforderungen"
  },"243": {
    "doc": "Cluster Autoscaler",
    "title": "Installieren von Kubernetes-Autoscaler auf GKS Cluster",
    "content": "Sie können den Kubernetes Autoscaler auf einem laufenden GKS Cluster mithilfe des GKS Add-on Mechanismus installieren, der bereits in das GKS Cluster Dashboard integriert ist. Schritt 1 . Erstellen Sie einen GKS Cluster, indem Sie Ihr Projekt auf dem Dashboard auswählen und auf Create Cluster klicken. Weitere Details finden Sie auf unserer Dokumentationsseite. Schritt 2 . Wenn der Cluster bereit ist, überprüfen Sie die Pods im kube-system Namespace, um festzustellen, ob ein Autoscaler läuft. $ kubectl get deployment -n kube-system NAME READY UP-TO-DATE AVAILABLE AGE coredns 2/2 2 2 1d flatcar-linux-update-operator 1/1 1 1 1d openvpn-client 1/1 1 1 1d . Wie oben dargestellt, ist der Autoscaler nicht Teil der laufenden Kubernetes-Komponenten innerhalb des Namspaces. Schritt 3 . Fügen Sie den Autoscaler zum Cluster hinzu, indem Sie im Dashboard im Bereich Addons auf Install Addon klicken. Wählen Sie cluster-autoscaler aus. Klicken Sie auf Install : . Schritt 4 . Gehen Sie zum Cluster und überprüfen Sie die Pods im kube-system Namespace mit dem folgenden kubectl Kommando: . $ kubectl get deployment -n kube-system NAME READY UP-TO-DATE AVAILABLE AGE cluster-autoscaler 1/1 1 1 6m27s coredns 2/2 2 2 1d flatcar-linux-update-operator 1/1 1 1 1d openvpn-client 1/1 1 1 1d . Wie oben dargestellt, wurde der Autoscaler provisioniert und läuft. ",
    "url": "/gks/addons/cluster-autoscaler/#installieren-von-kubernetes-autoscaler-auf-gks-cluster",
    
    "relUrl": "/gks/addons/cluster-autoscaler/#installieren-von-kubernetes-autoscaler-auf-gks-cluster"
  },"244": {
    "doc": "Cluster Autoscaler",
    "title": "Machine Deployments für die automatische Skalierung mit Anmerkungen versehen",
    "content": "Der Cluster-Autoscaler berücksichtigt nur Machine Deployments mit gültigen Annotationen. Die Annotationen werden verwendet, um die minimale und maximale Anzahl von Replikas pro Machine Deployment zu steuern. Sie müssen diese Anmerkungen nicht auf alle Machine Deployment-Objekte anwenden, sondern nur auf Machine Deployments, die der Cluster Autoscaler berücksichtigen soll. cluster.k8s.io/cluster-api-autoscaler-node-group-min-size - die kleinste Anzahl an Worker-Nodes im Cluster (muss größer als Null sein!) cluster.k8s.io/cluster-api-autoscaler-node-group-max-size - die höchste Anzahl an Worker-Nodes im Cluster . Sie können die Anmerkungen auf Machine Deployments anwenden, sobald der Cluster bereitgestellt ist und die Machine Deployments erstellt und ausgeführt werden. Führen Sie die folgenden Schritte aus: . Schritt 1 . Führen Sie den folgenden kubectl Befehl aus, um die verfügbaren Machine Deployments zu überprüfen: . $ kubectl get machinedeployments -n kube-system NAME AGE DELETED REPLICAS AVAILABLEREPLICAS PROVIDER OS VERSION epic-goldwasser-worker-289mgt 1d 2 2 openstack flatcar 1.21.5 . Schritt 2 . Der Annotatierungsbefehl wird zusammen mit einer der obigen Machine Deployments verwendet, um die gewünschten Machine Deployments mit Anmerkungen zu versehen. In diesem Fall wird test-cluster-worker-v5drmq annotiert, um das Minimum und Maximum festzulegen. Minimum Annotation: . $ kubectl annotate machinedeployment -n kube-system epic-goldwasser-worker-289mgt cluster.k8s.io/cluster-api-autoscaler-node-group-min-size=\"1\" machinedeployment.cluster.k8s.io/epic-goldwasser-worker-289mgt annotated . Maximum Annotation: . $ kubectl annotate machinedeployment -n kube-system epic-goldwasser-worker-289mgt cluster.k8s.io/cluster-api-autoscaler-node-group-max-size=\"5\" machinedeployment.cluster.k8s.io/epic-goldwasser-worker-289mgt annotated . Schritt 3 . Überprüfen Sie die Beschreibung von Machine Deployment. $ kubectl describe machinedeployments -n kube-system epic-goldwasser-worker-289mgt Name: epic-goldwasser-worker-289mgt Namespace: kube-system Labels: &lt;none&gt; Annotations: cluster.k8s.io/cluster-api-autoscaler-node-group-max-size: 5 cluster.k8s.io/cluster-api-autoscaler-node-group-min-size: 1 machinedeployment.clusters.k8s.io/revision: 1 API Version: cluster.k8s.io/v1alpha1 Kind: MachineDeployment Metadata: Creation Timestamp: 2021-10-04T09:44:48Z Finalizers: foregroundDeletion Generation: 1 Managed Fields: API Version: cluster.k8s.io/v1alpha1 Fields Type: FieldsV1 [...] . Wie oben gezeigt, wurde die Machine Deployment mit einem Minimum von 1 und einem Maximum von 5 annotiert. Daher wird der Autoscaler nur die kommentierte Machine Deployment auf dem Cluster berücksichtigen. ",
    "url": "/gks/addons/cluster-autoscaler/#machine-deployments-f%C3%BCr-die-automatische-skalierung-mit-anmerkungen-versehen",
    
    "relUrl": "/gks/addons/cluster-autoscaler/#machine-deployments-für-die-automatische-skalierung-mit-anmerkungen-versehen"
  },"245": {
    "doc": "Cluster Autoscaler",
    "title": "Autoscaler deinstallieren",
    "content": "Um den Autoscaler zu deinstallieren, klicken Sie im Abschnitt Addons des Cluster-Dashboards auf die drei Punkte vor dem Cluster-Autoscaler und wählen Sie Delete. Nach dem Löschen können Sie den Cluster mit dem Befehl kubectl get deployment -n kube-system überprüfen, um sicherzustellen, dass der Autoscaler gelöscht wurde. ",
    "url": "/gks/addons/cluster-autoscaler/#autoscaler-deinstallieren",
    
    "relUrl": "/gks/addons/cluster-autoscaler/#autoscaler-deinstallieren"
  },"246": {
    "doc": "Cluster Autoscaler",
    "title": "Zusammenfassung",
    "content": "Sie haben erfolgreich einen Kubernetes Autoscaler auf einem GKS Cluster bereitgestellt und das gewünschte Machine Deployment annotiert, so dass der Autoscaler es berücksichtigen kann. ",
    "url": "/gks/addons/cluster-autoscaler/#zusammenfassung",
    
    "relUrl": "/gks/addons/cluster-autoscaler/#zusammenfassung"
  },"247": {
    "doc": "Cluster Autoscaler",
    "title": "Weiterführende Themen",
    "content": "Erfahren Sie mehr zum Kubernetes Autoscaler hier. ",
    "url": "/gks/addons/cluster-autoscaler/#weiterf%C3%BChrende-themen",
    
    "relUrl": "/gks/addons/cluster-autoscaler/#weiterführende-themen"
  },"248": {
    "doc": "Cluster Autoscaler",
    "title": "Cluster Autoscaler",
    "content": " ",
    "url": "/gks/addons/cluster-autoscaler/",
    
    "relUrl": "/gks/addons/cluster-autoscaler/"
  },"249": {
    "doc": "Produkt Übersicht",
    "title": "Produkt Übersicht",
    "content": "Die ONCITE Open Edition bietet eine benutzerfreundliche virtuelle Maschine (VM) und eine Container-Plattform auf Open-Source-Basis. Sie ist skalierbar von der Größe einer Edge bis zur Größe eines Rechenzentrums und bietet volle Datenhoheit sowie Echtzeitfähigkeiten. ",
    "url": "/edge/productoverview/",
    
    "relUrl": "/edge/productoverview/"
  },"250": {
    "doc": "Produkt Übersicht",
    "title": "Architektur",
    "content": "Um virtuelle Maschinen auf der Edge bereitzustellen, verwenden wir OpenStack. Der bereitgestellte Speicher wird von CEPH verwaltet. Um die virtuellen Maschinen zu erstellen und zu verwalten, kann der Kunde das OperationsCenter verwenden, das über LDAP für Single Sign-On verbunden ist. Administratoren können das OpenStack Horizon verwenden, das nicht mit LDAP verbunden ist. ",
    "url": "/edge/productoverview/#architektur",
    
    "relUrl": "/edge/productoverview/#architektur"
  },"251": {
    "doc": "Operations Center",
    "title": "Operations Center",
    "content": " ",
    "url": "/edge/operationscenter/",
    
    "relUrl": "/edge/operationscenter/"
  },"252": {
    "doc": "VPN as a Service",
    "title": "VPN as a Service",
    "content": " ",
    "url": "/edge/operationscenter/vpnaas/",
    
    "relUrl": "/edge/operationscenter/vpnaas/"
  },"253": {
    "doc": "VPN as a Service",
    "title": "Übersicht",
    "content": "Dieser Abschnitt veranschaulicht, wie die Nutzer mit dem Kundenprojekte von OpenStack über das OpenStack-Projekt-VPN-Gateway kommunizieren. Das OpenStack-Projekt-VPN-Gateway verwendet eine einzige öffentliche IP-Adresse. Es authentifiziert und leitet den Datenverkehr über verschiedene Ports zu den jeweiligen VPN-Servern weiter. Typischerweise hat jedes Projekt seinen eigenen VPN-Server. In dieser Konfiguration hat jedes VPN-Gateway eine einzige öffentliche IP-Adresse, die für alle VPN-Server gültig ist. ",
    "url": "/edge/operationscenter/vpnaas/#%C3%BCbersicht",
    
    "relUrl": "/edge/operationscenter/vpnaas/#übersicht"
  },"254": {
    "doc": "VPN as a Service",
    "title": "Zweck",
    "content": "GEC bietet eine VPNaaS-Lösung an, die es dem Kunden ermöglicht, ihre Anwendungen und Systeme mit den Systemen von Projektpartner zu integrieren. Externer Partner können Einzelpersonen sein, die mit ihren Computern oder Kommunikationssystemen auf das Projekt zugreifen. ",
    "url": "/edge/operationscenter/vpnaas/#zweck",
    
    "relUrl": "/edge/operationscenter/vpnaas/#zweck"
  },"255": {
    "doc": "VPN as a Service",
    "title": "Anforderungen",
    "content": "Die VPNaaS-Lösung hat folgende Anforderungen: . | Das Gateway benötigt eine öffentlich erreichbare IP welche mit der Floating-IP verbunden ist. | . ",
    "url": "/edge/operationscenter/vpnaas/#anforderungen",
    
    "relUrl": "/edge/operationscenter/vpnaas/#anforderungen"
  },"256": {
    "doc": "VPN as a Service",
    "title": "Einschränkungen",
    "content": "Die VPNaaS-Lösung unterliegt den folgenden Einschränkungen: . | Die Anzahl der VPN-Server wird durch den vordefinierten Portbereich im Gateway begrenzt. | Das Netzwerk unterstützt bis zu 250 Benutzer. | Konfigurationsänderungen am Gateway und Server sind nach der Erstellung nicht möglich; sie müssen neu erstellt werden. | Nach der Neuerstellung werden alle OVPN-Konfigurationen ungültig. | . ",
    "url": "/edge/operationscenter/vpnaas/#einschr%C3%A4nkungen",
    
    "relUrl": "/edge/operationscenter/vpnaas/#einschränkungen"
  },"257": {
    "doc": "VPN as a Service",
    "title": "Komponenten und Kommunikationsfluss",
    "content": "Dieser Abschnitt umreißt den Kommunikationsfluss zwischen dem Operations Center und den Kunden-VPN-Servern. Der Kunde initiiert eine Anfrage, und das VPN-Gateway authentifiziert und leitet die Anfragen an den jeweiligen VPN-Server weiter. VPN Gateway . Das VPN-Gateway dient als zentrale Managementzentrale für GEC. Es verwaltet VPN-Server und die Externe Verbindungen. Mit nur einer öffentlichen IP-Adresse stellen IPtable-Regeln sicher, dass Verbindungen zum richtigen VPN-Server geleitet werden. Diese Einrichtung erfordert ein Wide Area Network (WAN) und ein mit dem VPN-Gateway verbundenes VPN-Transfernetzwerk. Das VPN-Gateway verwaltet ausschließlich VPN-Server und Proxy-Verbindungen zu und von diesen Servern, um sicherzustellen, dass der Datenverkehr sein beabsichtigtes Ziel erreicht. VPN Server . Der VPN-Server verwendet OpenVPN, um VPN-Verbindungen mit dem Kundennetzwerk herzustellen. Diese Netzwerkverbindung arbeitet in einem eigenen Virtual Routing and Forwarding (VRF), um die Isolation zu verbessern, ohne zusätzliche Ports freizugeben. Externe Verbindungen werden über das VPN-Gateway geroutet. Der API-Server für den VPN-Server verwaltet den auf der Maschine laufenden OpenVPN-Server. ",
    "url": "/edge/operationscenter/vpnaas/#komponenten-und-kommunikationsfluss",
    
    "relUrl": "/edge/operationscenter/vpnaas/#komponenten-und-kommunikationsfluss"
  },"258": {
    "doc": "VPN as a Service",
    "title": "Sicherheit",
    "content": "WAN . | Nur die für die bestehende VPN-Server erforderlichen Ports sind geöffnet. | SSH ist deaktiviert. | Virtual Routing and Forwarding (VRF)-Trennung ist verfügbar. | Sicherheitsgruppen erlauben nur die VPN-Server-Portbereiche. | Firewall-Regeln sind vorhanden und werden bei Bedarf angepasst. | . VPNaaS . | Separate VRF wird für WAN verwendet. | Firewall-Regeln sind vorhanden und werden bei Bedarf angepasst. | SSH- und API-Zugriff werden bereitgestellt. | SSH-Schlüsselauthentifizierung wird verwendet. | . ",
    "url": "/edge/operationscenter/vpnaas/#sicherheit",
    
    "relUrl": "/edge/operationscenter/vpnaas/#sicherheit"
  },"259": {
    "doc": "VPN as a Service",
    "title": "Server- und Benutzerverwaltung",
    "content": "Serverstatus . | Klicken Sie auf die VPN-Ressource. | Überprüfen Sie den Status unter Eigenschaften. Die Konfigurationsbearbeitung ist nur möglich, wenn der Status aktiv ist. | . Benutzer hinzufügen . Hinweis: Der Projektinhaber wird automatisch für den Remotezugriff hinzugefügt und kann nicht entfernt werden. Voraussetzungen: Sie können nur Benutzer auswählen, die zuvor dem Projekt hinzugefügt wurden. | Gehen Sie zur VPN-Konfiguration. | Wählen Sie den gewünschten Benutzer aus, um ihn als VPN-Benutzer hinzuzufügen. | Speichern Sie die Auswahl. Der Benutzer wird sofort hinzugefügt. | . OVPN-Konfiguration und Passphrase erhalten . Hinweis: Sie können die OVPN-Konfiguration nur für sich selbst oder als Projektinhaber für externe Benutzer herunterladen und eine Passphrase anzeigen. Sie können die OVPN-Konfiguration oder Passphrasen für andere Benutzer nicht herunterladen. | Gehen Sie zur VPN-Konfiguration. | Klicken Sie auf den Welt-Icon-Link und wählen Sie zwischen Download und Passphrase. | . Benutzer löschen . | Klicken Sie auf das Löschsymbol neben dem Benutzer, den Sie löschen möchten. | Klicken Sie auf Speichern. | . ",
    "url": "/edge/operationscenter/vpnaas/#server--und-benutzerverwaltung",
    
    "relUrl": "/edge/operationscenter/vpnaas/#server--und-benutzerverwaltung"
  },"260": {
    "doc": "Openstack",
    "title": "Openstack",
    "content": "Openstack ist die Software die genutzt wird für den Infrastructure as a Service Layer. Alle VMs werden im Openstack erstellt, Openstack ist direkt mit CEPH verbunden worüber der Storage angeboten wird. ",
    "url": "/edge/openstack/",
    
    "relUrl": "/edge/openstack/"
  },"261": {
    "doc": "Openstack",
    "title": "Openstack - Horizon",
    "content": "Das Dashboard für Openstack ist Horizon. Horizon Login . Alle Administratoren erhalten bei der Übergabe einen Login. Horizon Instances . In der Instance Übersicht kann man die VMs ansehen die im ausgewählten Projekt laufen. Horizon Network . Im reiter Network kann man die Netzweke im aktuell Projekt sehen und die Provider Netzwerke. Die Provider Netzwerke sind die Netzwerke welche die VMs zum Kunden Netzwerk verbinden. Desweiteren kann man sich die Netzwerk Topologie ansehen, wie die VMs verbunden sind und wie die Netzwerke verbunden sind. Horizon New Network . Um ein neues Netzwerk anzulegen muss im Reiter “Network” auf “Create Network” geklickt werden. Dann kann man den Namen auswählen. Und man muss ein Subnet erstellen. Um mit dem Netzwerk zu arbeiten oder das Netzwerk nach aussen zu verbinden muss ein Router erstellt werden. Der Router muss dann mit einem Netzwerk verbunden werden. Das passiert durch die Erstellung eines Interfaces im Router, welches den Router dann mit einem Netzwerk verbindet. Um zwei Netzwerke zu verbinden braucht der Router ein Interface in beiden Netzwerken . ",
    "url": "/edge/openstack/#openstack---horizon",
    
    "relUrl": "/edge/openstack/#openstack---horizon"
  },"262": {
    "doc": "VM Erstellung",
    "title": "Erstellung einer VM",
    "content": "Zum erstellen einer neue Virtuele Machine im Openstack muss man sich im Openstack Dashboard einloggen. Diesen Zugriff haben nur Administratoren. Nach dem einloggen, können neue VMs im Reiter Instances erstellt werden. Man klickt auf “Launch Instance” um mit einen Wizard durch die Erstellung der VM durch geführt zu werden. ",
    "url": "/edge/openstack/create_vm/#erstellung-einer-vm",
    
    "relUrl": "/edge/openstack/create_vm/#erstellung-einer-vm"
  },"263": {
    "doc": "VM Erstellung",
    "title": "Details",
    "content": "Hier kann der Name ausgesucht werden, welche die VM erhält. Desweiteren kann die Anzahl der VMs geändert werden, der standard ist eine. ",
    "url": "/edge/openstack/create_vm/#details",
    
    "relUrl": "/edge/openstack/create_vm/#details"
  },"264": {
    "doc": "VM Erstellung",
    "title": "Source",
    "content": "Hier kann ausgewählt werden ob die Instance aus einem existierenden Volume erstellt werden soll. Oder ob die VM aus einem Image erstellt werden soll. Desweiteren kann eingestellt werden ob für die VM ein neues Volume erstellt werden soll und ob dieses Volume nach der Löschung der VM mit gelöscht werden soll. ",
    "url": "/edge/openstack/create_vm/#source",
    
    "relUrl": "/edge/openstack/create_vm/#source"
  },"265": {
    "doc": "VM Erstellung",
    "title": "Flavour",
    "content": "Hier kann eingestellt werden welche größe die VM haben soll und ob diese gegebenfalls mit einer Grafikkarte oder TSN Karte ausgestattet werden soll. ",
    "url": "/edge/openstack/create_vm/#flavour",
    
    "relUrl": "/edge/openstack/create_vm/#flavour"
  },"266": {
    "doc": "VM Erstellung",
    "title": "Network",
    "content": "Hier kann das Netzwerk ausgewählt werden mit welcher die VM bei der Erstellung verbunden ist. ",
    "url": "/edge/openstack/create_vm/#network",
    
    "relUrl": "/edge/openstack/create_vm/#network"
  },"267": {
    "doc": "VM Erstellung",
    "title": "Key Pair",
    "content": "Hier kann der SSH Key ausgewählt werden, mit welchen man sich über SSH verbinden kann. Desweiteren können hier neue SSH keys importiert werden. ",
    "url": "/edge/openstack/create_vm/#key-pair",
    
    "relUrl": "/edge/openstack/create_vm/#key-pair"
  },"268": {
    "doc": "VM Erstellung",
    "title": "VM Erstellung",
    "content": " ",
    "url": "/edge/openstack/create_vm/",
    
    "relUrl": "/edge/openstack/create_vm/"
  },"269": {
    "doc": "FAQ",
    "title": "FAQ",
    "content": " ",
    "url": "/edge/faq/",
    
    "relUrl": "/edge/faq/"
  },"270": {
    "doc": "FAQ",
    "title": "Openstack",
    "content": "Wie werden User hinzugefügt? . Aktuell nur auf Anfrage per Jira Helpdesk Ticket da jeder Openstack User auch Admin ist. Wie kann man VM’s löschen von anderen Usern/Projekten? Im Horizon dashboard kann man über den Adminbereich auf alle VMs zugreifen und diese löschen. Sonst ist der Zugriff auch über die Openstack CLI möglich . Wie kann die Plattengröße einer bestehenden VM erweitert werden? . Dies ist in der aktuellen Openstack Version nicht möglich. Welche Subnetze gibt es . Die folgenden Subnetze sind immer vorhanden: . | public | shared | private/internal | . Auf Anfrage kann ein viertes Netzwerk, Labor, für eine direkte Verbindung ohne die Barracuda Firewall erstellt werden. Wie erfolgt die Trennung der Subnetze: . Subnetze werden über OpenVSwitch virtualisiert zur garantierten Trennung können Securityregeln angelegt werden. Diese Regeln sind durch IPFilter Regeln in OpenStack Neutron dar. Die Einhaltung von Zonen wird im Operations Center über Sicherheitsregeln und Sicherheitsgruppen verwaltet. ",
    "url": "/edge/faq/#openstack",
    
    "relUrl": "/edge/faq/#openstack"
  },"271": {
    "doc": "FAQ",
    "title": "Operations Center",
    "content": "Wie werden User hinzugefügt im Operations Center? . Der Zugriff auf das Operations Center wird über SSO realisiert. Nachdem Benutzer sich angemeldet haben können sie durch Projekteigentümer zu einem Projekt hinzugefügt werden. Jeder Nutzer kann eigene Projekte anlegen. Zusätzlich können im Operations Center externe Benutzer angelegt werden. Siehe Rolle Externe Benutzer . Gruppen vs Rollen Unterschied von Rollen und Gruppen . Das Operations Center hat aktuell 4 Benutzergruppen: . | Externe Benutzer: Externe Benutzer werden im Operations Center verwaltet. Sie können Zugriff per VPN auf VMs erhalten. Zusätzlich kann der öffentliche SSH Schlüssel zu VMS hinzugefügt werden. | Benutzer (SSO): Benutzer können Projekte anlegen und Verwalten. Auf Projektebene gibt es drei Rollen | . | Owner: Vollzugriff auf das Projekt. | . | . | Member: Vollen Zugriff auf die Projektresourcen ohne Benutzerverwaltung. | . | . | Viewer: Nur lesender Zugriff. Der Gast kann keine Passwörter für VMs einsehen. | . | Admin (SSO)*können externe Benutzer verwalten und haben Zugriff auf die Ressourcenübersicht und Globale Nachrichten Funktion. | Global Admin (SSO)*: Haben zusätzlichen Zugriff auf alle Projekte und können das VPNaaS für die Projekte aktivieren VPN können die normalen Admins auch anlegen | . *Anfrage an GEC notwendig nachdem der Benutzer sich das erste mal am Operations Center angemeldet hat. Admin Rechte nur auf einem Projekt? Wie? . Benutzer haben automatisch vollen Zugriff auf Projekte, die sie anlegen. Andere Projektbesitzer können das Recht weitergeben. Wie kann ich den Besitzer von einem Projekt ändern? . Jeder Projektbesitzer und Admin mit Zugriff auf das Projekt kann die Berechtigungen ändern. Was passiert wenn ein User das Unternehmen verlässt? . Durch die Integration von SSO verliert der Benutzer automatisch den Zugriff. Es ist ggfls. notwendig seine Projekte neu zuzuweisen. Siehe vorheriger Punkt. VPN Zugänge müssen manuell gelöscht werden. Können Vorgaben gemacht werden wie ein Projekt heißen soll? . Beim Anlegen. Eine Änderung ist nicht vorgesehen. Wie wird ein VPN angelegt? . Globale Administratoren können die Funktion für einzelne Projekte aktivieren. Projektbesitzer können weiter Benutzer die dem Projekt hinzugefügt sind freischalten. Benutzer brauchen Zugriff auf das Projekt um ihre eigene Konfiguration herunterzuladen. Externe Benutzer . Externe Benutzer können auch für VPN freigeschaltet werden. Die VPN Konfiguration für externe Benutzer muss durch einen Projektbesitzer Benutzer heruntergeladen und zur Verfügung gestellt werden. Wie werden Projektefreigaben angelegt bzw. erstellt? . Auf der Startseite jedes Projektes über die Mitgliederliste. Wie werden die Flavor verwaltet? . Flavor werden in Openstack verwaltet. Jedes “public” Flavor steht im OC zur Konfiguration bereit. Falls ein “public” Flavor nicht mehr im Operations Center beim Erstellen einer VM auswählbar sein soll, dann können die Metadaten des Flavors entsprechend geändert werden. Dazu in Horizon das Flavor auswählen, auf “Aktualisiere Metadaten” klicken und im Feld “Custom” visibility eintragen und mit “+” hinzufügen. Das Feld taucht jetzt unter “Existing Metadata” auf. Hier den Wert “false” eintragen und die Änderung speichern. Das Flavor kann jetzt nicht mehr beim erstellen einer neuen VM ausgewählt werden. Bestehende VMs die dieses Flavor benutzen werden nicht beeinflusst und können weiterhin im Operations Center verwaltet werden. Flavor können nicht auf einzelne Benutzer beschränkt werden. Dürfen Flavor gelöscht werden . Ja, aber nur wenn sichergestellt ist, dass keine VM das Flavor nutzt. Das Löschen erfolgt über Horizon. Sollte ein Flavor, welches noch in Nutzung ist, gelöscht werden, kann jedes Projekt mit einer betroffenen VM nicht mehr über das Operations Center verwaltet werden. Wo kann man Logs sehen, wenn eine VM angelegt wurde aber sie nicht erscheint? . Über Horizon ist es möglich den Startvorgang der VM zu beobachten und zu wiederholen. Wird die VM in Horizon nicht angelegt, ist aktuell ein Ticket im Helpdesk anzulegen. Wie kann man VMs löschen von anderen Usern/Projekten? . Nur Benutzer und Globale Administratoren mit Zugriff auf Projekte können VMs löschen. Projektliste Operations Center vs Horizon . Die Projekte vom Operations Center sind auch in Horizon sichtbar. Projekte, die in Horizon erzeugt werden sind nicht im Operations Center sichtbar. Wie können VMs gestartet werden? . Der Start von VMs ist über die Schaltfläche “Starten” im Operations Center möglich. Wie kann die Plattengröße einer bestehenden VM erweitert werden im Operations Center? . Es ist nicht möglich. Wie können externe/interne User gelöscht werden? . Interne User werden über das SSO authentifiziert. Werden sie dort deaktiviert, werden sie automatisch gesperrt. Externe Benutzer können in der Übersicht der Externen Benutzer im Operations Center von Administratoren gelöscht werden. Wie kann ich den Router bearbeiten und Konfigurieren? . Das Operations Center konfiguriert die Router entsprechend der Projektvorgaben automatisch. Wie können Routen bearbeitet werden im Operations Center? . Es können im Operations Center keine zusätzlichen Routen eingerichtet werden. Wie kann man einen SSH key einer vorhandenen VM hinzufügen? . Es ist nicht möglich SSH keys nach der Erstellung einer VM über das Operations Center hinzuzufügen. Dies müssen direkt in der VM hinzugefügt werden. Volumegrößen . Das Operations Center erlaubt das Anlegen von Volumes in unterschiedlichen Größen. Die Liste kann auf Anfrage erweitert werden. Kann das Operations Center per API konfiguriert werden . Nein, das ist nicht vorgesehen. Wie kann ich mich bei einer Windows VM anmelden? Der User operation funktioniert nicht mit dem eingegebenen Passwort. Anstelle des Benutzers “operation” muss für Windows VMs der Benutzer des ausgewählten images verwendet werden. Oft Administrator. ",
    "url": "/edge/faq/#operations-center",
    
    "relUrl": "/edge/faq/#operations-center"
  },"272": {
    "doc": "FAQ",
    "title": "General",
    "content": "Zonen . Sicherheitszonen sind in hierarchisch absteigender Reihenfolge strukturiert, beginnend mit der Private / Internal Zone, gefolgt von der Shared Zone und Public Zone. Auf dieser Grundlage können virtuelle Maschinen (VMs) durch die Zuweisung von Floating IPs hierarchisch absteigend kommunizieren. Wenn VMs in sich in der Private / Internal Zonen befinden ist eine Kommunikation mit den Floating IPs in der Shared und Public Zone möglich. Allerdings ist der umgekehrte Weg, also von der Shared Zone zurPrivate / Internal Zonen, nicht gestattet. Analog ist die Kommunikation von der Public Zone in die Shared Zone zur Private / Internal Zonen nicht gestattet. Die Kommunikation folgt dabei stets dem Prinzip von sicher zu unsicher. Diese hierarchische Struktur ermöglicht eine geordnete und sicherheitsbewusste Kommunikation zwischen den verschiedenen Zonen. Des weiteren sind Floating IPs aus der Private / Internal Zonen nur aus dem Instituts Netz erreichbar, Services mit einer Floating IP aus der Public IP können durch die Instituts IT ins Internet bereitgestellt werden. Als spezial Zone kann auf Anfrage eine Lab Zone eingerichtet werden mit direkt Verbindung an den Switchen. Die Zonen werden durch das Operations Center verwaltet. Bei Projekten, die in Openstack angelegt werden, ist der Nutzer selbst verantwortlich. Wie können die Hardware Server runter/hoch gefahren werden? z.B. bei einer kurzfristigen Stromabschaltung. Aktuell nur per Anfrage im Jira Helpdesk . Starten die Server nach Strom automatisch? . Die Server Starten automatisch aber die Edge ist nicht automatisch nutzbar, die bei einem geregelten Herunterfahren die CEPH Replikation deaktiviert wird. Diese muss bei einem Neustart wieder aktiviert werden. Wenn das beigefügte Windows Image ausgewählt ist, ist da bereits eine Lizenz mit dabei. Um was für eine Lizenz handelt es sich? . Es handelt sich um eine Trial Lizenz von Microsoft. Bei Windows VM’s, ist der Kunde für den Erwerb und das Einfügen einer Lizenz verantwortlich. NVLink und NVSwitches . NV Link und NVSwitches verbinden NVidia Grafikkarten zum direkten Austausch von Daten zwischen den Grafikkarten. Aktuell verbauen wir Supermicro GPU Server diese haben keine NVLink oder NVSwitch zwischen den einzelnen GPUs. Zum aktuellen Zeitpunkt werden NVLink und NVSwitches nur in NVidia eigenen Servern verbaut der Marke DGX oder HGX. ",
    "url": "/edge/faq/#general",
    
    "relUrl": "/edge/faq/#general"
  },"273": {
    "doc": "Guided Tour",
    "title": "Guided Tour",
    "content": "Aus dem Browser zum eigenen Stack per Heat . ",
    "url": "/optimist/guided_tour/",
    
    "relUrl": "/optimist/guided_tour/"
  },"274": {
    "doc": "01: Das Dashboard (Horizon)",
    "title": "Schritt 1: Das Dashboard (Horizon)",
    "content": " ",
    "url": "/optimist/guided_tour/step01/#schritt-1-das-dashboard-horizon",
    
    "relUrl": "/optimist/guided_tour/step01/#schritt-1-das-dashboard-horizon"
  },"275": {
    "doc": "01: Das Dashboard (Horizon)",
    "title": "Vorwort",
    "content": "In diesem Schritt für Schritt Tutorial werden wir uns schrittweise der Bedienung von Openstack widmen. Den Anfang macht das Horizon(Dashboard), nach einer kleinen Einführung, wird dann auf die Konsole gewechselt und der Abschluss bildet die Erstellung eigener Heat-Templates. ",
    "url": "/optimist/guided_tour/step01/#vorwort",
    
    "relUrl": "/optimist/guided_tour/step01/#vorwort"
  },"276": {
    "doc": "01: Das Dashboard (Horizon)",
    "title": "Login",
    "content": "Nachdem die Zugangsdaten vorliegen, ist der erste Schritt der Login. WICHTIG: Es gibt keinen Reset-Knopf für das Passwort. Für ein neues Passwort, schreiben Sie uns bitte eine E-Mail an support@gec.io. Hierzu wechseln wir im Browser auf folgende URL: https://optimist.gec.io/ . Im sich öffnenden Fenster wählen wir bei Domain default, und tragen den zugesendeten Benutzer (User-Name) sowie das zugehörige Passwort(Password) ein und klicken auf Connect. Nun öffnet sich das Horizon(Dashboard). ",
    "url": "/optimist/guided_tour/step01/#login",
    
    "relUrl": "/optimist/guided_tour/step01/#login"
  },"277": {
    "doc": "01: Das Dashboard (Horizon)",
    "title": "Passwort ändern",
    "content": "Da aus Sicherheitsgründen empfohlen wird das Passwort nach Erhalt zu ändern, klicken wir im Horizon(Dashboard) dafür rechts oben auf den Benutzernamen(1) und auf Settings(2). Im sich nun öffnenden Fenster sehen wir zuerst Settings, wo unter anderem auch die Sprache umgestellt werden kann. Um das Passwort zu ändern, klicken wir rechts auf Change Password(1). Hier können nun das Passwort geändert werden. Dafür geben wir zunächst unser bisheriges Passwort ein(2), geben dann das neue an(3) und bestätigen es in der neuen Zeile (4). Damit das neue Passwort auch übernommen wird, fehlt noch ein Klick auf Change(5). ",
    "url": "/optimist/guided_tour/step01/#passwort-%C3%A4ndern",
    
    "relUrl": "/optimist/guided_tour/step01/#passwort-ändern"
  },"278": {
    "doc": "01: Das Dashboard (Horizon)",
    "title": "Abschluss",
    "content": "Sie haben Ihre ersten Schritte im Dashboard ausgeführt und Ihr Passwort geändert! . ",
    "url": "/optimist/guided_tour/step01/#abschluss",
    
    "relUrl": "/optimist/guided_tour/step01/#abschluss"
  },"279": {
    "doc": "01: Das Dashboard (Horizon)",
    "title": "01: Das Dashboard (Horizon)",
    "content": " ",
    "url": "/optimist/guided_tour/step01/",
    
    "relUrl": "/optimist/guided_tour/step01/"
  },"280": {
    "doc": "02: SSH-Key per Horizon anlegen",
    "title": "Schritt 2: SSH-Key per Horizon anlegen",
    "content": " ",
    "url": "/optimist/guided_tour/step02/#schritt-2-ssh-key-per-horizon-anlegen",
    
    "relUrl": "/optimist/guided_tour/step02/#schritt-2-ssh-key-per-horizon-anlegen"
  },"281": {
    "doc": "02: SSH-Key per Horizon anlegen",
    "title": "Vorwort",
    "content": "Um im nächsten Schritt einen Stack inkl. einer Instanz zu starten, wird ein SSH Keypair (Schlüsselpaar) benötigt. Für den Fall, dass bereits ein Keypair vorhanden ist und der Umgang damit bekannt ist, kann dieser Schritt übersprungen werden. ",
    "url": "/optimist/guided_tour/step02/#vorwort",
    
    "relUrl": "/optimist/guided_tour/step02/#vorwort"
  },"282": {
    "doc": "02: SSH-Key per Horizon anlegen",
    "title": "Installation",
    "content": "Es gibt verschiedene Wege, um ein Keypair zu erzeugen. Einer der späteren Schritte erklärt den Weg zu einem selbst erstellten Keypair. Hier wird der Schlüssel direkt im Horizon(Dashboard) erstellt, um im nächsten Schritt den Stack zu erstellen. Um nun den Schlüssel zu erstellen, wechseln wir im Horizon(Dashboard) in der Navigation auf Compute → Key Pairs und klicken dort auf Create Key Pair. Im sich öffnenden Fenster kann nun ein Name für den Key vergeben werden, in dem Beispiel wird BeispielKey verwendet, anschließend klicken wir auf Create Key Pair. ",
    "url": "/optimist/guided_tour/step02/#installation",
    
    "relUrl": "/optimist/guided_tour/step02/#installation"
  },"283": {
    "doc": "02: SSH-Key per Horizon anlegen",
    "title": "Abschluss",
    "content": "Wir haben jetzt unser SSH Keypair erstellt und sind bereit für den Rest des Tutorials! . ",
    "url": "/optimist/guided_tour/step02/#abschluss",
    
    "relUrl": "/optimist/guided_tour/step02/#abschluss"
  },"284": {
    "doc": "02: SSH-Key per Horizon anlegen",
    "title": "02: SSH-Key per Horizon anlegen",
    "content": " ",
    "url": "/optimist/guided_tour/step02/",
    
    "relUrl": "/optimist/guided_tour/step02/"
  },"285": {
    "doc": "03: Einen Stack starten",
    "title": "Schritt 3: Einen Stack starten",
    "content": " ",
    "url": "/optimist/guided_tour/step03/#schritt-3-einen-stack-starten",
    
    "relUrl": "/optimist/guided_tour/step03/#schritt-3-einen-stack-starten"
  },"286": {
    "doc": "03: Einen Stack starten",
    "title": "Vorwort",
    "content": "In diesem Schritt beschäftigen wir uns damit, im Horizon Dashboard einen Stack zu starten und damit auch das Horizon Dashboard besser kennenzulernen. Wichtige Voraussetzung ist an dieser Stelle ein SSH-Key, den wir in Schritt 2 erzeugt haben. ",
    "url": "/optimist/guided_tour/step03/#vorwort",
    
    "relUrl": "/optimist/guided_tour/step03/#vorwort"
  },"287": {
    "doc": "03: Einen Stack starten",
    "title": "Start",
    "content": "Um einen Stack zu starten, loggen wir uns zunächst im Horizon Dashboard mit denen in Schritt 1 geänderten Zugangsdaten ein. Hier navigieren wir über Orchestration zu Stacks und klicken auf Launch Stack. Um den Stack auch zu starten, benötigen wir zunächst ein Template, welches in dem Stack eine Instanz startet. Hierfür nutzen wir die SingleServer.yaml aus dem GECio Github Repository. In dem sich nun öffnenden Fenster, wählen wir bei Template Source File aus und nehmen bei Template File, die eben heruntergeladene SingleServer.yaml. Den Rest belassen wir so wie es ist und klicken auf Next. Nun werden weitere Eingaben benötigt, genauer sind das folgende und am Ende klicken wir auf Launch: . | Stack Name: BeispielServer | Creation Timeout: 60 | Password for User: Bitte das eigene Passwort eintragen | availability_zone: ix1 | flavor_name: m1.micro | key_name: BeispielKey | machine_name: singleserver | public_network_id: provider | . Nun wird der Stack auch direkt gestartet und das Horizon Dashboard sieht dann so aus: . Um nun zu überprüfen ob die Instanz korrekt gestartet wurde, wechseln wir in der Navigation auf Compute → Instances und die Übersicht sieht dann wie folgt aus: . Nachdem nun also der Stack und auch die darin enthaltene Instanz gestartet wurden, löschen wir jetzt wieder den Stack inklusive Instanz. Wir könnten auch die Instanz alleine löschen, das kann aber im Nachgang zu Problemen beim löschen des Stacks führen. Um den Stack zu löschen, wechseln wir in der Navigation wieder auf Orchestration → Stacks. Klicken hinter dem Stack, unter Actions, auf den Pfeil nach unten und wählen dort Delete Stack. ",
    "url": "/optimist/guided_tour/step03/#start",
    
    "relUrl": "/optimist/guided_tour/step03/#start"
  },"288": {
    "doc": "03: Einen Stack starten",
    "title": "Abschluss",
    "content": "Wir haben unseren ersten Stack erstellt … und ihn dann gelöscht! . ",
    "url": "/optimist/guided_tour/step03/#abschluss",
    
    "relUrl": "/optimist/guided_tour/step03/#abschluss"
  },"289": {
    "doc": "03: Einen Stack starten",
    "title": "03: Einen Stack starten",
    "content": " ",
    "url": "/optimist/guided_tour/step03/",
    
    "relUrl": "/optimist/guided_tour/step03/"
  },"290": {
    "doc": "04: Der Weg vom Horizon auf die Kommandozeile",
    "title": "Schritt 4: Der Weg vom Horizon auf die Kommandozeile",
    "content": " ",
    "url": "/optimist/guided_tour/step04/#schritt-4-der-weg-vom-horizon-auf-die-kommandozeile",
    
    "relUrl": "/optimist/guided_tour/step04/#schritt-4-der-weg-vom-horizon-auf-die-kommandozeile"
  },"291": {
    "doc": "04: Der Weg vom Horizon auf die Kommandozeile",
    "title": "Vorwort",
    "content": "Auf den ersten Blick kann es komfortabel erscheinen, seine OpenStack Umgebung mit dem Horizon Dashboard zu verwalten. Für einfache, nicht wiederkehrende Aufgaben, kann das Horizon Dashboard mit seinen grafische Ansichten wirklich hilfreich sein. Sobald Aufgaben regelmäßig wiederholt werden oder ein komplexerer Stack verwaltet werden soll, ist es sinnvoller, den OpenStack Client und auch Heat(welches in den späteren Schritten mit erklärt wird) zu verwenden. Anfangs mag die Handhabung ungewohnt sein, mit ein wenig Übung kann die Arbeit an den eigenen Stacks schnell und effizient erledigt werden. Der OpenStack Client ist sehr hilfreich bei der Administration der OpenStack Umgebung, da dort bereits Komponenten wie Nova, Glance, Heat, Cinder, Neutron enthalten sind. Da wir auch im weiteren Verlauf der Dokumentation den Client nutzen, installieren wir ihn in diesem Schritt. ",
    "url": "/optimist/guided_tour/step04/#vorwort",
    
    "relUrl": "/optimist/guided_tour/step04/#vorwort"
  },"292": {
    "doc": "04: Der Weg vom Horizon auf die Kommandozeile",
    "title": "Installation",
    "content": "Um den OpenStackClient installieren zu können, wird mindestens Python 2.7 noch die Python Setuptools (diese sind bei macOS bereits vorinstalliert). Es gibt verschiedene Optionen die Installation durchzuführen, pip hat sich hierbei als eine gute Lösung herausgestellt und wird als Grundlage in der Dokumentation verwendet. Selbiges ist einfach zu bedienen, stellt sicher das die aktuellste Version der Pakete genutzt wird und kann im Nachhinein Updates einspielen. Man kann den Client ohne weiteres in root/admin installieren, das kann aber zu weiteren Problem führen, daher nutzen wir eine virtuelle Umgebung für den Clienten. macOS . Damit nun der OpenStack Client installieren werden kann, wird zunächst pip benötigt. Um pip zu installieren, wird zunächst die Konsole geöffnet (diese kann zum Beispiel über das Launchpad → Konsole geöffnet werden)  und dann folgender Befehl ausgeführt: . $ easy_install pip Searching for pip Best match: pip 9.0.1 Adding pip 9.0.1 to easy-install.pth file Installing pip script to /usr/local/bin Installing pip2.7 script to /usr/local/bin Installing pip2 script to /usr/local/bin Using /usr/local/lib/python2.7/site-packages Processing dependencies for pip Finished processing dependencies for pip . Sobald die Installation von pip abgeschlossen ist, wird nun die Virtuelle Umgebung angelegt: . $ pip install virtualenv Collecting virtualenv Downloading virtualenv-15.1.0-py2.py3-none-any.whl (1.8MB) 100% |????????????????????????????????| 1.8MB 619kB/s Installing collected packages: virtualenv Successfully installed virtualenv-15.1.0 . Nachdem wir nun mit virtualenv eine virtuelle Umgebung nutzen können, erstellen wir direkt eine: . $ virtualenv ~/.virtualenvs/openstack New python executable in /Users/iNNOVO/.virtualenvs/openstack/bin/python Installing setuptools, pip, wheel...done. Jetzt wechseln wir in die erzeugte virtuelle Umgebung: . $ source ~/.virtualenvs/openstack/bin/activate (openstack) $ . Nachdem dieser Wechsel funktioniert hat, können wir in der geschaffenen Umgebung auch direkt den OpenStackClient installieren: . (openstack) $ pip install python-openstackclient . Da wir im Verlauf der Dokumentation auch andere Services benutzen, installieren wir die entsprechenden Clienten gleich mit: . (openstack) $ pip install python-heatclient python-designateclient python-octaviaclient . Nun verlassen wir die Umgebung auch direkt wieder: . (openstack) $ deactivate . Damit wir den OpenStackClient auch nutzen können, ist es nun notwendig dies in die Path Variablen aufzunehmen. export PATH=\"$HOME/.virtualenvs/openstack/bin/:$PATH\" . Um zu sehen ob alles korrekt funktioniert hat, testen wir die Ausgabe: . $ type -a openstack openstack is /home/iNNOVO/.virtualenvs/openstack/bin/openstack . Windows . Um pip zu nutzen, ist zuerst der Wechsel in den Ordner der Python Installation notwendig (Speicherort der Standart Installation: C:\\Python27\\Scripts). pip wird dann mit dem Befehl easy_install pip installiert: . C:\\Python27\\Scripts&gt;easy_install pip Searching for pip Best match: pip 9.0.1 Adding pip 9.0.1 to easy-install.pth file Installing pip-script.py script to c:\\python27\\scripts Installing pip.exe script to c:\\python27\\scripts Installing pip.exe.manifest cript to c:\\python27\\scripts Installing pip3.5-script.py script to c:\\python27\\scripts Installing pip3.5.exe script to c:\\python27\\scripts Installing pip3.5.exe.manifest script to c:\\python27\\scripts Installing pip3-script.py script to c:\\python27\\scripts Installing pip3.exe script to c:\\python27\\scripts Installing pip3.exe.manifest script to c:\\python27\\scripts Using c:\\python27\\lib\\site-packages Processing dependencies for pip Finished processing dependencies for pip . Nach der erfolgreichen Installation von pip, kann direkt mit pip install python-openstackclient der OpenStackClient auch installiert werden: . C:\\Python27\\Scripts&gt;pip install python-openstackclient Collecting python-openstackclient Downloading python_openstackclient-3.12.0-py2.py3-none-any.whl (772kB) 100% |################################| 778kB 1.1MB/s . Linux (in diesem Beispiel Ubuntu) . Zunächst wird auch hier pip benötigt, dafür wird apt-get genutzt: . $ sudo apt-get install python3-pip Reading package lists... Done Building dependency tree Reading state information... Done . Sobald die Installation von pip abgeschlossen ist, wird nun die Virtuelle Umgebung angelegt: . $ sudo apt-get install python3-virtualenv Reading package lists... Done Building dependency tree Reading state information... Done . Nachdem wir nun mit virtualenv eine virtuelle Umgebung nutzen können, erstellen wir direkt eine: . $ virtualenv ~/.virtualenvs/openstack New python executable in /Users/iNNOVO/.virtualenvs/openstack/bin/python Installing setuptools, pip, wheel...done. Jetzt wechseln wir in die erzeugte virtuelle Umgebung: . $ source ~/.virtualenvs/openstack/bin/activate (openstack) $ . Nachdem dieser Wechsel funktioniert hat, können wir in der geschaffenen Umgebung auch direkt den OpenStackClient installieren: . (openstack) $ pip install python-openstackclient . Da wir im Verlauf der Dokumentation auch Heat benutzen, installieren wir den entsprechenden Heat Clienten gleich mit: . (openstack) $ pip install python-heatclient . Nun verlassen wir die Umgebung auch direkt wieder: . (openstack) $ deactivate . Damit wir den OpenStackClient auch nutzen können, ist es nun notwendig dies in die Path Variablen aufzunehmen. export PATH=\"$HOME/.virtualenvs/openstack/bin/:$PATH\" . Um zu sehen ob alles korrekt funktioniert hat, testen wir die Ausgabe: . $ type -a openstack openstack is /home/iNNOVO/.virtualenvs/openstack/bin/openstack . ",
    "url": "/optimist/guided_tour/step04/#installation",
    
    "relUrl": "/optimist/guided_tour/step04/#installation"
  },"293": {
    "doc": "04: Der Weg vom Horizon auf die Kommandozeile",
    "title": "Zugangsdaten",
    "content": "Nachdem der OpenStackClient nun installiert ist, werden noch die Zugangsdaten für Openstack benötigt. Diese können direkt im Horizon Dashboard heruntergeladen werden. Dafür loggen wir uns ein und klicken dann rechts oben in der Ecke auf die E-Mail-Adresse und dann auf Download OpenStack RC File v3. Die heruntergeladene Datei trägt den Projektnamen (Projektname.sh), in unserem Beispiel nennen wir sie Beispiel.sh . macOS | Linux . Um die Zugangsdaten in den OpenStackClienten einzulesen, führen wir nun folgenden Befehl aus: . source Beispiel.sh . Windows . Um unter Windows die Zugangsdaten einzulesen, ist es notwendig entweder PowerShell, Git for Windows oder Linux on Windows zu nutzen. Bei Linux on Windows und Git for Windows via Git Bash, wird der gleiche Befehl wie im Beispiel für macOS | Linux genutzt: . source Beispiel.sh . Bei der Nutzung von PowerShell müssen die Variablen einzeln gesetzt werden. Alle notwendigen Variablen befinden sich in der Datei Beispiel.sh und diese kann mit einem Editor geöffnet werden. Um die Variablen zu setzen, kann folgender Befehl genutzt werden: . set-item env:OS_AUTH_URL -value \"https://identity.optimist.gec.io/v3\" set-item env:OS_PROJECT_ID -value \"Projekt ID eintragen\" set-item env:OS_PROJECT_NAME -value \"Namen eintrage\" set-item env:OS_USER_DOMAIN_NAME -value \"Default\" set-item env:OS_USERNAME -value \"Usernamen eintragen\" set-item env:OS_PASSWORD -value \"Passwort eingeben\" set-item env:OS_USER_DOMAIN_NAME -value \"Default\" set-item env:OS_REGION_NAME -value \"fra\" set-item env:OS_INTERFACE -value \"public\" set-item env:OS_IDENTITY_API_VERSION -value \"3\" . ",
    "url": "/optimist/guided_tour/step04/#zugangsdaten",
    
    "relUrl": "/optimist/guided_tour/step04/#zugangsdaten"
  },"294": {
    "doc": "04: Der Weg vom Horizon auf die Kommandozeile",
    "title": "Ziel",
    "content": "Die Installation des OpenStackClienten ist abgeschlossen und die ersten Befehle können damit getestet werden. Eine Übersicht über alle Befehle, kann mit folgendem Kommando abgerufen werden: . openstack --help . ",
    "url": "/optimist/guided_tour/step04/#ziel",
    
    "relUrl": "/optimist/guided_tour/step04/#ziel"
  },"295": {
    "doc": "04: Der Weg vom Horizon auf die Kommandozeile",
    "title": "04: Der Weg vom Horizon auf die Kommandozeile",
    "content": " ",
    "url": "/optimist/guided_tour/step04/",
    
    "relUrl": "/optimist/guided_tour/step04/"
  },"296": {
    "doc": "05: Die wichtigsten Befehle des OpenStackClients",
    "title": "Schritt 5: Die wichtigsten Befehle des OpenStackClients",
    "content": " ",
    "url": "/optimist/guided_tour/step05/#schritt-5-die-wichtigsten-befehle-des-openstackclients",
    
    "relUrl": "/optimist/guided_tour/step05/#schritt-5-die-wichtigsten-befehle-des-openstackclients"
  },"297": {
    "doc": "05: Die wichtigsten Befehle des OpenStackClients",
    "title": "Vorwort",
    "content": "Nachdem in Schritt 4 der OpenStack Client installiert wurde, werden wir in diesem Schritt alle wichtigen Befehle einmal auflisten. Die Übersicht der spezifischen Subbefehle kann in der Kommandozeile mit einem --help hinter dem eigentlichen Befehl separat angezeigt werden. Um alle Befehle aufzulisten, kann der Schalter --help auch ohne weitere Angaben von einem Bestandteil  verwendet werden: . openstack --help . ",
    "url": "/optimist/guided_tour/step05/#vorwort",
    
    "relUrl": "/optimist/guided_tour/step05/#vorwort"
  },"298": {
    "doc": "05: Die wichtigsten Befehle des OpenStackClients",
    "title": "Server",
    "content": "Mit dem Kommando openstack server ist es mögliche eigene Instanz zu erstellen, diese zu verwalten, zu löschen und andere} administrative Aufgaben durchzuführen. Hier eine Liste der wichtigsten Kommandos:} . | openstack server add Einer bestehenden Instanz können verschiedene Bestandteile (Fixed IP, Floating IP, Security Group, Volume) zugewiesen werden | openstack server create Mit diesem Befehl kann eine neue Instanz erstellt werden | openstack server delete Löscht die im Befehl angegebene Instanz | openstack server list Listet alle bestehenden Instanzen auf | openstack server remove Kann verschiedene Bestandteile (Fixed IP, Floating IP, Security Group, Volume) wieder entfernen | openstack server show Zeigt alle verfügbaren Informationen zu der im Befehl genannten Instanz an | . ",
    "url": "/optimist/guided_tour/step05/#server",
    
    "relUrl": "/optimist/guided_tour/step05/#server"
  },"299": {
    "doc": "05: Die wichtigsten Befehle des OpenStackClients",
    "title": "Stack (Heat)",
    "content": "Genauso wie mit openstack server ... Befehlen einzelne Instanzen administriert werden, kann man mit openstack stack ... ganze Stacks verwalten. Auch hier eine kurze Auflistung der wichtigsten Befehle: . | openstack stack create Kann einen neuen Stack erstellen | openstack stack list Listet alle bestehenden Stacks auf | openstack stack show Zeigt alle Informationen zu dem im Befehl angegebenen Stack | openstack stack delete Löscht den im Befehl angegebenen Stack | . ",
    "url": "/optimist/guided_tour/step05/#stack-heat",
    
    "relUrl": "/optimist/guided_tour/step05/#stack-heat"
  },"300": {
    "doc": "05: Die wichtigsten Befehle des OpenStackClients",
    "title": "Security Group",
    "content": "Security Groups werden verwendet, um Instanzen eingehende und ausgehende Netzwerk-Verbindungen basierend auf IP-Adressen und Ports zu erlauben oder zu verbieten. Auch Security Groups kann man mit dem OpenStackClienten verwalten. Hier eine beispielhafte Liste üblicher Aufrufe: . | openstack security group create Erstellt eine neue Security Group | openstack security group delete Löscht die im Befehl angegebene Security Group | openstack security group list Listet alle bestehenden Gruppen auf | openstack security group show Zeigt alle verfügbaren Informationen zu der im Befehl angegebenen Security Group | openstack security group rule create Fügt eine Regel zu einer Security Group hinzu | openstack security group rule delete Löscht die im Befehl angegeben Regel | . ",
    "url": "/optimist/guided_tour/step05/#security-group",
    
    "relUrl": "/optimist/guided_tour/step05/#security-group"
  },"301": {
    "doc": "05: Die wichtigsten Befehle des OpenStackClients",
    "title": "Network",
    "content": "Um später auch Instanzen sinnvoll nutzen zu können, benötigen diesen ein Netzwerk, hier eine kurze Auflistung der wichtigsten Befehle um ein Netzwerk zu erstellen: . | openstack network create Erstellt ein neues Netzwerk | openstack nerwork list Listet alle bestehenden Netzwerke auf | openstack network show Zeigt alle Informationen zu dem im Befehl angegebenen Netzwerk | openstack network delete Löscht das im Befehl angegebene Netzwerk | . ",
    "url": "/optimist/guided_tour/step05/#network",
    
    "relUrl": "/optimist/guided_tour/step05/#network"
  },"302": {
    "doc": "05: Die wichtigsten Befehle des OpenStackClients",
    "title": "Router",
    "content": "Damit eine Instanz mit einem Netzwerk verbunden werden kann, ist ein virtueller Router notwendig und lässt sich mit diesen Kommando administrieren. Hier eine kurze Liste der möglichen Befehle: . | openstack router create Erstellt einen neuen Router | openstack router delete Löscht einen bestehenden Router | openstack router add port Weist dem angegebenen Router, den angegebenen Port zu | openstack router add subnet Weist dem angegeben Router, das angegebene Subnet zu | . ",
    "url": "/optimist/guided_tour/step05/#router",
    
    "relUrl": "/optimist/guided_tour/step05/#router"
  },"303": {
    "doc": "05: Die wichtigsten Befehle des OpenStackClients",
    "title": "Subnet",
    "content": "Um den virtuellen Router korrekt zu betreiben, wird auch ein Subnet benötigt, welches mit dem Kommando openstack subnet administriert werden kann. Möglich Befehle sind: . | openstack subnet create Erstellt ein neues Subnet | openstack subnet delete Löscht ein bestehendes Subnet | openstack subnet show Zeigt alle verfügbaren Informationen zu einem Subnet an | . ",
    "url": "/optimist/guided_tour/step05/#subnet",
    
    "relUrl": "/optimist/guided_tour/step05/#subnet"
  },"304": {
    "doc": "05: Die wichtigsten Befehle des OpenStackClients",
    "title": "Port",
    "content": "Nachdem nun bereits virtuelle Router und Subnets bekannt sind, darf der Port nicht fehlen. Die wichtigsten Befehle: . | openstack port create Erstellt einen neuen Port | openstack port delete Löscht einen bestehenden Port | openstack port show Zeigt alle verfügbaren Informationen zu einem Port an | . ",
    "url": "/optimist/guided_tour/step05/#port",
    
    "relUrl": "/optimist/guided_tour/step05/#port"
  },"305": {
    "doc": "05: Die wichtigsten Befehle des OpenStackClients",
    "title": "Volume",
    "content": "Volumes sind persistente Speicherorte, die über die Existenz von einzelnen Instanzen hinaus erhalten bleiben. Wichtige Befehle sind: . | openstack volume create Erstellt ein neues Volume | openstack volume delete Löscht ein bestehendes Volume | openstack volume show Zeigt alle verfügbaren Informationen zu einem Volume an | . ",
    "url": "/optimist/guided_tour/step05/#volume",
    
    "relUrl": "/optimist/guided_tour/step05/#volume"
  },"306": {
    "doc": "05: Die wichtigsten Befehle des OpenStackClients",
    "title": "Abschluss",
    "content": "In diesem Schritt wurden die wichtigsten Befehle einmal aufgelistet und auch mit einer kleinen Erklärung versehen. Die genannten Befehle werden in den nächsten Schritten benötigti und bilden somit die Grundlage für die Guided Tour. In Schritt 6 wird das Thema ein selbst erstelltes SSH Key Pair sein. ",
    "url": "/optimist/guided_tour/step05/#abschluss",
    
    "relUrl": "/optimist/guided_tour/step05/#abschluss"
  },"307": {
    "doc": "05: Die wichtigsten Befehle des OpenStackClients",
    "title": "05: Die wichtigsten Befehle des OpenStackClients",
    "content": " ",
    "url": "/optimist/guided_tour/step05/",
    
    "relUrl": "/optimist/guided_tour/step05/"
  },"308": {
    "doc": "06: Einen eigenen SSH-Key per Konsole erstellen und nutzen",
    "title": "Schritt 6: Einen eigenen SSH-Key per Konsole erstellen und nutzen",
    "content": " ",
    "url": "/optimist/guided_tour/step06/#schritt-6-einen-eigenen-ssh-key-per-konsole-erstellen-und-nutzen",
    
    "relUrl": "/optimist/guided_tour/step06/#schritt-6-einen-eigenen-ssh-key-per-konsole-erstellen-und-nutzen"
  },"309": {
    "doc": "06: Einen eigenen SSH-Key per Konsole erstellen und nutzen",
    "title": "Vorwort",
    "content": "Um später Zugriff auf den ersten deployten Stack per SSH zu erhalten, ist es notwendig, ein Key Pair zu erzeugen und dieses im Gegensatz zu Schritt 2 auch zu nutzen. Sollte bereits ein Keypair vorhanden sein, ist es nicht notwendig einen neuen zu erstellen. ",
    "url": "/optimist/guided_tour/step06/#vorwort",
    
    "relUrl": "/optimist/guided_tour/step06/#vorwort"
  },"310": {
    "doc": "06: Einen eigenen SSH-Key per Konsole erstellen und nutzen",
    "title": "Installation",
    "content": "Wie in Schritt 2 erwähnt, gibt es mehrere Optionen um einen Key zu erstellen. Da wir bereits per Horizon(Dashboard) einen Key erzeugt haben, wird in diesem Schritt er direkt über einen Befehl in der Kommandozeile erstellt. $ ssh-keygen -t rsa -f Beispiel.key Generating public/private rsa key pair. Enter passphrase (empty for no passphrase): Enter same passphrase again: Your identification has been saved in Beispiel.key. Your public key has been saved in Beispiel.key.pub. The key fingerprint is: SHA256:UKSodmr6MFCO1fSqNYAoyM7uX8n/O5a43cPEV5vJXW8 The key's randomart image is: +---[RSA 2048]----+ | .o |+. o o o |=.+ o + |+= o ..|oo+ = S . o B|o. =... o . =E|o.+ + . + . |.= ...+.o |.oo. o++o.. | +----[SHA256]-----+ . Mit dem oben genutzten Befehl (ssh-keygen -t rsa -f Beispiel.key) werden zwei Dateien erzeugt, also das vorher genannte Keypair. Zum einen die Beispiel.key Datei und die Beispiel.key.pub, dabei ist Beispiel.key der private Teil, der nur uns bekannt sein soll und Beispiel.key.pub wird als öffentlicher Teil genutzt. ",
    "url": "/optimist/guided_tour/step06/#installation",
    
    "relUrl": "/optimist/guided_tour/step06/#installation"
  },"311": {
    "doc": "06: Einen eigenen SSH-Key per Konsole erstellen und nutzen",
    "title": "Einsatzort",
    "content": "Um den gerade erstellten Key zu nutzen, muss dieser eingebunden und für später erstellte Instanzen/Stacks bereit gestellt werden. Dies geht direkt mit dem vorher installierten OpenStackClient. In der Dokumentation gehen wir davon aus, dass der erzeugte Key in ~/.ssh/ liegt, sollte sich dieser an einer anderen Stelle befinden, muss das Keypair dorthin kopiert werden oder der Befehl entsprechend angepasst werden: . $ openstack keypair create --public-key ~/.ssh/Beispiel.key.pub Beispiel +-------------+-------------------------------------------------+ | Field | Value | +-------------+-------------------------------------------------+ | fingerprint | ec:a6:75:f9:33:4b:e0:ba:e7:bb:b6:8a:a1:5d:48:ff | name | Beispiel | user_id | 9bf501f4c3d14b7eb0f1443efe80f656 | +-------------+-------------------------------------------------+ . Da im weiteren Verlauf der SSH-Key genutzt wird, sollte der Name, der statt Beispiel vergeben wird, leicht merkbar sein. Um zu überprüfen, ob der Key korrekt abgelegt wurde oder um sich den Namen erneut anzeigen zu lassen, nutzt man folgenden Befehl: . $ openstack keypair list +----------+-------------------------------------------------+ | Name | Fingerprint | +----------+-------------------------------------------------+ | Beispiel | ec:a6:75:f9:33:4b:e0:ba:e7:bb:b6:8a:a1:5d:48:ff | +----------+-------------------------------------------------+ . ",
    "url": "/optimist/guided_tour/step06/#einsatzort",
    
    "relUrl": "/optimist/guided_tour/step06/#einsatzort"
  },"312": {
    "doc": "06: Einen eigenen SSH-Key per Konsole erstellen und nutzen",
    "title": "Abschluss",
    "content": "Da der SSH Key jetzt genutzt werden kann, wird es Zeit weiter vorzugehen und eine eigene Instanz zu erstellen. Wie das genau funktioniert, erklären wir in Schritt 7. ",
    "url": "/optimist/guided_tour/step06/#abschluss",
    
    "relUrl": "/optimist/guided_tour/step06/#abschluss"
  },"313": {
    "doc": "06: Einen eigenen SSH-Key per Konsole erstellen und nutzen",
    "title": "06: Einen eigenen SSH-Key per Konsole erstellen und nutzen",
    "content": " ",
    "url": "/optimist/guided_tour/step06/",
    
    "relUrl": "/optimist/guided_tour/step06/"
  },"314": {
    "doc": "07: Die erste eigene Instanz",
    "title": "Schritt 7: Die erste eigene Instanz",
    "content": " ",
    "url": "/optimist/guided_tour/step07/#schritt-7-die-erste-eigene-instanz",
    
    "relUrl": "/optimist/guided_tour/step07/#schritt-7-die-erste-eigene-instanz"
  },"315": {
    "doc": "07: Die erste eigene Instanz",
    "title": "Vorwort",
    "content": "Wir wissen jetzt alles, was nötig ist, um die erste eigene Instanz anzulegen und zu starten. Es ist am sinnvollsten, das gleich in einem Stack zu organisieren und diesen mit einem Template zu beschreiben, anstatt alle notwendigen Arbeitsschritte von Hand durchzuführen. Trotzdem erzeugen wir im allerersten Schritt erst mal eine Instanz von Hand. ",
    "url": "/optimist/guided_tour/step07/#vorwort",
    
    "relUrl": "/optimist/guided_tour/step07/#vorwort"
  },"316": {
    "doc": "07: Die erste eigene Instanz",
    "title": "Installation",
    "content": "Der Grundbefehl für das Erstellen einer Instanz in der Kommandozeile lautet: . openstack server create test . Wenn der Befehl ohne weitere Zusätze ausgeführt wird, erscheint direkt eine Fehlermeldung: . usage: openstack server create [-h] [-f {json,shell,table,value,yaml}] [-c COLUMN] [--max-width &lt;integer&gt;] [--print-empty] [--noindent] [--prefix PREFIX] (--image &lt;image&gt; | --volume &lt;volume&gt;) --flavor &lt;flavor&gt; [--security-group &lt;security-group-name&gt;] [--key-name &lt;key-name&gt;] [--property &lt;key=value&gt;] [--file &lt;dest-filename=source-filename&gt;] [--user-data &lt;user-data&gt;] [--availability-zone &lt;zone-name&gt;] [--block-device-mapping &lt;dev-name=mapping&gt;] [--nic &lt;net-id=net-uuid,v4-fixed-ip=ip-addr,v6-fixed-ip=ip-addr,port-id=port-uuid,auto,none&gt;] [--hint &lt;key=value&gt;] [--config-drive &lt;config-drive-volume&gt;|True] [--min &lt;count&gt;] [--max &lt;count&gt;] [--wait] &lt;server-name&gt; openstack server create: error: argument --flavor is required . Der Fehler besagt, dass kein Flavor angegeben ist. Damit nun eine Instanz gestartet werden kann, wird dem Befehl noch der entsprechende Flavor hinzugefügt. Um zu sehen welche Flavors bereit stehen, führen wir folgenden Befehl aus: . $ openstack flavor list +--------------------------------------+------------+-------+------+-----------+-------+-----------+ | ID | Name | RAM | Disk | Ephemeral | VCPUs | Is Public | +--------------------------------------+------------+-------+------+-----------+-------+-----------+ | 090bcc91-6207-465d-aff0-bfcc10a9e063 | m1.medium | 8192 | 20 | 0 | 4 | True | 4ade7a50-f829-4bf6-af15-266798ea8d6f | win.large | 32768 | 80 | 0 | 8 | True | 5dd72380-088e-48cd-9a18-112cb5a9cab5 | win.small | 8192 | 80 | 0 | 2 | True | 884d5b93-1467-4bc1-a445-ff7c74271cbd | m1.micro | 1024 | 20 | 0 | 1 | True | b7c4fa0b-7960-4311-a86b-507dbf58e8ac | m1.small | 4096 | 20 | 0 | 2 | True | d45e3029-8364-4e4c-beab-242e8b4622a3 | win.medium | 16384 | 80 | 0 | 4 | True | dfead62e-96a8-46e9-bdae-342ecce32d41 | win.micro | 2048 | 80 | 0 | 1 | True | ed18c320-324a-487f-88e1-3e9eb9244509 | m1.large | 16384 | 20 | 0 | 8 | True | +--------------------------------------+------------+-------+------+-----------+-------+-----------+ . Sollte man nun den Befehl openstack stack create Beispiel mit --flavor m1.micro starten, würde erneut eine Fehlermeldung angezeigt werden, da weitere Parameter fehlen. Um eine Instanz über diesen Weg zu starten, wird neben dem Flavor(--flavor) auch noch der SSH-Key (--key-name), das Image (--image), das verfügbare Netz (--network, in alten Versionen des Clients muss --nic net_id= verwendet werden) und eine SecurityGroup (--security-group) benötigt. Der SSH-Key wurde im letzten Schritt bereits erstellt und braucht so nicht erneut angelegt werden. Damit fehlen noch das Image (--image) und das Netz. Starten wir zunächst mit dem Image, wie bereits bei dem Flavor, kann auch hier eine Übersicht der möglichen Images mit folgendem Befehl angezeigt werden: . $ openstack image list +--------------------------------------+---------------------------------------+--------+ | ID | Name | Status | +--------------------------------------+---------------------------------------+--------+ | fd8ad5aa-6b33-4198-a05d-8be42fc0f20e | CentOS 7 - Latest | active | 82242d21-d990-4fc2-92a5-c7bd7820e790 | Ubuntu 16.04 Xenial Xerus - Latest | active | 8e82fd42-3d6f-44a7-9f20-92f5661823cf | Windows Server 2012 R2 Std - Latest | active | 536c086c-d2a4-43dd-80ea-a9d05ee2b97f | Windows Server 2016 Std - Latest | active | c94ced87-a03e-4eec-89f7-48f2c0ec6cd2 | debian-9.1.5-20170910-openstack-amd64 | active | b1195ddf-9336-42a7-a134-4f2e7ea57710 | iNNOVO-OPNsense-17.7.8 | active | 9134b6ed-8c5a-4a9a-907e-733dc2b5f0ef | iNNOVO_pfSense 2.3.4 | active | +--------------------------------------+---------------------------------------+--------+ . Nun fehlt noch ein Netzwerk. An dieser Stelle gibt es 2 Möglichkeiten für das Netzwerk, zum einen kann man ein sehr simples Netzwerk anlegen und so die Instanz starten, dafür nutzt man folgenden Befehl: . $ openstack network create BeispielNetzwerk +---------------------------+--------------------------------------+ | Field | Value | +---------------------------+--------------------------------------+ | admin_state_up | UP | availability_zone_hints | | availability_zones | | created_at | 2017-12-08T08:32:44Z | description | | dns_domain | None | id | a783d691-7efe-4f67-9226-99a014fa8926 | ipv4_address_scope | None | ipv6_address_scope | None | is_default | False | mtu | 1500 | name | BeispielNetzwerk | port_security_enabled | True | project_id | b15cde70d85749689e08106f973bb002 | provider:network_type | None | provider:physical_network | None | provider:segmentation_id | None | qos_policy_id | None | revision_number | 2 | router:external | Internal | segments | None | shared | False | status | ACTIVE | subnets | | updated_at | 2017-12-08T08:32:44Z | +---------------------------+--------------------------------------+ . Der Nachteil an diesem Netzwerk ist, dass man die Instanz nicht erreichen kann. Soll die Instanz nutzbar sein, wird ein funktionierendes Netz benötigt, welches in Schritt 10 komplett angelegt wird. Nachdem alle Bestandteile jetzt bekannst sind, kann die erste Instanz erstellt werden. Dafür wird das --flavor m1.small, der SSH-Key aus Schritt 6, das Netzwerk von weiter oben, das --image \"Ubuntu 16.04 Xenial Xerus - Latest\" und die --security-group default: . $ openstack server create BeispielServer --flavor m1.small --key-name Beispiel --image 82242d21-d990-4fc2-92a5-c7bd7820e790 --network=BeispielNetzwerk --security-group default +-----------------------------+--------------------------------------------------------+ | Field | Value | +-----------------------------+--------------------------------------------------------+ | OS-DCF:diskConfig | MANUAL | OS-EXT-AZ:availability_zone | es1 | OS-EXT-STS:power_state | NOSTATE | OS-EXT-STS:task_state | scheduling | OS-EXT-STS:vm_state | building | OS-SRV-USG:launched_at | None | OS-SRV-USG:terminated_at | None | accessIPv4 | | accessIPv6 | | addresses | | adminPass | 6MotuEMy4c3t | config_drive | | created | 2017-12-06T14:15:02Z | flavor | m1.small (676d2587-b5aa-49eb-998d-d91c1bd6c056) | hostId | | id | 44ff2688-4ce5-417d-962b-3a80199bf1bc | image | cirros-tempest1 (2fbe66ef-adc8-44d0-b2e2-03d95dc36936) | key_name | cg | name | BeispielServer | progress | 0 | project_id | 1e775e2cc71a461991be42d4fad8a5cb | properties | | security_groups | name='3265503b-ac24-4f60-a8d0-466b7c812916' | status | BUILD | updated | 2017-12-06T14:15:02Z | user_id | b54fda3f4d1a484797b3ad4de9b3f4f9 | volumes_attached | +-----------------------------+--------------------------------------------------------+ . Weitere mögliche Parameter für die Erstellung einer Instanz können mit --help abgefragt werden: . $ openstack server create --help usage: openstack server create [-h] [-f {json,shell,table,value,yaml}] [-c COLUMN] [--max-width &lt;integer&gt;] [--print-empty] [--noindent] [--prefix PREFIX] (--image &lt;image&gt; | --volume &lt;volume&gt;) --flavor &lt;flavor&gt; [--security-group &lt;security-group-name&gt;] [--key-name &lt;key-name&gt;] [--property &lt;key=value&gt;] [--file &lt;dest-filename=source-filename&gt;] [--user-data &lt;user-data&gt;] [--availability-zone &lt;zone-name&gt;] [--block-device-mapping &lt;dev-name=mapping&gt;] [--nic &lt;net-id=net-uuid,v4-fixed-ip=ip-addr,v6-fixed-ip=ip-addr,port-id=port-uuid,auto,none&gt;] [--hint &lt;key=value&gt;] [--config-drive &lt;config-drive-volume&gt;|True] [--min &lt;count&gt;] [--max &lt;count&gt;] [--wait] &lt;server-name&gt; Create a new server positional arguments: &lt;server-name&gt; New server name optional arguments: -h, --help show this help message and exit --image &lt;image&gt; Create server boot disk from this image (name or ID) --volume &lt;volume&gt; Create server using this volume as the boot disk (name or ID) --flavor &lt;flavor&gt; Create server with this flavor (name or ID) --security-group &lt;security-group-name&gt; Security group to assign to this server (name or ID) (repeat option to set multiple groups) --key-name &lt;key-name&gt; Keypair to inject into this server (optional extension) --property &lt;key=value&gt; Set a property on this server (repeat option to set multiple values) --file &lt;dest-filename=source-filename&gt; File to inject into image before boot (repeat option to set multiple files) --user-data &lt;user-data&gt; User data file to serve from the metadata server --availability-zone &lt;zone-name&gt; Select an availability zone for the server --block-device-mapping &lt;dev-name=mapping&gt; Map block devices; map is &lt;id&gt;:&lt;type&gt;:&lt;size(GB)&gt;:&lt;delete_on_terminate&gt; (optional extension) --nic &lt;net-id=net-uuid,v4-fixed-ip=ip-addr,v6-fixed-ip=ip-addr,port-id=port-uuid,auto,none&gt; Create a NIC on the server. Specify option multiple times to create multiple NICs. Either net-id or port- id must be provided, but not both. net-id: attach NIC to network with this UUID, port-id: attach NIC to port with this UUID, v4-fixed-ip: IPv4 fixed address for NIC (optional), v6-fixed-ip: IPv6 fixed address for NIC (optional), none: (v2.37+) no network is attached, auto: (v2.37+) the compute service will automatically allocate a network. Specifying a --nic of auto or none cannot be used with any other --nic value. --hint &lt;key=value&gt; Hints for the scheduler (optional extension) --config-drive &lt;config-drive-volume&gt;|True Use specified volume as the config drive, or 'True' to use an ephemeral drive --min &lt;count&gt; Minimum number of servers to launch (default=1) --max &lt;count&gt; Maximum number of servers to launch (default=1) --wait Wait for build to complete output formatters: output formatter options -f {json,shell,table,value,yaml}, --format {json,shell,table,value,yaml} the output format, defaults to table -c COLUMN, --column COLUMN specify the column(s) to include, can be repeated table formatter: --max-width &lt;integer&gt; Maximum display width, &lt;1 to disable. You can also use the CLIFF_MAX_TERM_WIDTH environment variable, but the parameter takes precedence. --print-empty Print empty table if there is no data to show. json formatter: --noindent whether to disable indenting the JSON shell formatter: a format a UNIX shell can parse (variable=\"value\") --prefix PREFIX add a prefix to all variable names . ",
    "url": "/optimist/guided_tour/step07/#installation",
    
    "relUrl": "/optimist/guided_tour/step07/#installation"
  },"317": {
    "doc": "07: Die erste eigene Instanz",
    "title": "Abschluss",
    "content": "Nachdem wir in diesem Schritt nicht nur eine neue Instanz erstellt , sondern auch noch einige Basis Befehle für OpenStack angewedet haben. Werden wir im Schritt 8 diese VM wieder löschen. ",
    "url": "/optimist/guided_tour/step07/#abschluss",
    
    "relUrl": "/optimist/guided_tour/step07/#abschluss"
  },"318": {
    "doc": "07: Die erste eigene Instanz",
    "title": "07: Die erste eigene Instanz",
    "content": " ",
    "url": "/optimist/guided_tour/step07/",
    
    "relUrl": "/optimist/guided_tour/step07/"
  },"319": {
    "doc": "08: Löschen der ersten eigenen Instanz",
    "title": "Schritt 8: Löschen der ersten eigenen Instanz",
    "content": " ",
    "url": "/optimist/guided_tour/step08/#schritt-8-l%C3%B6schen-der-ersten-eigenen-instanz",
    
    "relUrl": "/optimist/guided_tour/step08/#schritt-8-löschen-der-ersten-eigenen-instanz"
  },"320": {
    "doc": "08: Löschen der ersten eigenen Instanz",
    "title": "Vorwort",
    "content": "Nachdem in Schritt 7: Die erste eigene Instanz die Instanz in der Kommandozeile angelegt wurde, wird in diesem Schritt erklärt, wie man eine Instanz wieder löscht. ",
    "url": "/optimist/guided_tour/step08/#vorwort",
    
    "relUrl": "/optimist/guided_tour/step08/#vorwort"
  },"321": {
    "doc": "08: Löschen der ersten eigenen Instanz",
    "title": "Vorgehen",
    "content": "Damit eine Instanz generell gelöscht werden kann, wird entweder der Name oder die ID der zu löschenden Instanz benötigt. Bei wenigen Instanzen in einem Stack, kann der Name für das Löschen verwendet werden. Sobald allerdings mehrere Instanzen verwendet werden, wird von uns empfohlen für das Löschen die ID zu nutzen, da Namen im Gegensatz zu IDs nicht einzigartig sind. Der OpenStackClient zeigt einem sonst an, dass es mehrere Instanzen mit dem entsprechenden Namen gibt. Um nun eine Liste aller verfügbaren Instanzen zu erhalten, kann openstack server list als Befehl ausgeführt werden: . $ openstack server list +--------------------------------------+--------------+--------+---------------------------------------------------+------------------------------------+ | ID | Name | Status | Networks | Image Name | +--------------------------------------+--------------+--------+---------------------------------------------------+------------------------------------+ | 801b3021-0c00-4566-881e-b50d47152e63 | singleserver | ACTIVE | single_internal_network=10.0.0.12, 185.116.245.39 | Ubuntu 16.04 Xenial Xerus - Latest | +--------------------------------------+--------------+--------+---------------------------------------------------+------------------------------------+ . Die angezeigte Liste listet alle verfügbaren Instanzen auf und enthält neben dem Namen der jeweiligen Instanz, auch die zugehörige ID. Um die im vorigen Schritt erstelle Instanz zu löschen, wird der Befehl openstack server delete ID verwendet, wobei “ID” durch die korrekte ID der Instanz ausgetauscht wird. In unserem Beispiel lautet der Befehl also wie folgt: . openstack server delete 801b3021-0c00-4566-881e-b50d47152e63 . Bei einer erneuten Ausgabe von openstack server list, sollte kein Server mehr angezeigt werden: . $ openstack server list $ . ",
    "url": "/optimist/guided_tour/step08/#vorgehen",
    
    "relUrl": "/optimist/guided_tour/step08/#vorgehen"
  },"322": {
    "doc": "08: Löschen der ersten eigenen Instanz",
    "title": "Abschluss",
    "content": "Nachdem im vorigen Schritt eine Instanz per Hand erstellt wurde, haben wir diese Instanz hier gelöscht. Außerdem konnte mit dem Befehl openstack server list eine Übersicht über alle Instanzen gewonnen werden. In Schritt 9: Die erste Security-Group wird an den bisherigen Erfahrungen angeknüpft und das gewonnene Wissen um das Thema Security-Groups erweitert. ",
    "url": "/optimist/guided_tour/step08/#abschluss",
    
    "relUrl": "/optimist/guided_tour/step08/#abschluss"
  },"323": {
    "doc": "08: Löschen der ersten eigenen Instanz",
    "title": "08: Löschen der ersten eigenen Instanz",
    "content": " ",
    "url": "/optimist/guided_tour/step08/",
    
    "relUrl": "/optimist/guided_tour/step08/"
  },"324": {
    "doc": "09: Die erste Security-Group",
    "title": "Schritt 9: Die erste Security-Group",
    "content": " ",
    "url": "/optimist/guided_tour/step09/#schritt-9-die-erste-security-group",
    
    "relUrl": "/optimist/guided_tour/step09/#schritt-9-die-erste-security-group"
  },"325": {
    "doc": "09: Die erste Security-Group",
    "title": "Vorwort",
    "content": "Standardmäßig ist jeglicher Zugriff auf eine Instanz von außerhalb verboten. Um Zugriff auf eine Instanz zu erlauben, muss (mindestens) eine Security Group definiert und der Instanz zugewiesen werden. Es ist möglich, alle Zugriffsregeln in einer Security Group zusammenzufassen, doch für komplexe Stacks macht es Sinn, die Regeln nach Aufgabe einzelner Instanzen in eigenen Security Groups zu hinterlegen. ",
    "url": "/optimist/guided_tour/step09/#vorwort",
    
    "relUrl": "/optimist/guided_tour/step09/#vorwort"
  },"326": {
    "doc": "09: Die erste Security-Group",
    "title": "Vorgehen",
    "content": "Der Grundbefehl für das erstellen einer Security Group lautet openstack security group create allow-ssh-from-anywhere --description Beispiel: . $ openstack security group create allow-ssh-from-anywhere --description Beispiel +-----------------+-----------------------------------------------------------------------------------------------------------------------------------------------------+ | Field | Value | +-----------------+-----------------------------------------------------------------------------------------------------------------------------------------------------+ | created_at | 2017-12-08T12:01:42Z | description | Beispiel | id | 1cab4a62-0fda-40d9-bac8-fd73275b472d | name | allow-ssh-from-anywhere | project_id | b15cde70d85749689e08106f973bb002 | revision_number | 2 | rules | created_at='2017-12-08T12:01:42Z', direction='egress', ethertype='IPv6', id='5a852e4b-1d79-4fe9-b359-64ca54c98501', | | updated_at='2017-12-08T12:01:42Z' | | created_at='2017-12-08T12:01:42Z', direction='egress', ethertype='IPv4', id='fa90a1ee-d3b9-40d4-9bb5-89fdd5005c02', | | updated_at='2017-12-08T12:01:42Z' | updated_at | 2017-12-08T12:01:42Z | +-----------------+-----------------------------------------------------------------------------------------------------------------------------------------------------+ . Damit eine Security Group nicht nur eine leere Hülle ist, kann der Befehl durch weitere Zusätze sinnvoll erweitert werden. Hier eine kurze Übersicht der wichtigsten Optionen: . | --protocol = Definition des genutzten Protokolls (mögliche Optionen: icmp, tcp, udp) | --dst-port = Gibt den Port oder die Range der Ports an. (22:22 ist Port 22, 1:[65535 würde alle Ports definieren)]{style=”color: rgb(34,34,34);”} | --remote-ip = Kann eine IP oder IP-Range definieren. (Default um den Zugang über alle IPs zu gewähren ist 0.0.0.0/0 | --ingress bzw. --egress = ingress definiert den eingehenden Verkehr, egress den ausgehenden | . Da die wichtigsten Optionen nun bekannt sind, kann jetzt eine Security Group erstellt werden, die es erlaubt theoretisch Zugriff per SSH zu erhalten. Der Befehl lautet openstack security group rule create allow-ssh-from-anywhere --protocol tcp --dst-port 22:22 --remote-ip 0.0.0.0/0 . $ openstack security group rule create allow-ssh-from-anywhere --protocol tcp --dst-port 22:22 --remote-ip 0.0.0.0/0 +-------------------+--------------------------------------+ | Field | Value | +-------------------+--------------------------------------+ | created_at | 2017-12-08T12:02:15Z | description | | direction | ingress | ether_type | IPv4 | id | 694a0573-b4c3-423c-847d-550f79e83f2b | name | None | port_range_max | 22 | port_range_min | 22 | project_id | b15cde70d85749689e08106f973bb002 | protocol | tcp | remote_group_id | None | remote_ip_prefix | 0.0.0.0/0 | revision_number | 0 | security_group_id | 1cab4a62-0fda-40d9-bac8-fd73275b472d | updated_at | 2017-12-08T12:02:15Z | +-------------------+--------------------------------------+ . Um zu prüfen, ob die Security Group korrekt angelegt wurde und um eine Übersicht über alle zu erhalten, kann folgender Befehl genutzt werden, openstack security group show allow-ssh-from-anywhere . $ openstack security group show allow-ssh-from-anywhere +-----------------+-----------------------------------------------------------------------------------------------------------------------------------------------------+ | Field | Value | +-----------------+-----------------------------------------------------------------------------------------------------------------------------------------------------+ | created_at | 2017-12-08T12:01:42Z | description | Beispiel | id | 1cab4a62-0fda-40d9-bac8-fd73275b472d | name | allow-ssh-from-anywhere | project_id | b15cde70d85749689e08106f973bb002 | revision_number | 3 | rules | created_at='2017-12-08T12:01:42Z', direction='egress', ethertype='IPv6', id='5a852e4b-1d79-4fe9-b359-64ca54c98501', | | updated_at='2017-12-08T12:01:42Z' | | created_at='2017-12-08T12:02:15Z', direction='ingress', ethertype='IPv4', id='694a0573-b4c3-423c-847d-550f79e83f2b', port_range_max='22', | | port_range_min='22', protocol='tcp', remote_ip_prefix='0.0.0.0/0', updated_at='2017-12-08T12:02:15Z' | | created_at='2017-12-08T12:01:42Z', direction='egress', ethertype='IPv4', id='fa90a1ee-d3b9-40d4-9bb5-89fdd5005c02', | | updated_at='2017-12-08T12:01:42Z' | updated_at | 2017-12-08T12:02:15Z | +-----------------+-----------------------------------------------------------------------------------------------------------------------------------------------------+ . ",
    "url": "/optimist/guided_tour/step09/#vorgehen",
    
    "relUrl": "/optimist/guided_tour/step09/#vorgehen"
  },"327": {
    "doc": "09: Die erste Security-Group",
    "title": "Abschluss",
    "content": "Nach dem erfolgreichen erstellen der Security-Group, ist der nächste Schritt ein Netzwerk hinzuzufügen. Dies erfolgt im Schritt 10: Zugriff aus dem Internet vorbereiten: Ein Netzwerk anlegen. ",
    "url": "/optimist/guided_tour/step09/#abschluss",
    
    "relUrl": "/optimist/guided_tour/step09/#abschluss"
  },"328": {
    "doc": "09: Die erste Security-Group",
    "title": "09: Die erste Security-Group",
    "content": " ",
    "url": "/optimist/guided_tour/step09/",
    
    "relUrl": "/optimist/guided_tour/step09/"
  },"329": {
    "doc": "10: Zugriff aus dem Internet vorbereiten; Ein Netzwerk anlegen",
    "title": "Schritt 10: Zugriff aus dem Internet vorbereiten: Ein Netzwerk anlegen",
    "content": " ",
    "url": "/optimist/guided_tour/step10/#schritt-10-zugriff-aus-dem-internet-vorbereiten-ein-netzwerk-anlegen",
    
    "relUrl": "/optimist/guided_tour/step10/#schritt-10-zugriff-aus-dem-internet-vorbereiten-ein-netzwerk-anlegen"
  },"330": {
    "doc": "10: Zugriff aus dem Internet vorbereiten; Ein Netzwerk anlegen",
    "title": "Vorwort",
    "content": "In Schritt 7 wurde zuerst eine Instanz manuell erstellt und in Schritt 9 dann eine Security Group. Nun ist der nächste Schritt ein virtuelles Netzwerk zu erstellen. ",
    "url": "/optimist/guided_tour/step10/#vorwort",
    
    "relUrl": "/optimist/guided_tour/step10/#vorwort"
  },"331": {
    "doc": "10: Zugriff aus dem Internet vorbereiten; Ein Netzwerk anlegen",
    "title": "Netzwerk",
    "content": "Den Start dafür macht das eigentliche Netzwerk. Wie bisher gibt es mehrere zusätzliche Optionen, die wie gewohnt mit dem Zusatz --help aufgelistet werden können. Um das Netzwerk zu erstellen, nutzen wir den Befehl: . $ openstack network create BeispielNetzwerk +---------------------------+--------------------------------------+ | Field | Value | +---------------------------+--------------------------------------+ | admin_state_up | UP | availability_zone_hints | | availability_zones | | created_at | 2017-12-08T12:06:38Z | description | | dns_domain | None | id | ff6d8654-66d6-4881-9528-2686bddcb6dc | ipv4_address_scope | None | ipv6_address_scope | None | is_default | False | mtu | 1500 | name | BeispielNetzwerk | port_security_enabled | True | project_id | b15cde70d85749689e08106f973bb002 | provider:network_type | None | provider:physical_network | None | provider:segmentation_id | None | qos_policy_id | None | revision_number | 2 | router:external | Internal | segments | None | shared | False | status | ACTIVE | subnets | | updated_at | 2017-12-08T12:06:38Z | +---------------------------+--------------------------------------+ . ",
    "url": "/optimist/guided_tour/step10/#netzwerk",
    
    "relUrl": "/optimist/guided_tour/step10/#netzwerk"
  },"332": {
    "doc": "10: Zugriff aus dem Internet vorbereiten; Ein Netzwerk anlegen",
    "title": "Subnet",
    "content": "Da das Netzwerk nun angelegt wurde, ist der nächste logische Schritt, ein zugehöriges Subnet. Auch das Subnet hat sehr viele zusätzliche Optionen, für das Beispiel werden folgende genutzt: . | --network = Gibt an, in welchem Netzwerk das Subnet angelegt werden soll | --subnet-range = CIDR des Subnets. Im Beispiel wird 192.168.2.0/24 verwendet | . Um das Subnet in das vorher erstellte Netzwerk zu integrieren und die CIDR zu definieren lautet der korrekte Befehl: . $ openstack subnet create BeispielSubnet --network BeispielNetzwerk --subnet-range 192.168.2.0/24 +-------------------------+--------------------------------------+ | Field | Value | +-------------------------+--------------------------------------+ | allocation_pools | 192.168.2.2-192.168.2.254 | cidr | 192.168.2.0/24 | created_at | 2017-12-08T12:09:07Z | description | | dns_nameservers | | enable_dhcp | True | gateway_ip | 192.168.2.1 | host_routes | | id | 984b24bf-db60-46a9-83c3-d68f6f1062e4 | ip_version | 4 | ipv6_address_mode | None | ipv6_ra_mode | None | name | BeispielSubnet | network_id | ff6d8654-66d6-4881-9528-2686bddcb6dc | project_id | b15cde70d85749689e08106f973bb002 | revision_number | 0 | segment_id | None | service_types | | subnetpool_id | None | updated_at | 2017-12-08T12:09:07Z | use_default_subnet_pool | None | +-------------------------+--------------------------------------+ . ",
    "url": "/optimist/guided_tour/step10/#subnet",
    
    "relUrl": "/optimist/guided_tour/step10/#subnet"
  },"333": {
    "doc": "10: Zugriff aus dem Internet vorbereiten; Ein Netzwerk anlegen",
    "title": "Router",
    "content": "Damit das Subnet auch sinnvoll genutzt werden kann, wird noch ein virtueller Router benötigt: . $ openstack router create BeispielRouter +-------------------------+--------------------------------------+ | Field | Value | +-------------------------+--------------------------------------+ | admin_state_up | UP | availability_zone_hints | | availability_zones | | created_at | 2017-12-08T12:09:49Z | description | | distributed | False | external_gateway_info | None | flavor_id | None | ha | False | id | bfb91c7f-acca-450a-aae0-c519ab563d38 | name | BeispielRouter | project_id | b15cde70d85749689e08106f973bb002 | revision_number | None | routes | | status | ACTIVE | updated_at | 2017-12-08T12:09:49Z | +-------------------------+--------------------------------------+ . Um eine Verbindung ins Internet zu ermöglichen, benötigt der Router ein externes Gateway, welches mit diesem Befehl gesetzt wird: . openstack router set BeispielRouter --external-gateway provider . Da nun schon die Verbindung hergestellt ist, wird dem Router nun noch das Subnet zugewiesen: . openstack router add subnet BeispielRouter BeispielSubnet . ",
    "url": "/optimist/guided_tour/step10/#router",
    
    "relUrl": "/optimist/guided_tour/step10/#router"
  },"334": {
    "doc": "10: Zugriff aus dem Internet vorbereiten; Ein Netzwerk anlegen",
    "title": "Port",
    "content": "Nachdem nun bereits der Router und das Subnet erstellt wurden, fehlt im letzten Schritt noch der zugehörige Port. Bei der Erstellung wird mit --network definiert, in welchem Netzwerk der Port verwendet werden soll: . $ openstack port create BeispielPort --network BeispielNetzwerk +-----------------------+----------------------------------------------------------------------------+ | Field | Value | +-----------------------+----------------------------------------------------------------------------+ | admin_state_up | UP | allowed_address_pairs | | binding_host_id | None | binding_profile | None | binding_vif_details | None | binding_vif_type | None | binding_vnic_type | normal | created_at | 2017-12-08T12:12:13Z | description | | device_id | | device_owner | | dns_assignment | None | dns_name | None | extra_dhcp_opts | | fixed_ips | ip_address='192.168.2.8', subnet_id='984b24bf-db60-46a9-83c3-d68f6f1062e4' | id | 31777c0a-a952-43ca-bb7f-11ad33926dae | ip_address | None | mac_address | fa:16:3e:09:88:c8 | name | BeispielPort | network_id | ff6d8654-66d6-4881-9528-2686bddcb6dc | option_name | None | option_value | None | port_security_enabled | True | project_id | b15cde70d85749689e08106f973bb002 | qos_policy_id | None | revision_number | 3 | security_group_ids | 3d3e3074-3087-4965-9a64-34a6d56193b9 | status | DOWN | subnet_id | None | updated_at | 2017-12-08T12:12:13Z | +-----------------------+----------------------------------------------------------------------------+ . ",
    "url": "/optimist/guided_tour/step10/#port",
    
    "relUrl": "/optimist/guided_tour/step10/#port"
  },"335": {
    "doc": "10: Zugriff aus dem Internet vorbereiten; Ein Netzwerk anlegen",
    "title": "Abschluss",
    "content": "Nachdem Router, Subnet und Port angelegt und diese miteinander verknüpft wurden, ist die Einrichtung des Beispielnetzwerks abgeschlossen und im nächsten Schritt fügen wir noch den Zugriff per IPv6 hinzu. ",
    "url": "/optimist/guided_tour/step10/#abschluss",
    
    "relUrl": "/optimist/guided_tour/step10/#abschluss"
  },"336": {
    "doc": "10: Zugriff aus dem Internet vorbereiten; Ein Netzwerk anlegen",
    "title": "10: Zugriff aus dem Internet vorbereiten; Ein Netzwerk anlegen",
    "content": " ",
    "url": "/optimist/guided_tour/step10/",
    
    "relUrl": "/optimist/guided_tour/step10/"
  },"337": {
    "doc": "11: Zugriff aus dem Internet vorbereiten; Wir ergänzen IPv6",
    "title": "Schritt 11: Zugriff aus dem Internet vorbereiten: Wir ergänzen IPv6",
    "content": " ",
    "url": "/optimist/guided_tour/step11/#schritt-11-zugriff-aus-dem-internet-vorbereiten-wir-erg%C3%A4nzen-ipv6",
    
    "relUrl": "/optimist/guided_tour/step11/#schritt-11-zugriff-aus-dem-internet-vorbereiten-wir-ergänzen-ipv6"
  },"338": {
    "doc": "11: Zugriff aus dem Internet vorbereiten; Wir ergänzen IPv6",
    "title": "Vorwort",
    "content": "In Schritt 10 wurde von uns bereits ein Netzwerk angelegt und in diesem Schritt erweitern wir selbiges um IPv6. Dabei nutzen wir die bereits bestehenden Router etc. Wichtig ist, dass die IPv4-Adresse auf dem ersten Interface läuft. Die Cloud Images sind so konzipiert, dass das primäre Interface mit DHCP vorkonfiguriert ist. Erst wenn das erfolgt ist, wird auf den Metadata Service zugegriffen um IPv6 überhaupt hoch zu fahren. ",
    "url": "/optimist/guided_tour/step11/#vorwort",
    
    "relUrl": "/optimist/guided_tour/step11/#vorwort"
  },"339": {
    "doc": "11: Zugriff aus dem Internet vorbereiten; Wir ergänzen IPv6",
    "title": "Subnet",
    "content": "Für die IPv6 Netze gibt es bereits einen Pool, aus dem man sich einfach ein eigenes Subnetz generieren lassen kann. Welche Pools es gibt, findet man mit diesem Befehl heraus: . $ openstack subnet pool list +--------------------------------------+---------------+---------------------+ | ID | Name | Prefixes | +--------------------------------------+---------------+---------------------+ | f541f3b6-af22-435a-9cbb-b233d12e74f4 | customer-ipv6 | 2a00:c320:1000::/48 | +--------------------------------------+---------------+---------------------+ . Aus diesem Pool kann man sich nun eigene Subnetze generieren lassen und die Prefixlänge von 64Bit ist dabei pro generiertem Subnet fest vorgegeben. Bei der Erstellung der Pools kann man die Subnets direkt mit angeben oder man überlässt es OpenStack. Dafür wird im Befehl openstack subnet create --network BeispielNetzwerk --ip-version 6 --use-default-subnet-pool --ipv6-address-mode dhcpv6-stateful --ipv6-ra-mode dhcpv6-stateful BeispielSubnetIPv6 einfach der Zusatz --use-default-subnet-pool genutzt. $ openstack subnet create --network BeispielNetzwerk --ip-version 6 --use-default-subnet-pool --ipv6-address-mode dhcpv6-stateful --ipv6-ra-mode dhcpv6-stateful BeispielSubnetIPv6 +-------------------------+----------------------------------------------------------+ | Field | Value | +-------------------------+----------------------------------------------------------+ | allocation_pools | 2a00:c320:1000:2::2-2a00:c320:1000:2:ffff:ffff:ffff:ffff | cidr | 2a00:c320:1000:2::/64 | created_at | 2017-12-08T12:41:42Z | description | | dns_nameservers | | enable_dhcp | True | gateway_ip | 2a00:c320:1000:2::1 | host_routes | | id | 0046c29b-a9b0-47c3-b5dd-704aa801704d | ip_version | 6 | ipv6_address_mode | dhcpv6-stateful | ipv6_ra_mode | dhcpv6-stateful | name | BeispielSubnetIPv6 | network_id | ff6d8654-66d6-4881-9528-2686bddcb6dc | project_id | b15cde70d85749689e08106f973bb002 | revision_number | 0 | segment_id | None | service_types | | subnetpool_id | f541f3b6-af22-435a-9cbb-b233d12e74f4 | updated_at | 2017-12-08T12:41:42Z | use_default_subnet_pool | True | +-------------------------+----------------------------------------------------------+ . ",
    "url": "/optimist/guided_tour/step11/#subnet",
    
    "relUrl": "/optimist/guided_tour/step11/#subnet"
  },"340": {
    "doc": "11: Zugriff aus dem Internet vorbereiten; Wir ergänzen IPv6",
    "title": "Router",
    "content": "Da nun das IPv6 Netz auch erstellt ist, werden wir in diesem Schritt das neue Netz mit dem in Schritt 10 erstellten Router verbinden. Dafür nutzen wir den Befehl: . openstack router add subnet BeispielRouter BeispielSubnetIPv6 . ",
    "url": "/optimist/guided_tour/step11/#router",
    
    "relUrl": "/optimist/guided_tour/step11/#router"
  },"341": {
    "doc": "11: Zugriff aus dem Internet vorbereiten; Wir ergänzen IPv6",
    "title": "Security Group",
    "content": "Die Regeln die wir zuvor in Schritt 9 angelegt haben, beziehen sich nur auf IPv4. Damit auch IPv6 genutzt werden kann, legen wir noch zwei weitere Regeln in den schon bestehenden SecurityGroups an. Um auch per IPv6 Zugriff per SSH auf die VM zu erlangen nutzen wir den Befehl: . $ openstack security group rule create --remote-ip \"::/0\" --protocol tcp --dst-port 22:22 --ethertype IPv6 --ingress allow-ssh-from-anywhere +-------------------+--------------------------------------+ | Field | Value | +-------------------+--------------------------------------+ | created_at | 2017-12-08T12:44:04Z | description | | direction | ingress | ether_type | IPv6 | id | 7d871e85-05fa-4620-b558-c6fc64076cde | name | None | port_range_max | 22 | port_range_min | 22 | project_id | b15cde70d85749689e08106f973bb002 | protocol | tcp | remote_group_id | None | remote_ip_prefix | ::/0 | revision_number | 0 | security_group_id | 1cab4a62-0fda-40d9-bac8-fd73275b472d | updated_at | 2017-12-08T12:44:04Z | +-------------------+--------------------------------------+ . Nun fehlt noch der Zugriff per ICMP, damit wir die VM auch über IPv6 per Ping erreichen können. Dies geht mit diesem Befehl: . $ openstack security group rule create --remote-ip \"::/0\" --protocol ipv6-icmp --ingress allow-ssh-from-anywhere +-------------------+--------------------------------------+ | Field | Value | +-------------------+--------------------------------------+ | created_at | 2017-12-08T12:44:44Z | description | | direction | ingress | ether_type | IPv6 | id | f63e4787-9965-4732-b9d2-20ce0fedc974 | name | None | port_range_max | None | port_range_min | None | project_id | b15cde70d85749689e08106f973bb002 | protocol | ipv6-icmp | remote_group_id | None | remote_ip_prefix | ::/0 | revision_number | 0 | security_group_id | 1cab4a62-0fda-40d9-bac8-fd73275b472d | updated_at | 2017-12-08T12:44:44Z | +-------------------+--------------------------------------+ . ",
    "url": "/optimist/guided_tour/step11/#security-group",
    
    "relUrl": "/optimist/guided_tour/step11/#security-group"
  },"342": {
    "doc": "11: Zugriff aus dem Internet vorbereiten; Wir ergänzen IPv6",
    "title": "Anpassungen am Betriebssystem",
    "content": "Startet man nun eine VM innerhalb des angelegten Netzes, bekommt diese eine IPv4, als auch eine IPv6 Adresse. Die Standardimages der Hersteller sind aber leider noch nicht für IPv6 vorkonfiguriert, weshalb tatsächlich nur die IPv4 Adresse in der VM ankommt. Nutzt man unsere bereitgestellten Heat Templates, sind die notwendigen Anpassungen bereits im Template enthalten. Um dies auch bei bestehenden Instanzen nachträglich auch zu ermöglichen, gibt es hier für verschiedene Distributionen einen Anleitung. Ubuntu 16.04 . Um IPv6 korrekt nutzen zu können, müssen folgende Dateien, mit dem angegeben Inhalt erstellt werden. | /etc/dhcp/dhclient6.conf . timeout 30; . | /etc/cloud/cloud.cfg.d/99-disable-network-config.cfg . network: {config: disabled} . | /etc/network/interfaces.d/lo.cfg . auto lo iface lo inet loopback . | /etc/network/interfaces.d/ens3.cfg . iface ens3 inet6 auto up sleep 5 up dhclient -1 -6 -cf /etc/dhcp/dhclient6.conf -lf /var/lib/dhcp/dhclient6.ens3.leases -v ens3 || true . | . Im Anschluss wird das entsprechende Interface neugestartet: . sudo ifdown ens3 &amp;&amp; sudo ifup ens3 . Die VM hat jetzt eine weitere IPv6 Adresse auf dem Interface, auf dem vorher nur die IPv4 Adresse existierte und kann somit auch per IPv6 korrekt erreicht werden. Damit man die beschriebenen Punkte nicht jedes mal manuell abarbeiten muss, kann man folgende cloud-init Konfiguration verwenden (Was cloud-init genau ist, erklären wir in Schritt 19: Unsere Instanz lernt IPv6): . #cloud-config write_files: - path: /etc/dhcp/dhclient6.conf content: \"timeout 30;\" - path: /etc/cloud/cloud.cfg.d/99-disable-network-config.cfg content: \"network: {config: disabled}\" - path: /etc/network/interfaces.d/lo.cfg content: | auto lo iface lo inet loopback - path: /etc/network/interfaces.d/ens3.cfg content: | iface ens3 inet6 auto up sleep 5 up dhclient -1 -6 -cf /etc/dhcp/dhclient6.conf -lf /var/lib/dhcp/dhclient6.ens3.leases -v ens3 || true runcmd: - [ ifdown, ens3] - [ ifup, ens3] . CentOS 7 . Die genannten Parameter müssen den angegebenen Dateien neu hinzugefügt oder falls diese bereits vorhanden sind ergänzt werden: . | /etc/sysconfig/network . NETWORKING_IPV6=yes . | /etc/sysconfig/network-scripts/ifcfg-eth0 . IPV6INIT=yes DHCPV6C=yes . | . Anschließend wird das entsprechende Interface neugestartet: . sudo ifdown eth0 &amp;&amp; sudo ifup eth0 . Die VM hat jetzt eine weitere IPv6 Adresse auf dem Interface, auf dem vorher nur die IPv4 Adresse existierte und kann somit auch per IPv6 korrekt erreicht werden. Damit man die beschriebenen Punkte nicht jedes mal manuell abarbeiten muss, kann man folgende cloud-init Konfiguration verwenden(Was cloud-init genau ist, erklären wir für Ubuntu 16.04 in Schritt 19: Unsere Instanz lernt IPv6: . #cloud-config write_files: - path: /etc/sysconfig/network owner: root:root permissions: '0644' content: | NETWORKING=yes NOZEROCONF=yes NETWORKING_IPV6=yes - path: /etc/sysconfig/network-scripts/ifcfg-eth0 owner: root:root permissions: '0644' content: | DEVICE=\"eth0\" BOOTPROTO=\"dhcp\" ONBOOT=\"yes\" TYPE=\"Ethernet\" USERCTL=\"yes\" PEERDNS=\"yes\" PERSISTENT_DHCLIENT=\"1\" IPV6INIT=yes DHCPV6C=yes runcmd: - [ ifdown, eth0] - [ ifup, eth0] . ",
    "url": "/optimist/guided_tour/step11/#anpassungen-am-betriebssystem",
    
    "relUrl": "/optimist/guided_tour/step11/#anpassungen-am-betriebssystem"
  },"343": {
    "doc": "11: Zugriff aus dem Internet vorbereiten; Wir ergänzen IPv6",
    "title": "Externer Zugriff",
    "content": "Wichtig: Diese VM ist ab sofort von überall auf der Welt über ihre IPv6 Adresse zu erreichen. Natürlich nur auf den Ports, die wir auch in den Security Groups aktiviert haben. Wir benötigen also keine weitere Floating IP um externen Zugriff auf diese VM zu ermöglichen. Es ist deshalb so wichtig zu erwähnen, da wir hier ein anderes Verhalten haben, als mit den IPv4 Adressen. Möchte man per IPv4 aus dem Internet auf diese VM zugreifen, muss man weiterhin auf die Floating IPs zurückgreifen. Hat man selbst lokal kein IPv6, möchte aber testen ob seine VM prinzipiell erreichbar ist, kann man auf Online Tools zurückgreifen, wie z.B. https://www.subnetonline.com/pages/ipv6-network-tools/online-ipv6-ping.php . ",
    "url": "/optimist/guided_tour/step11/#externer-zugriff",
    
    "relUrl": "/optimist/guided_tour/step11/#externer-zugriff"
  },"344": {
    "doc": "11: Zugriff aus dem Internet vorbereiten; Wir ergänzen IPv6",
    "title": "Abschluss",
    "content": "Nachdem im letzten Schritt bereits eine Verbindung per IPv4 erfolgte, wurde nun auch noch der Zugriff per IPv6 hinzugefügt. Im nächsten Schritt wird dann die Instanz aus Schritt 7 als Vorlage genutzt und erreichbar von außen. ",
    "url": "/optimist/guided_tour/step11/#abschluss",
    
    "relUrl": "/optimist/guided_tour/step11/#abschluss"
  },"345": {
    "doc": "11: Zugriff aus dem Internet vorbereiten; Wir ergänzen IPv6",
    "title": "11: Zugriff aus dem Internet vorbereiten; Wir ergänzen IPv6",
    "content": " ",
    "url": "/optimist/guided_tour/step11/",
    
    "relUrl": "/optimist/guided_tour/step11/"
  },"346": {
    "doc": "12: Eine nutzbare Instanz",
    "title": "Schritt 12: Eine nutzbare Instanz",
    "content": " ",
    "url": "/optimist/guided_tour/step12/#schritt-12-eine-nutzbare-instanz",
    
    "relUrl": "/optimist/guided_tour/step12/#schritt-12-eine-nutzbare-instanz"
  },"347": {
    "doc": "12: Eine nutzbare Instanz",
    "title": "Vorwort",
    "content": "In Schritt 7 wurde bereits eine Instanz erstellt, diese konnte jedoch nur genutzt werden, wenn man ein paar Schritte übersprungen hat und das entsprechende Netzwerk mit erstellt. Es gab nur so die Möglichkeit eine Verbindung zu dieser herzustellen. Daher werden wir in diesem Schritt eine Instanz erstellen, die diese Problematik nicht mehr hat. ",
    "url": "/optimist/guided_tour/step12/#vorwort",
    
    "relUrl": "/optimist/guided_tour/step12/#vorwort"
  },"348": {
    "doc": "12: Eine nutzbare Instanz",
    "title": "Installation",
    "content": "Damit die Instanz all die fehlenden Einstellungen enthält, wird der Befehl aus Schritt 7 modifiziert: . openstack server create BeispielInstanz --flavor m1.small --key-name Beispiel --image \"Ubuntu 16.04 Xenial Xerus - Latest\" --security-group allow-ssh-from-anywhere --network=BeispielNetzwerk . $ openstack server create BeispielInstanz --flavor m1.small --key-name Beispiel --image \"Ubuntu 16.04 Xenial Xerus - Latest\" --security-group allow-ssh-from-anywhere --network=BeispielNetzwerk +-----------------------------+---------------------------------------------------------------------------+ | Field | Value | +-----------------------------+---------------------------------------------------------------------------+ | OS-DCF:diskConfig | MANUAL | OS-EXT-AZ:availability_zone | es1 | OS-EXT-STS:power_state | NOSTATE | OS-EXT-STS:task_state | scheduling | OS-EXT-STS:vm_state | building | OS-SRV-USG:launched_at | None | OS-SRV-USG:terminated_at | None | accessIPv4 | | accessIPv6 | | addresses | | adminPass | jkSdvP3A9yo6 | config_drive | | created | 2017-12-08T12:52:37Z | flavor | m1.small (b7c4fa0b-7960-4311-a86b-507dbf58e8ac) | hostId | | id | 1de98aa4-7d2b-4427-a8a5-d369ea8bdaf5 | image | Ubuntu 16.04 Xenial Xerus - Latest (82242d21-d990-4fc2-92a5-c7bd7820e790) | key_name | Beispiel | name | BeispielInstanz | progress | 0 | project_id | b15cde70d85749689e08106f973bb002 | properties | | security_groups | name='allow-ssh-from-anywhere' | status | BUILD | updated | 2017-12-08T12:52:37Z | user_id | 9bf501f4c3d14b7eb0f1443efe80f656 | volumes_attached | +-----------------------------+---------------------------------------------------------------------------+ . Genutzt wurden die folgenden Parameter: . | --flavor = Gibt den Flavor (Größe) der Instanz an. Eine Übersicht aller verfügbaren Flavors kann mit  openstack flavor list aufgerufen werden | --key-name = Der Name des zu verwendenden SSH-Keys | --image = Gibt an welches Image für die Instanz genutzt wird. Auch ist es möglich, sich im Vorfeld eine Liste aller verfügbaren Images anzusehen \"openstack image list\" | --security-group = Gibt an, welche Security-Groups genutzt wird | --network = Mit diesem Parameter kann unter anderem das gewünscht Netzwerk angeben werden (in alten Versionen des clients --nic net-id=&lt;network&gt;) | . Damit die erstellte Instanz über das Internet erreichbar ist, wird noch eine IP benötigt, welche zuerst angelegt wird. $ openstack floating ip create provider +---------------------+--------------------------------------+ | Field | Value | +---------------------+--------------------------------------+ | created_at | 2017-12-08T12:53:37Z | description | | fixed_ip_address | None | floating_ip_address | 185.116.245.65 | floating_network_id | 54258498-a513-47da-9369-1a644e4be692 | id | 84eca140-9ac1-42c3-baf6-860ba920a23c | name | None | port_id | None | project_id | b15cde70d85749689e08106f973bb002 | revision_number | 0 | router_id | None | status | DOWN | updated_at | 2017-12-08T12:53:37Z | +---------------------+--------------------------------------+ . Die gerade erstellte IP wird im nächsten Schritt mit der vorher erstellten Instanz verbunden. openstack server add floating ip BeispielInstanz 185.116.245.145 . ",
    "url": "/optimist/guided_tour/step12/#installation",
    
    "relUrl": "/optimist/guided_tour/step12/#installation"
  },"349": {
    "doc": "12: Eine nutzbare Instanz",
    "title": "Nutzung",
    "content": "Die erstellte Instanz ist nun erreichbar. Um zu testen, ob alle Schritte funktionieren, stellen wir nun eine Verbindung per SSH her. Wichtig ist hierbei, dass eine Verbindung nur funktioniert, wenn der weiter oben genutzte SSH Key auch existiert und verwendet wird (Siehe Schritt 6: Einen eigenen SSH-Key per Konsole erstellen und nutzen): . $ ssh ubuntu@185.116.245.145 The authenticity of host '185.116.245.145 (185.116.245.145)' can't be established. ECDSA key fingerprint is SHA256:kbSkm8eJA0748911RkbWK2/pBVQOjJBASD1oOOXalk. Are you sure you want to continue connecting (yes/no)? yes Warning: Permanently added '185.116.245.145' (ECDSA) to the list of known hosts. Enter passphrase for key '/Users/ubuntu/.ssh/id_rsa': . ",
    "url": "/optimist/guided_tour/step12/#nutzung",
    
    "relUrl": "/optimist/guided_tour/step12/#nutzung"
  },"350": {
    "doc": "12: Eine nutzbare Instanz",
    "title": "Clean-Up",
    "content": "Für den Fall, dass die in den vorigen Schritten erstellten Bestandteile wieder gelöscht werden sollen, muss das in folgender Reihenfolge, mit dem entsprechenden Befehl geschehen. Sollte man dies nicht befolgen, kann es dazu führen, dass Bestandteile sich nicht löschen lassen. | Instanz . | openstack server delete BeispielInstanz | . | Floating-IP . | openstack floating ip delete 185.116.245.145 | . | Port . | openstack port delete BeispielPort | . | Router . | openstack router delete BeispielRouter | . | Subnet . | openstack subnet delete BeispielSubnet | . | Netzwerk . | openstack network delete BeispielNetzwerk | . | . ",
    "url": "/optimist/guided_tour/step12/#clean-up",
    
    "relUrl": "/optimist/guided_tour/step12/#clean-up"
  },"351": {
    "doc": "12: Eine nutzbare Instanz",
    "title": "Abschluss",
    "content": "In den Schritten 7 bis 11 wurde eine Instanz Schritt für Schritt erstellt und jeder Schritt hat einen Teilbereich hinzugefügt (inklusive Netzwerk und einer eigenen Security-Group). Im nächsten Schritt lösen wir uns von einzelnen Instanzen und erstellen einen Stack. ",
    "url": "/optimist/guided_tour/step12/#abschluss",
    
    "relUrl": "/optimist/guided_tour/step12/#abschluss"
  },"352": {
    "doc": "12: Eine nutzbare Instanz",
    "title": "12: Eine nutzbare Instanz",
    "content": " ",
    "url": "/optimist/guided_tour/step12/",
    
    "relUrl": "/optimist/guided_tour/step12/"
  },"353": {
    "doc": "13: Der strukturierte Weg zu einer Instanz (mit Stacks)",
    "title": "Schritt 13: Der strukturierte Weg zu einer Instanz (mit Stacks)",
    "content": " ",
    "url": "/optimist/guided_tour/step13/#schritt-13-der-strukturierte-weg-zu-einer-instanz-mit-stacks",
    
    "relUrl": "/optimist/guided_tour/step13/#schritt-13-der-strukturierte-weg-zu-einer-instanz-mit-stacks"
  },"354": {
    "doc": "13: Der strukturierte Weg zu einer Instanz (mit Stacks)",
    "title": "Vorwort",
    "content": "Nachdem die erste Instanz, inklusive einer Security Group und virtuellem Netzwerk, in einem sehr aufwendigen Prozess per Hand angelegt wurde, wird in diesem Schritt eine Alternative aufgezeigt. Wie diese funktioniert und was es hierbei zu beachten gibt, wird auch wie gewohnt Schritt für Schritt erklärt. Voraussetzung für die folgenden Schritte ist die Installation des Paketes python-heatclient. Siehe Schritt 4: Der Weg vom Horizon auf die Kommandozeile. ",
    "url": "/optimist/guided_tour/step13/#vorwort",
    
    "relUrl": "/optimist/guided_tour/step13/#vorwort"
  },"355": {
    "doc": "13: Der strukturierte Weg zu einer Instanz (mit Stacks)",
    "title": "Installation",
    "content": "Anstatt einzelne Instanzen von Hand anzulegen, kann man beliebige OpenStack Resourcen (z.B. Instanzen, Netzwerke, Router, Security Groups) auch in einem definierten Verbund, einem so genannten Stack (oder Heat Stack) betreiben. Dadurch werden sie logisch zusammengefaßt und können einfach erstellt und gelöscht werden – je nach Verwendungszweck. Wir verwenden in diesem Schritt Heat-Templates, die auch Grundlage für folgende Schritte sind. Die bisherigen Schritte 9 bis 11, lassen sich einfach in einem Template zusammenfassen. Um nicht zu theoretisch zu bleiben, gibt es ein Template unter BeispielTemplates. Dieses Template erstellt einen Stack, in diesem ist eine Instanz, zwei Security Groups, ein virtuelles Netzwerk (inkl. Router, Port, Subnet) und eine Floating-IP enthalten. Um den Stack zu erstellen, ist es notwendig, sich im Verzeichnis des Templates zu befinden und dann folgenden Befehl zu nutzen: . $ openstack stack create -t SingleServer.yaml --parameter key_name=Beispiel SingleServer --wait 2017-12-08 13:13:43Z [SingleServer]: CREATE_IN_PROGRESS Stack CREATE started 2017-12-08 13:13:44Z [SingleServer.router]: CREATE_IN_PROGRESS state changed 2017-12-08 13:13:45Z [SingleServer.enable_traffic]: CREATE_IN_PROGRESS state changed 2017-12-08 13:13:45Z [SingleServer.enable_traffic]: CREATE_COMPLETE state changed 2017-12-08 13:13:46Z [SingleServer.internal_network_id]: CREATE_IN_PROGRESS state changed 2017-12-08 13:13:46Z [SingleServer.router]: CREATE_COMPLETE state changed 2017-12-08 13:13:46Z [SingleServer.internal_network_id]: CREATE_COMPLETE state changed 2017-12-08 13:13:47Z [SingleServer.enable_ssh]: CREATE_IN_PROGRESS state changed 2017-12-08 13:13:47Z [SingleServer.subnet]: CREATE_IN_PROGRESS state changed 2017-12-08 13:13:47Z [SingleServer.enable_ssh]: CREATE_COMPLETE state changed 2017-12-08 13:13:48Z [SingleServer.start-config]: CREATE_IN_PROGRESS state changed 2017-12-08 13:13:48Z [SingleServer.subnet]: CREATE_COMPLETE state changed 2017-12-08 13:13:49Z [SingleServer.router_subnet_bridge]: CREATE_IN_PROGRESS state changed 2017-12-08 13:13:49Z [SingleServer.port]: CREATE_IN_PROGRESS state changed 2017-12-08 13:13:50Z [SingleServer.start-config]: CREATE_COMPLETE state changed 2017-12-08 13:13:50Z [SingleServer.port]: CREATE_COMPLETE state changed 2017-12-08 13:13:50Z [SingleServer.host]: CREATE_IN_PROGRESS state changed 2017-12-08 13:13:52Z [SingleServer.router_subnet_bridge]: CREATE_COMPLETE state changed 2017-12-08 13:13:53Z [SingleServer.floating_ip]: CREATE_IN_PROGRESS state changed 2017-12-08 13:13:55Z [SingleServer.floating_ip]: CREATE_COMPLETE state changed 2017-12-08 13:14:05Z [SingleServer.host]: CREATE_COMPLETE state changed 2017-12-08 13:14:06Z [SingleServer]: CREATE_COMPLETE Stack CREATE completed successfully +---------------------+-------------------------------------------------+ | Field | Value | +---------------------+-------------------------------------------------+ | id | 0f5cdf0e-24cc-4292-a0bc-adf2e9f8618a | stack_name | SingleServer | description | A simple template to deploy your first instance | creation_time | 2017-12-08T13:13:42Z | updated_time | None | stack_status | CREATE_COMPLETE | stack_status_reason | Stack CREATE completed successfully | +---------------------+-------------------------------------------------+ . Der Befehl openstack stack create erstellt dabei den Stack, mit -t SingleServer.yaml wird festgelegt, dass das angegebene Template verwendet werden soll. Außerdem wird mit --parameter key_name=BEISPIEL noch ein SSH-Schlüssel angegeben und mit SingleServer wird der Name des Stacks festgelegt. Der letzte Bestandteil --wait zeigt alle Zwischenschritte der Erstellung an. (Siehe das obere Bild) . Nach kurzer Zeit ist der Stack inkl. Instanz erstellt und kann per SSH erreicht werden. Die notwendige IP lässt sich mit folgendem Befehl herausfinden, wichtig ist dabei die korrekte ID zu nutzen: . $ openstack stack output show 0f5cdf0e-24cc-4292-a0bc-adf2e9f8618a instance_fip +--------------+---------------------------------+ | Field | Value | +--------------+---------------------------------+ | description | External IP address of instance | output_key | instance_fip | output_value | 185.116.245.70 | +--------------+---------------------------------+ . Nun stellen wir noch die Verbindung per SSH her: . $ ssh ubuntu@185.116.245.70 The authenticity of host '185.116.245.70 (185.116.245.70)' can't be established. ECDSA key fingerprint is SHA256:kbSkm8eJA0748911RkbWK2/pBVQOjJBASD1oOOXalk. Are you sure you want to continue connecting (yes/no)? yes Warning: Permanently added '185.116.245.70' (ECDSA) to the list of known hosts. Enter passphrase for key '/Users/ubuntu/.ssh/id_rsa': . ",
    "url": "/optimist/guided_tour/step13/#installation",
    
    "relUrl": "/optimist/guided_tour/step13/#installation"
  },"356": {
    "doc": "13: Der strukturierte Weg zu einer Instanz (mit Stacks)",
    "title": "Abschluss",
    "content": "Wir haben die Schritte 9 bis 11 in einem Heat-Template zusammengefaßt und können sie nun leicht wiederholen. In den folgenden Schritten gehen wir weiter auf Heat ein und zeigen weiterführende Beispiele. ",
    "url": "/optimist/guided_tour/step13/#abschluss",
    
    "relUrl": "/optimist/guided_tour/step13/#abschluss"
  },"357": {
    "doc": "13: Der strukturierte Weg zu einer Instanz (mit Stacks)",
    "title": "13: Der strukturierte Weg zu einer Instanz (mit Stacks)",
    "content": " ",
    "url": "/optimist/guided_tour/step13/",
    
    "relUrl": "/optimist/guided_tour/step13/"
  },"358": {
    "doc": "14: Unsere ersten Schritte mit Heat",
    "title": "Schritt 14: Unsere ersten Schritte mit Heat",
    "content": " ",
    "url": "/optimist/guided_tour/step14/#schritt-14-unsere-ersten-schritte-mit-heat",
    
    "relUrl": "/optimist/guided_tour/step14/#schritt-14-unsere-ersten-schritte-mit-heat"
  },"359": {
    "doc": "14: Unsere ersten Schritte mit Heat",
    "title": "Vorwort",
    "content": "Nachdem im letzten Schritt die erste Berührung mit einem Template erfolgte, ist der nächste Schritt, zu verstehen, wie Templates mit Heat aufgebaut sind und funktionieren. Dieser Schritt erklärt nur die einzelnen Punkte eines Templates und soll diese näher bringen. ",
    "url": "/optimist/guided_tour/step14/#vorwort",
    
    "relUrl": "/optimist/guided_tour/step14/#vorwort"
  },"360": {
    "doc": "14: Unsere ersten Schritte mit Heat",
    "title": "Das Template",
    "content": "Jedes Heat-Template folgt der gleichen Struktur und diese ist wie folgt aufgebaut: . heat_template_version: 2016-10-14   description: # Die Beschreibung des Templates (optional)   parameter_groups: # Die Definition der Eingabeparameter Gruppen und deren Reihenfolge   parameters: # Die Definition der Eingabeparameter   resources: # Die Definition der Ressourcen des Templates   outputs: # Die Definition der Ausgangsparameter   conditions: # Die Definition der Bedingungen . ",
    "url": "/optimist/guided_tour/step14/#das-template",
    
    "relUrl": "/optimist/guided_tour/step14/#das-template"
  },"361": {
    "doc": "14: Unsere ersten Schritte mit Heat",
    "title": "Heat Template Version",
    "content": "Die Template Version kann tatsächlich nicht willkürlich gewählt werden, sondern hat feste Vorgaben. Diese unterscheiden sich in den möglichen Befehlen und aktuell sind folgende Daten möglich: . | 2013-05-23 | 2014-10-16 | 2015-04-30 | 2015-10-15 | 2016-04-08 | 2016-10-14 | 2017-02-24 | . ",
    "url": "/optimist/guided_tour/step14/#heat-template-version",
    
    "relUrl": "/optimist/guided_tour/step14/#heat-template-version"
  },"362": {
    "doc": "14: Unsere ersten Schritte mit Heat",
    "title": "Description",
    "content": "Die Description oder auch Beschreibung ist ein komplett optionales Feld. Das bedeutet, dass das Feld nicht genutzt werden muss. Es bietet sich allerdings an, denn damit kann das Template in seinen Grundzügen beschrieben und direkt auf mögliche Besonderheiten hingewiesen werden. Auch besteht die Möglichkeit jederzeit eine Zeile mit dem Zeichen # auszukommentieren und dadurch das Template nach den jeweiligen Bedürfnissen zu verändern oder mehr Kommentare zu verfassen. ",
    "url": "/optimist/guided_tour/step14/#description",
    
    "relUrl": "/optimist/guided_tour/step14/#description"
  },"363": {
    "doc": "14: Unsere ersten Schritte mit Heat",
    "title": "Parameter Groups",
    "content": "In diesem Bereich ist es möglich zu spezifizieren, wie Parameter gruppiert werden sollen und die Reihenfolge für die Gruppierung festzulegen. Die Gruppen sind in eine Liste aufgegliedert, welche wiederum die einzelnen Parameter enthält. Jeder Parameter sollte nur einer Gruppe zugeordnet sein, damit es später zu keinen Problemen führt. Jede Parameter Group ist dabei wie folgt aufgebaut: . parameter_groups: - label: &lt;Name der Gruppe&gt; description: &lt;Beschreibung der Gruppe&gt; parameters: - &lt;Name des Parameters&gt; - &lt;Name des Parameters&gt; . | label: Name der Gruppe. | description: Dieses Attribut gibt  die Möglichkeit die Parameter Gruppe zu beschreiben und so für jeden verständlich zu machen, wofür diese genutzt wird. | parameter: Eine Auflistung aller Parameter die für diese Parameter Gruppe gelten. | Name des Parameters: Der in der Parameter Sektion definiert wurde. | . ",
    "url": "/optimist/guided_tour/step14/#parameter-groups",
    
    "relUrl": "/optimist/guided_tour/step14/#parameter-groups"
  },"364": {
    "doc": "14: Unsere ersten Schritte mit Heat",
    "title": "Parameter",
    "content": "Diese Sektion erlaubt es die eingegebenen Parameter zu spezifizieren, welche für die Ausführung benötigt werden. Die Parameter werden typischerweise dafür genutzt, jedes deployment zu individualisieren. Dabei wird jeder Parameter in einem separaten Block definiert, wobei am Anfang immer der Parameter genannt wird und dann weitere Attribute diesem zugeordnet werden.  parameters: &lt;Parameter Name&gt;: type: &lt;string | number | json | comma_delimited_list | boolean&gt; label: &lt;Name des Parameters&gt; description: &lt;Beschreibung des Parameters&gt; default: &lt;Standardwert des Parameters&gt; hidden: &lt;true | false&gt; constraints: &lt;Vorgaben für den Parameter&gt; immutable: &lt;true | false&gt; . | Parameter Name: Der Name des Parameters. | type: Der Typ des Parameters. Unterstützte Typen: string, number, json, comma_delimited_list, boolean. | label: Name des Parameters. (optional) | description: Dieses Attribut gibt die Möglichkeit den Parameter zu beschreiben und so für jeden verständlich zu machen, wofür dieser genutzt wird. (optional) | default: Der vorgegebene Wert des Parameters. Dieser Wert wird genutzt, wenn durch den User kein spezifischer Wert festgelegt werden soll. (optional) | hidden: Gibt an ob der Parameter bei einer Abfrage nach der Erstellung angezeigt wird oder versteckt ist. (optional und per Default auf false gesetzt) | constraints: Hier kann eine Liste von Vorgaben definiert werden. Sollten diese beim deployment nicht erfüllt werden, schlägt die Erstellung des Stacks fehl. | immutable: Definiert ob der Parameter aktualisiert werden kann. Für den Fall, dass der Parameter auf true gesetzt ist und sich der Wert bei einem stack update ändert, schlägt das Update fehl. | . ",
    "url": "/optimist/guided_tour/step14/#parameter",
    
    "relUrl": "/optimist/guided_tour/step14/#parameter"
  },"365": {
    "doc": "14: Unsere ersten Schritte mit Heat",
    "title": "Resources",
    "content": "Dieser Punkt definiert die aktuell genutzten Resourcen, die bei der Erstellung des Stacks genutzt werden. Dabei wird jede Ressource in einem eigenen Block definiert: . resources: &lt;ID der Ressource&gt;: type: &lt;Ressourcen Typ&gt; properties: &lt;Name der Eigenschaft&gt;: &lt;Wert der Eigenschaft&gt; metadata: &lt;Ressourcen spezifische Metadaten&gt; depends_on: &lt;Ressourcen ID oder eine Liste der IDs&gt; update_policy: &lt;Update Regel&gt; deletion_policy: &lt;Regel für das Löschen&gt; external_id: &lt;Externe Ressourcen ID&gt; condition: &lt;Name der Kondition oder Ausdruck oder boolean&gt; . | ID der Ressource: Diese muss einzigartig im Bereich der Ressourcen sein. Es darf durchaus ein sprechender Name gewählt werden z. B. (web_network). | type: Der Typ der Ressource, wie zum Beispiel OS::NEUTRON::SecurityGroup (für eine Security Group). (benötigt) | properties: Eine Liste der Ressourcen spezifischen Eigenschaften. (optional) | metadata: Hier können für die jeweilige Ressource spezifische Metadaten hinterlegt werden. (optional) | depends_on: Hier können verschiedene Abhängigkeiten zu anderen Ressourcen hinterlegt werden. (optional) | update_policy: Hier können Update Regeln festgelegt werden. Die Voraussetzung dafür ist, dass die entsprechende Resource dies auch unterstützt. (optional) | deletion_policy: Hier werden die Regeln für das Löschen festgelegt. Erlaubt sind Delete, Retain und Snapshot. Mit der heat_template_version 2016-10-14 ist nun auch die Kleinschreibung der Werte erlaubt . | external_id: Falls notwendig, können externe Ressourcen IDs verwendet werden | condition: Anhand der Kondition wird entschieden, ob die Ressource erstellt wird oder nicht. (optional) | . ",
    "url": "/optimist/guided_tour/step14/#resources",
    
    "relUrl": "/optimist/guided_tour/step14/#resources"
  },"366": {
    "doc": "14: Unsere ersten Schritte mit Heat",
    "title": "Output",
    "content": "Die Output-Sektion definiert die ausgegeben Parameter die für den User oder auch für andere Templates nach dem Erstellen des Stacks verfügbar sind. Das können zum Beispiel Parameter wie die IP-Adresse der erstellten Instanz oder auch die URL der erstellten Web-App sein. Auch wird jeder Parameter in einem eigenen Block definiert: . outputs: &lt;Name des Parameters&gt;: description: &lt;Beschreibung&gt; value: &lt;Wert des Parameters&gt; condition: &lt;Name der Kondition oder Ausdruck oder boolean&gt; . | Name des Parameters: Dieser muss wieder einzigartig sein. | description: Es kann eine Beschreibung für den Parameter hinterlegt werden. (optional) | value: Hier wird der Wert des Parameters vermerkt. (benötigt) | condition: Hier kann eine Kondition für den Parameter festlegt werden. (optional) | . ",
    "url": "/optimist/guided_tour/step14/#output",
    
    "relUrl": "/optimist/guided_tour/step14/#output"
  },"367": {
    "doc": "14: Unsere ersten Schritte mit Heat",
    "title": "Condition",
    "content": "Dieser Bereich legt die verschiedenen Konditionen fest und das auf Basis der eingegebenen Parameter des Users, beim Erstellen oder updaten des Stacks. Die Konditionen können mit Resources, Resource properties und Outputs verbunden werden. Auch dieser Bereich folgt wieder einem Muster: . conditions: &lt;Name der Condition1&gt;: {Bezeichnung1} &lt;Name der Condition2&gt;: {Bezeichnung2} . | Name der Condition: Dieser muss wieder einzigartig im Bereich der Condition sein. | Bezeichnung: Bei der Bezeichnung wird erwartet, dass sie ein true oder false zurückgibt. | . ",
    "url": "/optimist/guided_tour/step14/#condition",
    
    "relUrl": "/optimist/guided_tour/step14/#condition"
  },"368": {
    "doc": "14: Unsere ersten Schritte mit Heat",
    "title": "Abschluss",
    "content": "In diesem Schritt wurden wichtige Bestandteile eines Heat-Templates vorgestellt. Mit diesem Wissen wird im nächsten Schritt das erste eigene Heat-Template erstellt. ",
    "url": "/optimist/guided_tour/step14/#abschluss",
    
    "relUrl": "/optimist/guided_tour/step14/#abschluss"
  },"369": {
    "doc": "14: Unsere ersten Schritte mit Heat",
    "title": "14: Unsere ersten Schritte mit Heat",
    "content": " ",
    "url": "/optimist/guided_tour/step14/",
    
    "relUrl": "/optimist/guided_tour/step14/"
  },"370": {
    "doc": "15: Das erste eigene Heat Orchestration Template (HOT)",
    "title": "Schritt 15: Das erste eigene Heat Orchestration Template (HOT)",
    "content": " ",
    "url": "/optimist/guided_tour/step15/#schritt-15-das-erste-eigene-heat-orchestration-template-hot",
    
    "relUrl": "/optimist/guided_tour/step15/#schritt-15-das-erste-eigene-heat-orchestration-template-hot"
  },"371": {
    "doc": "15: Das erste eigene Heat Orchestration Template (HOT)",
    "title": "Vorwort",
    "content": "Im folgenden Schritt sind die wichtigsten Elemente eines Templates erläutert worden und auf dieses Wissen, wird in diesem Schritt aufgebaut. ",
    "url": "/optimist/guided_tour/step15/#vorwort",
    
    "relUrl": "/optimist/guided_tour/step15/#vorwort"
  },"372": {
    "doc": "15: Das erste eigene Heat Orchestration Template (HOT)",
    "title": "Der Anfang",
    "content": "Dieser ist bei jedem Template gleich und ist immer heat_template_version . Für das Beispiel wird Version 2016-10-14 genutzt und somit sieht das Template erst einmal so aus: . heat_template_version: 2016-10-14 . Nachdem die heat_template_version festgelegt ist, wird dem Template nun eine Beschreibung hinzugefügt: . heat_template_version: 2016-10-14   description: Ein einfaches Template, um eine Instanz zu erstellen . Nachdem die Beschreibung in das Template integriert wurde, wird nun eine Ressource, also die Instanz hinzugefügt. Dabei sind einige Punkte zu beachten, starten wir zunächst mit der Ressource. Wichtig ist dabei, dass eine Strukturierung mit Leerzeichen genutzt wird. Dies dient der Übersichtlichkeit, außerdem würden Tabstops zu Fehlern führen und nur so kann das Template korrekt ausgeführt werden: . heat_template_version: 2016-10-14 description: Ein einfaches Template, um eine Instanz zu erstellen resources: Instanz: . Der nächste Schritt ist dann den Typ der Ressource zu benennen. Eine ausführliche Liste aller verfügbaren Typen befindet sich unter anderem in der offiziellen OpenStack Dokumentation . Da im Beispiel eine Instanz erstellt werden soll, ist der Typ dann folgender: . heat_template_version: 2016-10-14 description: Ein einfaches Template, um eine Instanz zu erstellen resources: Instanz: type: OS::Nova::Server . Nach dem Typ sind dann die Eigenschaften der nächste Punkt. Im Beispiel soll dies ein SSH-Key, ein Flavor und ein Image sein: . heat_template_version: 2016-10-14 description: Ein einfaches Template, um eine Instanz zu erstellen resources: Instanz: type: OS::Nova::Server properties: key_name: BeispielKey image: Ubuntu 16.04 Xenial Xerus - Latest flavor: m1.small . ",
    "url": "/optimist/guided_tour/step15/#der-anfang",
    
    "relUrl": "/optimist/guided_tour/step15/#der-anfang"
  },"373": {
    "doc": "15: Das erste eigene Heat Orchestration Template (HOT)",
    "title": "Abschluss",
    "content": "Damit ist das Erste eigenes Template fertiggestellt und kann, wenn es gespeichert wird, einfach mit dem OpenStackClienten wie in Schritt 13: “Der strukturierte Weg zu einer Instanz (mit Stacks)” beschrieben, gestartet werden. ",
    "url": "/optimist/guided_tour/step15/#abschluss",
    
    "relUrl": "/optimist/guided_tour/step15/#abschluss"
  },"374": {
    "doc": "15: Das erste eigene Heat Orchestration Template (HOT)",
    "title": "15: Das erste eigene Heat Orchestration Template (HOT)",
    "content": " ",
    "url": "/optimist/guided_tour/step15/",
    
    "relUrl": "/optimist/guided_tour/step15/"
  },"375": {
    "doc": "16: Wir lernen Heat besser kennen",
    "title": "Schritt 16: Wir lernen Heat besser kennen",
    "content": " ",
    "url": "/optimist/guided_tour/step16/#schritt-16-wir-lernen-heat-besser-kennen",
    
    "relUrl": "/optimist/guided_tour/step16/#schritt-16-wir-lernen-heat-besser-kennen"
  },"376": {
    "doc": "16: Wir lernen Heat besser kennen",
    "title": "Vorwort",
    "content": "Bisher konnte der Eindruck entstehen, das Heat und das manuelle Erstellen per Kommandozeile genau so viel Zeit in Anspruch nimmt, was beim einmaligen Erstellen auch stimmt. Dadurch das nun ein Template existiert, können wir diese Grundlage immer wieder nutzen und im Zweifel weiter entwickeln. Damit dies auch möglich ist, wird in diesem Schritt weiter auf Heat eingegangen. ",
    "url": "/optimist/guided_tour/step16/#vorwort",
    
    "relUrl": "/optimist/guided_tour/step16/#vorwort"
  },"377": {
    "doc": "16: Wir lernen Heat besser kennen",
    "title": "Parameter",
    "content": "Da das Ganze aufgebaut werden soll, ist es zunächst sinnvoll, bekannte oder individuelle Parameter zu definieren. In diesem Kontext wird der vorgegebene SSH-Key ersetzt und statt einem festen Wert, wird er als individueller Parameter definiert, der beim Start angegeben werden kann: . heat_template_version: 2014-10-16   parameters: key_name: type: string . Wie bisher gelernt, beginnt das Template mit der Version und wird dann mit parameters fortgeführt. Nach dem Parameter wird der Name, welcher individuell benannt werden kann, vergeben. Auch ist es notwendig den Typ anzugeben, in diesem Fall ist es string. Nachdem der Parameter festgelegt ist, nutzen wir als Vorlage das vorige Template und ergänzen es. Damit sieht das Template dann so aus: . heat_template_version: 2014-10-16 parameters: key_name: type: string resources: Instanz: type: OS::Nova::Server properties: key_name: BeispielKey image: Ubuntu 16.04 Xenial Xerus - Latest flavor: m1.small . Um das Template zu komplettieren, wird key_name durch den vorher definierten Parameter ersetzt. Der Befehl dafür lautet get_param. Dieser sagt aus, dass er einen definierten Parameter nutzen soll und damit das Template weiß, welchen Parameter er nutzen soll, ergänzen wir den Befehl get_param um key_name: . heat_template_version: 2014-10-16 parameters: key_name: type: string resources: Instanz: type: OS::Nova::Server properties: key_name: { get_param: key_name } image: Ubuntu 16.04 Xenial Xerus - Latest flavor: m1.small . ",
    "url": "/optimist/guided_tour/step16/#parameter",
    
    "relUrl": "/optimist/guided_tour/step16/#parameter"
  },"378": {
    "doc": "16: Wir lernen Heat besser kennen",
    "title": "Abschluss",
    "content": "Das Template wurde jetzt bereits über einen frei definierbaren Parameter erweitert und im nächsten Schritt wird das Netzwerk hinzugefügt. ",
    "url": "/optimist/guided_tour/step16/#abschluss",
    
    "relUrl": "/optimist/guided_tour/step16/#abschluss"
  },"379": {
    "doc": "16: Wir lernen Heat besser kennen",
    "title": "16: Wir lernen Heat besser kennen",
    "content": " ",
    "url": "/optimist/guided_tour/step16/",
    
    "relUrl": "/optimist/guided_tour/step16/"
  },"380": {
    "doc": "17: Das Netzwerk im Heat",
    "title": "Schritt 17: Das Netzwerk im Heat",
    "content": " ",
    "url": "/optimist/guided_tour/step17/#schritt-17-das-netzwerk-im-heat",
    
    "relUrl": "/optimist/guided_tour/step17/#schritt-17-das-netzwerk-im-heat"
  },"381": {
    "doc": "17: Das Netzwerk im Heat",
    "title": "Vorwort",
    "content": "Im letzten Schritt war ein individueller Parameter das Ziel und in diesem das komplette Netzwerk. ",
    "url": "/optimist/guided_tour/step17/#vorwort",
    
    "relUrl": "/optimist/guided_tour/step17/#vorwort"
  },"382": {
    "doc": "17: Das Netzwerk im Heat",
    "title": "Das Template",
    "content": "Um nicht bei Null zu starten, dient das Template aus dem vorigen Schritt als Vorlage. Wichtig ist dabei, dass direkt ein neuer Parameter hinzufügt wird, genauer die ID des öffentlichen Netzwerks: . heat_template_version: 2014-10-16 parameters: key_name: type: string public_network_id: type: string default: provider resources: Instanz: type: OS::Nova::Server properties: key_name: { get_param: key_name } image: Ubuntu 16.04 Xenial Xerus - Latest flavor: m1.small . ",
    "url": "/optimist/guided_tour/step17/#das-template",
    
    "relUrl": "/optimist/guided_tour/step17/#das-template"
  },"383": {
    "doc": "17: Das Netzwerk im Heat",
    "title": "Netzwerk",
    "content": "Nachdem dieser Parameter eingefügt wurde, ist es Zeit das Netzwerk in das Template einzufügen. Hierbei handelt es sich um eine weitere Ressource und wird unter dem Punkt resources eingefügt. Der zugehörige Typ lautet OS::Neutron::Net: . heat_template_version: 2014-10-16 parameters: key_name: type: string public_network_id: type: string default: provider resources: Instanz: type: OS::Nova::Server properties: key_name: { get_param: key_name } image: Ubuntu 16.04 Xenial Xerus - Latest flavor: m1.small Netzwerk: type: OS::Neutron::Net properties: name: BeispielNetzwerk . ",
    "url": "/optimist/guided_tour/step17/#netzwerk",
    
    "relUrl": "/optimist/guided_tour/step17/#netzwerk"
  },"384": {
    "doc": "17: Das Netzwerk im Heat",
    "title": "Der Port",
    "content": "Der nächste Schritt ist dann der Port, der Typ lautet dafür OS::Neutron::Port. Wichtig ist, dass der Port in das bestehende Netzwerk eingegliedert wird und die Instanz dem Port zuzuordnen ist. Um dies zu erreichen, wird erneut ein get Befehl genutzt und statt dem Parameter eine Ressource eingebunden: . heat_template_version: 2014-10-16 parameters: key_name: type: string public_network_id: type: string default: provider resources: Instanz: type: OS::Nova::Server properties: key_name: { get_param: key_name } image: Ubuntu 16.04 Xenial Xerus - Latest flavor: m1.small Netzwerk: type: OS::Neutron::Net properties: name: BeispielNetzwerk Port: type: OS::Neutron::Port properties: network: { get_resource: Netzwerk } . ",
    "url": "/optimist/guided_tour/step17/#der-port",
    
    "relUrl": "/optimist/guided_tour/step17/#der-port"
  },"385": {
    "doc": "17: Das Netzwerk im Heat",
    "title": "Der Router",
    "content": "Nachdem Netzwerk und Port, wird nun ein Router (Typ = OS::Neutron::Router) in das Template eingebunden. Bei diesem Typ ist es wichtig, das öffentliche Netzwerk einzubinden: . heat_template_version: 2014-10-16 parameters: key_name: type: string public_network_id: type: string default: provider resources: Instanz: type: OS::Nova::Server properties: key_name: { get_param: key_name } image: Ubuntu 16.04 Xenial Xerus - Latest flavor: m1.small Netzwerk: type: OS::Neutron::Net properties: name: BeispielNetzwerk Port: type: OS::Neutron::Port properties: network: { get_resource: Netzwerk } Router: type: OS::Neutron::Router properties: external_gateway_info: { \"network\": { get_param: public_network_id } name: BeispielRouter . ",
    "url": "/optimist/guided_tour/step17/#der-router",
    
    "relUrl": "/optimist/guided_tour/step17/#der-router"
  },"386": {
    "doc": "17: Das Netzwerk im Heat",
    "title": "Das Subnet",
    "content": "Der vorletzte Schritt ist das Subnet (Typ = OS::Neutron::Subnet ) . In selbigem werden eigene Nameserver eintragen, die Informationen des Netzwerks eingebunden, die IP-Version sowie der IP-Adressraum festgelegt und die verfügbaren IPs definiert: . heat_template_version: 2014-10-16 parameters: key_name: type: string public_network_id: type: string default: provider resources: Instanz: type: OS::Nova::Server properties: key_name: { get_param: key_name } image: Ubuntu 16.04 Xenial Xerus - Latest flavor: m1.small Netzwerk: type: OS::Neutron::Net properties: name: BeispielNetzwerk Port: type: OS::Neutron::Port properties: network: { get_resource: Netzwerk } Router: type: OS::Neutron::Router properties: external_gateway_info: { \"network\": { get_param: public_network_id } name: BeispielRouter Subnet: type: OS::Neutron::Subnet properties: name: BeispielSubnet dns_nameservers: - 8.8.8.8 - 8.8.4.4 network: { get_resource: Netzwerk } ip_version: 4 cidr: 10.0.0.0/24 allocation_pools: - { start: 10.0.0.10, end: 10.0.0.250 } . ",
    "url": "/optimist/guided_tour/step17/#das-subnet",
    
    "relUrl": "/optimist/guided_tour/step17/#das-subnet"
  },"387": {
    "doc": "17: Das Netzwerk im Heat",
    "title": "Subnet Bridge",
    "content": "Im letzten Schritt wird eine Subnet Bridge (Typ = OS::Neutron::RouterInterface) angelegt, also eine Brücke zwischen Router und Subnet. Auch gibt es hier eine weitere neue Komponente, genauer depends_on. Damit können wir Resource erstellen lassen, die nur dann gebaut werden, wenn es die referenzierte Resource auch gibt.   . In unserem Beispiel wird die Bridge zwischen Subnet und Router nur gebaut, wenn es auch ein Subnet gibt. heat_template_version: 2014-10-16 parameters: key_name: type: string public_network_id: type: string default: provider resources: Instanz: type: OS::Nova::Server properties: key_name: { get_param: key_name } image: Ubuntu 16.04 Xenial Xerus - Latest flavor: m1.small Netzwerk: type: OS::Neutron::Net properties: name: BeispielNetzwerk Port: type: OS::Neutron::Port properties: network: { get_resource: Netzwerk } Router: type: OS::Neutron::Router properties: external_gateway_info: { \"network\": { get_param: public_network_id } } name: BeispielRouter Subnet: type: OS::Neutron::Subnet properties: name: BeispielSubnet dns_nameservers: - 8.8.8.8 - 8.8.4.4 network: { get_resource: Netzwerk } ip_version: 4 cidr: 10.0.0.0/24 allocation_pools: - { start: 10.0.0.10, end: 10.0.0.250 } Router_Subnet_Bridge: type: OS::Neutron::RouterInterface depends_on: Subnet properties: router: { get_resource: Router } subnet: { get_resource: Subnet } . ",
    "url": "/optimist/guided_tour/step17/#subnet-bridge",
    
    "relUrl": "/optimist/guided_tour/step17/#subnet-bridge"
  },"388": {
    "doc": "17: Das Netzwerk im Heat",
    "title": "Abschluss",
    "content": "Nachdem nun das komplette Netzwerk eingerichtet wurde, wird im nächsten Schritt eine eigene Security Group erstellt und zusätzlich der Instanz eine öffentliche IP-Adresse zugewiesen. ",
    "url": "/optimist/guided_tour/step17/#abschluss",
    
    "relUrl": "/optimist/guided_tour/step17/#abschluss"
  },"389": {
    "doc": "17: Das Netzwerk im Heat",
    "title": "17: Das Netzwerk im Heat",
    "content": " ",
    "url": "/optimist/guided_tour/step17/",
    
    "relUrl": "/optimist/guided_tour/step17/"
  },"390": {
    "doc": "18: Unsere Instanz wird von außen per IPv4 erreichbar",
    "title": "Schritt 18: Unsere Instanz wird von außen per IPv4 erreichbar",
    "content": " ",
    "url": "/optimist/guided_tour/step18/#schritt-18-unsere-instanz-wird-von-au%C3%9Fen-per-ipv4-erreichbar",
    
    "relUrl": "/optimist/guided_tour/step18/#schritt-18-unsere-instanz-wird-von-außen-per-ipv4-erreichbar"
  },"391": {
    "doc": "18: Unsere Instanz wird von außen per IPv4 erreichbar",
    "title": "Vorwort",
    "content": "Im letzten Schritt wurde das komplette Netzwerk eingerichtet und es ist nun an der Zeit, die Instanz von außen zu erreichen (u.a. per ICMP und SSH Zugriff). ",
    "url": "/optimist/guided_tour/step18/#vorwort",
    
    "relUrl": "/optimist/guided_tour/step18/#vorwort"
  },"392": {
    "doc": "18: Unsere Instanz wird von außen per IPv4 erreichbar",
    "title": "Floating-IP",
    "content": "Den Anfang macht die öffentliche IP-Adresse, welche auch als Ressource hinzugefügt wird. (Der zugehörige Typ lautet OS::Neutron::FloatingIP). Wichtig ist, dass der Floating IP der entsprechende Port und welches das öffentliche Netz genutzt wird, mitgeteilt wird: . heat_template_version: 2014-10-16 parameters: key_name: type: string public_network_id: type: string default: provider resources: Instanz: type: OS::Nova::Server properties: key_name: { get_param: key_name } image: Ubuntu 16.04 Xenial Xerus - Latest flavor: m1.small Netzwerk: type: OS::Neutron::Net properties: name: BeispielNetzwerk Port: type: OS::Neutron::Port properties: network: { get_resource: Netzwerk } Router: type: OS::Neutron::Router properties: external_gateway_info: { \"network\": { get_param: public_network_id } } name: BeispielRouter Subnet: type: OS::Neutron::Subnet properties: name: BeispielSubnet dns_nameservers: - 8.8.8.8 - 8.8.4.4 network: { get_resource: Netzwerk } ip_version: 4 cidr: 10.0.0.0/24 allocation_pools: - { start: 10.0.0.10, end: 10.0.0.250 } Router_Subnet_Bridge: type: OS::Neutron::RouterInterface depends_on: Subnet properties: router: { get_resource: Router } subnet: { get_resource: Subnet } Floating_IP: type: OS::Neutron::FloatingIP properties: floating_network: { get_param: public_network_id } port_id: { get_resource: Port } . ",
    "url": "/optimist/guided_tour/step18/#floating-ip",
    
    "relUrl": "/optimist/guided_tour/step18/#floating-ip"
  },"393": {
    "doc": "18: Unsere Instanz wird von außen per IPv4 erreichbar",
    "title": "Security Groups",
    "content": "Wird das oben geschriebene Template gestartet, würde die Instanz erstellt werden, nur kann diese aufgrund der voreingestellten Security Group nicht erreicht werden. Um dies zu ändern, wird eine Security Group (Typ = OS::Neutron::SecurityGroup). Auch gibt es einige Besonderheiten zu beachten, es wird zum einen mit Regeln (rules) gearbeitet und zum anderen müssen selbige noch dem Port zugewiesen werden. So kann die direction (Richtung des Traffics) in ingress (eingehend) oder egress (ausgehend) eingeteilt, der entsprechende Port oder auch die Range der Ports definiert und auch das Protokoll festgelegt werden. Außerdem kann mit remote_ip_prefix noch festgelegt werden, wer die Instanz erreicht (falls nötig). heat_template_version: 2014-10-16 parameters: key_name: type: string public_network_id: type: string default: provider resources: Instanz: type: OS::Nova::Server properties: key_name: { get_param: key_name } image: Ubuntu 16.04 Xenial Xerus - Latest flavor: m1.small Netzwerk: type: OS::Neutron::Net properties: name: BeispielNetzwerk Port: type: OS::Neutron::Port properties: network: { get_resource: Netzwerk } Router: type: OS::Neutron::Router properties: external_gateway_info: { \"network\": { get_param: public_network_id } } name: BeispielRouter Subnet: type: OS::Neutron::Subnet properties: name: BeispielSubnet dns_nameservers: - 8.8.8.8 - 8.8.4.4 network: { get_resource: Netzwerk } ip_version: 4 cidr: 10.0.0.0/24 allocation_pools: - { start: 10.0.0.10, end: 10.0.0.250 } Router_Subnet_Bridge: type: OS::Neutron::RouterInterface depends_on: Subnet properties: router: { get_resource: Router } subnet: { get_resource: Subnet } Floating_IP: type: OS::Neutron::FloatingIP properties: floating_network: { get_param: public_network_id } port_id: { get_resource: Port } Sec_SSH: type: OS::Neutron::SecurityGroup properties: description: Diese Security Group erlaubt den eingehenden SSH-Traffic über Port22 und ICMP name: Ermöglicht SSH (Port22) und ICMP rules: - { direction: ingress, remote_ip_prefix: 0.0.0.0/0, port_range_min: 22, port_range_max: 22, protocol: tcp } - { direction: ingress, remote_ip_prefix: 0.0.0.0/0, protocol: icmp } . ",
    "url": "/optimist/guided_tour/step18/#security-groups",
    
    "relUrl": "/optimist/guided_tour/step18/#security-groups"
  },"394": {
    "doc": "18: Unsere Instanz wird von außen per IPv4 erreichbar",
    "title": "Abschluss",
    "content": "Die erstellte Instanz ist von außen erreichbar inklusive einer öffentliche IP. Im nächsten Schritt wird die Instanz per CloudConfig angepasst. ",
    "url": "/optimist/guided_tour/step18/#abschluss",
    
    "relUrl": "/optimist/guided_tour/step18/#abschluss"
  },"395": {
    "doc": "18: Unsere Instanz wird von außen per IPv4 erreichbar",
    "title": "18: Unsere Instanz wird von außen per IPv4 erreichbar",
    "content": " ",
    "url": "/optimist/guided_tour/step18/",
    
    "relUrl": "/optimist/guided_tour/step18/"
  },"396": {
    "doc": "19: Unsere Instanz lernt IPv6",
    "title": "Schritt 19: Unsere Instanz lernt IPv6",
    "content": " ",
    "url": "/optimist/guided_tour/step19/#schritt-19-unsere-instanz-lernt-ipv6",
    
    "relUrl": "/optimist/guided_tour/step19/#schritt-19-unsere-instanz-lernt-ipv6"
  },"397": {
    "doc": "19: Unsere Instanz lernt IPv6",
    "title": "Vorwort",
    "content": "Nachdem im letzten Schritt die Instanz mit einer öffentlichen IPv4 Adresse versehen wurde und diese auch per SSH erreichbar ist, wird es nun Zeit die Instanz selber anzupassen. Dafür nutzen wir in diesem Schritt CloudConfig und passen auch die Security Group an. ",
    "url": "/optimist/guided_tour/step19/#vorwort",
    
    "relUrl": "/optimist/guided_tour/step19/#vorwort"
  },"398": {
    "doc": "19: Unsere Instanz lernt IPv6",
    "title": "CloudConfig",
    "content": "Ist eine Ressource und wird daher auch unter resources geführt. (Typ = OS::HEAT::CloudConfig) . Es gibt sehr viele Möglichkeiten, was alles in einer Instanz mit CloudConfig bearbeiten werden kann. Im diesem Schritt beschäftigen wir uns damit, alles notwendige für IPv6 vorzubereiten. Der Start macht hierbei das erstellen der entsprechenden Dateien mit dem notwendigen Inhalt, den wir bereits aus Schritt 11: Zugriff aus dem Internet vorbereiten: Wir ergänzen IPv6 kennen und nutzen CloudConfig in der cloud_config den Befehl write_files: . heat_template_version: 2014-10-16 parameters: key_name: type: string public_network_id: type: string default: provider resources: Instanz: type: OS::Nova::Server properties: key_name: { get_param: key_name } image: Ubuntu 16.04 Xenial Xerus - Latest flavor: m1.small networks: - port: { get_resource: Port }   Instanz-Config: type: OS::Heat::CloudConfig properties: cloud_config: write_files: - path: /etc/dhcp/dhclient6.conf content: \"timeout 30;\" - path: /etc/cloud/cloud.cfg.d/99-disable-network-config.cfg content: \"network: {config: disabled}\" - path: /etc/network/interfaces.d/lo.cfg content: | auto lo iface lo inet loopback - path: /etc/network/interfaces.d/ens3.cfg content: | iface ens3 inet6 auto up sleep 5 up dhclient -1 -6 -cf /etc/dhcp/dhclient6.conf -lf /var/lib/dhcp/dhclient6.ens3.leases -v ens3 || true Netzwerk: type: OS::Neutron::Net properties: name: BeispielNetzwerk Port: type: OS::Neutron::Port properties: network: { get_resource: Netzwerk } security_groups: { get_resource: Sec_SSH } Router: type: OS::Neutron::Router properties: external_gateway_info: { \"network\": { get_param: public_network_id } } name: BeispielRouter Subnet: type: OS::Neutron::Subnet properties: name: BeispielSubnet dns_nameservers: - #MussNochEingetragenWerden - #MussNochEingetragenWerden network: { get_resource: Netzwerk } ip_version: 4 cidr: 10.0.0.0/24 allocation_pools: - { start: 10.0.0.10, end: 10.0.0.250 } Router_Subnet_Bridge: type: OS::Neutron::RouterInterface depends_on: Subnet properties: router: { get_resource: Router } subnet: { get_resource: Subnet } Floating_IP: type: OS::Neutron::FloatingIP properties: floating_network: { get_param: public_network_id } port_id: { get_resource: Port } Sec_SSH: type: OS::Neutron::SecurityGroup properties: description: Diese Security Group erlaubt den eingehenden SSH-Traffic über Port22 und ICMP name: Ermöglicht SSH (Port22) und ICMP rules: - { direction: ingress, remote_ip_prefix: 0.0.0.0/0, port_range_min: 22, port_range_max: 22, protocol:tcp } - { direction: ingress, remote_ip_prefix: 0.0.0.0/0, protocol: icmp } . Wir haben die Dateien erstellt und den entsprechenden Inhalt eingefügt. Wie in Schritt 11: Zugriff aus dem Internet vorbereiten: Wir ergänzen IPv6 beschrieben, ist es noch notwendig das Interface mit dem Befehl runcmd neu zustarten. heat_template_version: 2014-10-16 parameters: key_name: type: string public_network_id: type: string default: provider resources: Instanz: type: OS::Nova::Server properties: key_name: { get_param: key_name } image: Ubuntu 16.04 Xenial Xerus - Latest flavor: m1.small networks: - port: { get_resource: Port } Instanz-Config: type: OS::Heat::CloudConfig properties: cloud_config: write_files: - path: /etc/dhcp/dhclient6.conf content: \"timeout 30;\" - path: /etc/cloud/cloud.cfg.d/99-disable-network-config.cfg content: \"network: {config: disabled}\" - path: /etc/network/interfaces.d/lo.cfg content: | auto lo iface lo inet loopback - path: /etc/network/interfaces.d/ens3.cfg content: | iface ens3 inet6 auto up sleep 5 up dhclient -1 -6 -cf /etc/dhcp/dhclient6.conf -lf /var/lib/dhcp/dhclient6.ens3.leases -v ens3 || true runcmd: - [ ifdown, ens3] - [ ifup, ens3] Netzwerk: type: OS::Neutron::Net properties: name: BeispielNetzwerk Port: type: OS::Neutron::Port properties: network: { get_resource: Netzwerk } security_groups: { get_resource: Sec_SSH } Router: type: OS::Neutron::Router properties: external_gateway_info: { \"network\": { get_param: public_network_id } } name: BeispielRouter Subnet: type: OS::Neutron::Subnet properties: name: BeispielSubnet dns_nameservers: - 8.8.8.8 - 8.8.4.4 network: { get_resource: Netzwerk } ip_version: 4 cidr: 10.0.0.0/24 allocation_pools: - { start: 10.0.0.10, end: 10.0.0.250 } Router_Subnet_Bridge: type: OS::Neutron::RouterInterface depends_on: Subnet properties: router: { get_resource: Router } subnet: { get_resource: Subnet } Floating_IP: type: OS::Neutron::FloatingIP properties: floating_network: { get_param: public_network_id } port_id: { get_resource: Port } Sec_SSH: type: OS::Neutron::SecurityGroup properties: description: Diese Security Group erlaubt den eingehenden SSH-Traffic über Port22 und ICMP name: Ermöglicht SSH (Port22) und ICMP rules: - { direction: ingress, remote_ip_prefix: 0.0.0.0/0, port_range_min: 22, port_range_max: 22, protocol:tcp } - { direction: ingress, remote_ip_prefix: 0.0.0.0/0, protocol: icmp } . Im letzten Schritt passen wir die Security Group an, damit auch ein Zugriff über IPv6 möglich ist. heat_template_version: 2014-10-16 parameters: key_name: type: string public_network_id: type: string default: provider resources: Instanz: type: OS::Nova::Server properties: key_name: { get_param: key_name } image: Ubuntu 16.04 Xenial Xerus - Latest flavor: m1.small networks: - port: { get_resource: Port } Instanz-Config: type: OS::Heat::CloudConfig properties: cloud_config: write_files: - path: /etc/dhcp/dhclient6.conf content: \"timeout 30;\" - path: /etc/cloud/cloud.cfg.d/99-disable-network-config.cfg content: \"network: {config: disabled}\" - path: /etc/network/interfaces.d/lo.cfg content: | auto lo iface lo inet loopback - path: /etc/network/interfaces.d/ens3.cfg content: | iface ens3 inet6 auto up sleep 5 up dhclient -1 -6 -cf /etc/dhcp/dhclient6.conf -lf /var/lib/dhcp/dhclient6.ens3.leases -v ens3 || true runcmd: - [ ifdown, ens3] - [ ifup, ens3] Netzwerk: type: OS::Neutron::Net properties: name: BeispielNetzwerk Port: type: OS::Neutron::Port properties: network: { get_resource: Netzwerk } security_groups: { get_resource: Sec_SSH } Router: type: OS::Neutron::Router properties: external_gateway_info: { \"network\": { get_param: public_network_id } } name: BeispielRouter Subnet: type: OS::Neutron::Subnet properties: name: BeispielSubnet dns_nameservers: - 8.8.8.8 - 8.8.4.4 network: { get_resource: Netzwerk } ip_version: 4 cidr: 10.0.0.0/24 allocation_pools: - { start: 10.0.0.10, end: 10.0.0.250 } Router_Subnet_Bridge: type: OS::Neutron::RouterInterface depends_on: Subnet properties: router: { get_resource: Router } subnet: { get_resource: Subnet } Floating_IP: type: OS::Neutron::FloatingIP properties: floating_network: { get_param: public_network_id } port_id: { get_resource: Port } Sec_SSH: type: OS::Neutron::SecurityGroup properties: description: Diese Security Group erlaubt den eingehenden SSH-Traffic über Port22 und ICMP name: Ermöglicht SSH (Port22) und ICMP rules: - { direction: ingress, remote_ip_prefix: 0.0.0.0/0, port_range_min: 22, port_range_max: 22, protocol:tcp } - { direction: ingress, remote_ip_prefix: 0.0.0.0/0, protocol: icmp } - { direction: ingress, remote_ip_prefix: \"::/0\", port_range_min: 22, port_range_max: 22, protocol: tcp, ethertype: IPv6 } - { direction: ingress, remote_ip_prefix: \"::/0\", protocol: ipv6-icmp, ethertype: IPv6 } . ",
    "url": "/optimist/guided_tour/step19/#cloudconfig",
    
    "relUrl": "/optimist/guided_tour/step19/#cloudconfig"
  },"399": {
    "doc": "19: Unsere Instanz lernt IPv6",
    "title": "Abschluss",
    "content": "Wir haben nun die Möglichkeit Instanzen per Cloud-Init anzupassen und IPv6 nutzbar gemacht. Im nächsten und letzten Schritt werden wir mehrere Instanzen per Heat starten. ",
    "url": "/optimist/guided_tour/step19/#abschluss",
    
    "relUrl": "/optimist/guided_tour/step19/#abschluss"
  },"400": {
    "doc": "19: Unsere Instanz lernt IPv6",
    "title": "19: Unsere Instanz lernt IPv6",
    "content": " ",
    "url": "/optimist/guided_tour/step19/",
    
    "relUrl": "/optimist/guided_tour/step19/"
  },"401": {
    "doc": "20: Mehrere Instanzen gleichzeitig erstellen",
    "title": "Schritt 20: Mehrere Instanzen gleichzeitig erstellen",
    "content": " ",
    "url": "/optimist/guided_tour/step20/#schritt-20-mehrere-instanzen-gleichzeitig-erstellen",
    
    "relUrl": "/optimist/guided_tour/step20/#schritt-20-mehrere-instanzen-gleichzeitig-erstellen"
  },"402": {
    "doc": "20: Mehrere Instanzen gleichzeitig erstellen",
    "title": "Vorwort",
    "content": "Nachdem im Schritt 15 eine Instanz inklusive aller wichtigen Einstellungen angelegt wurde, ist der nächste Schritt, mehr als eine Instanz per Template zu starten. In diesem Schritt, werden zwei Instanzen erstellt, die ein gemeinsames Netzwerk nutzen. ",
    "url": "/optimist/guided_tour/step20/#vorwort",
    
    "relUrl": "/optimist/guided_tour/step20/#vorwort"
  },"403": {
    "doc": "20: Mehrere Instanzen gleichzeitig erstellen",
    "title": "Start",
    "content": "Neben den beiden Instanzen wird auch das Template aufgeteilt und in zwei Dateien erstellt. Dies hat verschiedene Teile und den Start macht ein simples Template, welches nur ein Net und ein Subnet enthält: . heat_template_version: 2014-10-16 description: Ein simples Template welches 2 Instanzen erstellt resources: BeispielNet: type: OS::Neutron::Net properties: name: BeispielNet BeispielSubnet: type: OS::Neutron::Subnet properties: name: BeispielSubnet dns_nameservers: - 8.8.8.8 - 8.8.4.4 network: {get_resource: BeispielNet} ip_version: 4 cidr: 10.0.0.0/24 allocation_pools: - {start: 10.0.0.10, end: 10.0.0.250} . Dies stellt das Grundgerüst des Stacks dar und wird vorerst als Gruppen.yaml gespeichert. Die Instanzen selber werden in einer zweiten Datei BeispielServer.yaml beschrieben, welche dem gleichen Aufbau wie in den vorigen Schritten folgt. Um image: zu füllen kann wahlweise der Image Name oder die Image-ID benutzt werden. Eine korrekte Image-ID bzw. einen korrekten Namen erhält man mit openstack image list. Es ist wichtig, dass kein Server-Namen definiert wird und network_id auch keinen Eintrag erfährt: . heat_template_version: 2014-10-16 description: Ein einzelner Server der durch eine Ressourcen Gruppe verwendet wird parameters: network_id: type: string server_name: type: string resources: Instanz: type: OS::Nova::Server properties: user_data_format: RAW image: Ubuntu 16.04 Xenial Xerus - Latest flavor: m1.small name: { get_param: server_name } networks: - port: { get_resource: BeispielPort } BeispielPort: type: OS::Neutron::Port properties: network: { get_param: network_id } . Nachdem die Datei fertiggestellt wurde, wird diese wie oben beschrieben als BeispielServer.yaml gespeichert. Um weiter fortzufahren, wird die Arbeit am ursprünglichen Template (Gruppen.yaml) fortgesetzt. Hier gilt es nun, das zweite erstellte Template als RessourceGroup einzubinden. Auch ist so direkt die Möglichkeit gegeben, die Anzahl der Instanzen, die Namen etc. anzugeben: . heat_template_version: 2014-10-16 description: Ein simples Template welches 2 Instanzen erstellt resources:   BeispielInstanzen: type: OS::Heat::ResourceGroup depends_on: BeispielSubnet properties: count: 2 resource_def: type: BeispielServer.yaml properties: network_id: { get_resource: BeispielNet } server_name: BeispielInstanz_%index% BeispielNet: type: OS::Neutron::Net properties: name: BeispielNet BeispielSubnet: type: OS::Neutron::Subnet properties: name: BeispielSubnet dns_nameservers: - 8.8.8.8 - 8.8.4.4 network: {get_resource: BeispielNet} ip_version: 4 cidr: 10.0.0.0/24 allocation_pools: - {start: 10.0.0.10, end: 10.0.0.250} . Nachdem die Arbeit an Gruppen.yaml abgeschlossen wurde, kann der so erstellte Stack direkt mit dem OpenStackClient gestartet werden: . openstack stack create -t Gruppen.yaml &lt;Name des Stacks&gt; . ",
    "url": "/optimist/guided_tour/step20/#start",
    
    "relUrl": "/optimist/guided_tour/step20/#start"
  },"404": {
    "doc": "20: Mehrere Instanzen gleichzeitig erstellen",
    "title": "Abschluss",
    "content": "Nachdem am Anfang der Guided Tour noch Instanzen per Hand erstellt wurden, können nun bereits mehrere Instanzen gleichzeitig per Template ausgerollt werden und stellen einen guten Startpunkt für die Administration von OpenStack dar. ",
    "url": "/optimist/guided_tour/step20/#abschluss",
    
    "relUrl": "/optimist/guided_tour/step20/#abschluss"
  },"405": {
    "doc": "20: Mehrere Instanzen gleichzeitig erstellen",
    "title": "20: Mehrere Instanzen gleichzeitig erstellen",
    "content": " ",
    "url": "/optimist/guided_tour/step20/",
    
    "relUrl": "/optimist/guided_tour/step20/"
  },"406": {
    "doc": "21: Eine Instanz von einem SSD-Volume starten",
    "title": "Schritt 21: Eine Instanz von einem SSD-Volume starten",
    "content": " ",
    "url": "/optimist/guided_tour/step21/#schritt-21-eine-instanz-von-einem-ssd-volume-starten",
    
    "relUrl": "/optimist/guided_tour/step21/#schritt-21-eine-instanz-von-einem-ssd-volume-starten"
  },"407": {
    "doc": "21: Eine Instanz von einem SSD-Volume starten",
    "title": "Vorwort",
    "content": "In den vorigen Schritten haben wir uns bereits eine eigene Instanz erstellt und auch die ersten Grundlagen in HEAT sind gelegt. Wir werden in diesem Schritt eine Instanz von einem Volume starten und dafür den SSD-Speicher nutzen. Auch hier gibt es mehrere Wege unser Ziel zu erreichen, daher werden wir in diesem Schritt sowohl das Horizon(Dashboard) nutzen, als auch unser HEAT Template aus Schritt 18 weiter modifizieren. ",
    "url": "/optimist/guided_tour/step21/#vorwort",
    
    "relUrl": "/optimist/guided_tour/step21/#vorwort"
  },"408": {
    "doc": "21: Eine Instanz von einem SSD-Volume starten",
    "title": "Der Weg über das Horizon(Dashboard)",
    "content": "Um zu starten, loggen wir uns zunächst wie in “Schritt 1: Das Dashboard (Horizon)” erklärt ins Horizon(Dashboard) ein. Hier wechseln wir links in der Seitenleiste auf Project → Volumes → Volumes und klicken dann rechts auf “+ Create Volume” . In dem sich öffnenden Fenster geben wir folgende Optionen an und klicken dann auf “Create Volume”: . | Volume Name: Hier wird der Name des Volumes vergeben, dieser kann frei gewählt werden. Im Beispiel wird nach der Auswahl des Images automatisch “Ubuntu 16.04 Xenial Xerus - Latest” eingetragen. | Description: In diesem Feld kann eine Beschreibung hinzugefügt werden, je nach Bedarf. Im Beispiel wird keine Beschreibung verwendet. | Volume Source: Hier kann zwischen “Image” und “No source, empty image” gewählt werden. Für unser Beispiel nutzen wir “Image”. | Use image as a source: Es kann ein beliebiges Image genutzt werden. Im Beispiel wird “Ubuntu 16.04 Xenial Xerus - Latest (276.2 MB)” verwendet. | Type: Hier besteht die Wahl zwischen “high-iops”, “low-iops” und “default”. Da wir SSD-Speicher nutzen wollen, wählen wir “high-iops” aus. | Size: In diesem Feld bestimmen wir die Größe des Volumes, bei unseren Flavors sind es 20 GiB, daher nutzen wir dies auch für unser Beispiel | Availability Zone: Hier kann man zwischen 3 Optionen “Any Availability Zone”, “es1” oder “ix1” wählen und die entsprechende Zone festlegen. Im Beispiel nutzen wir ix1. | . Nachdem das Horizon das Volume korrekt erstellt hat, sollte es in etwa so aussehen: . Um eine neue Instanz von diesem Volume zu starten, können wir entweder rechts auf den Pfeil nach unten, neben “Edit Volume”, klicken und dann auf “Launch as Instance” oder alternativ dazu kann man auch links in der Seitenleiste auf Compute → Instances wechseln und dort auf “Launch Instane” klicken. Im sich öffnenden Fenster geben wir der Instanz einen Namen (Instance Name), wählen dieselbe Availability Zone wie weiter oben, also ix1 und wechseln dann links auf Source. Unter Source wählen wir Volume als Select Boot Source aus und klicken dann neben unserem erstellten Volume auf den Pfeil nach oben. Nun klicken wir links auf Flavor und wählen einen der möglichen Flavors aus, indem wir auf den Pfeil nach oben neben dem gewünschten Flavor klicken. Im nächsten Schritt wählen wir links, über den Reiter Networks das Netzwerk für die VM aus. Auch hier klicken wir neben dem gewünschten Netzwerk auf den Pfeil nach oben. Damit sind alle wichtigen Einstellungen getroffen und die Instanz kann mit “Launch Instance” gestartet werden. Falls benötigt, können noch eigene Security Groups und/oder Key Pairs der Instanz hinzugefügt werden. ",
    "url": "/optimist/guided_tour/step21/#der-weg-%C3%BCber-das-horizondashboard",
    
    "relUrl": "/optimist/guided_tour/step21/#der-weg-über-das-horizondashboard"
  },"409": {
    "doc": "21: Eine Instanz von einem SSD-Volume starten",
    "title": "Der Weg über HEAT",
    "content": "Wie bereits im Vorwort erwähnt, nutzen wir unser HEAT Template aus Schritt 18. Dieses Template startet bereits eine Instanz. Damit diese nun aber ein SSD-Volume nutzt, bedarf es einiger Änderungen. Zunächst fügen wir unseren Parametern noch die “availability_zone” hinzu: . heat_template_version: 2014-10-16 parameters: key_name: type: string public_network_id: type: string default: provider availability_zone: type: string default: ix1 . Der nächste Schritt ist am Ende des Templates einen eigenen Punkt “boot_ssd” für das Volume hinzuzufügen: . boot_ssd: type: OS::Cinder::Volume properties: name: boot_ssd size: 20 availability_zone: { get_param: availability_zone } volume_type: high-iops image: \"Ubuntu 16.04 Xenial Xerus - Latest\" . Nun haben wir bereits einen Parameter hinzugefügt und nutzten diesen auch direkt in unserem neu erstellten Boot-Volume. Damit die Instanz auch vom Volume startet, überarbeiten wir den Punkt “Instanz” in unserem HEAT-Template . Dort können wir den Punkt “image” entfernen (im Beispiel ist er per # auskommentiert), da dieser ja über das Volume bereitgestellt wird. Wir fügen nun noch die “availability_zone”, einen Namen “name”, das Netzwerk “networks” und das Volume “block_device_mapping” hinzu: . Instanz: type: OS::Nova::Server properties: name: SSD-Test availability_zone: { get_param: availability_zone } key_name: { get_param: key_name } #image: Ubuntu 16.04 Xenial Xerus - Latest flavor: m1.small networks: - port: { get_resource: Port } block_device_mapping: [ { device_name: \"vda\", volume_id: { get_resource: boot_ssd }, delete_on_termination: \"true\" } ] . Damit ist unser HEAT-Template für diesen Schritt fertig und sollte so aussehen: . heat_template_version: 2014-10-16 parameters: key_name: type: string public_network_id: type: string default: provider availability_zone: type: string default: ix1 resources: Instanz: type: OS::Nova::Server properties: name: SSD-Test availability_zone: { get_param: availability_zone } key_name: { get_param: key_name } #image: Ubuntu 16.04 Xenial Xerus - Latest flavor: m1.small networks: - port: { get_resource: Port } block_device_mapping: [ { device_name: \"vda\", volume_id: { get_resource: boot_ssd }, delete_on_termination: \"true\" } ] Netzwerk: type: OS::Neutron::Net properties: name: BeispielNetzwerk Port: type: OS::Neutron::Port properties: network: { get_resource: Netzwerk } Router: type: OS::Neutron::Router properties: external_gateway_info: { \"network\": { get_param: public_network_id } name: BeispielRouter Subnet: type: OS::Neutron::Subnet properties: name: BeispielSubnet dns_nameservers: - 8.8.8.8 - 8.8.4.4 network: { get_resource: Netzwerk } ip_version: 4 cidr: 10.0.0.0/24 allocation_pools: - { start: 10.0.0.10, end: 10.0.0.250 } Router_Subnet_Bridge: type: OS::Neutron::RouterInterface depends_on: Subnet properties: router: { get_resource: Router } subnet: { get_resource: Subnet } Floating_IP: type: OS::Neutron::FloatingIP properties: floating_network: { get_param: public_network_id } port_id: { get_resource: Port } Sec_SSH: type: OS::Neutron:SecurityGroup properties: description: Diese Security Group erlaubt den eingehenden SSH-Traffic über Port22 und ICMP name: Ermöglicht SSH (Port22) und ICMP rules: - { direction: ingress, remote_ip_prefix: 0.0.0.0/0, port_range_min: 22, port_range_max: 22, protocol: tcp } - { direction: ingress, remote_ip_prefix: 0.0.0.0/0, protocol: icmp } boot_ssd: type: OS::Cinder::Volume properties: name: boot_ssd size: 20 availability_zone: { get_param: availability_zone } volume_type: high-iops image: \"Ubuntu 16.04 Xenial Xerus - Latest\" . ",
    "url": "/optimist/guided_tour/step21/#der-weg-%C3%BCber-heat",
    
    "relUrl": "/optimist/guided_tour/step21/#der-weg-über-heat"
  },"410": {
    "doc": "21: Eine Instanz von einem SSD-Volume starten",
    "title": "Abschluss",
    "content": "In diesem Schritt haben wir gelernt, dass es ohne Weiteres möglich ist, eine Instanz auch von einem Volume zu starten und auch gleichzeitig schnellen SSD Speicher zu nutzen. Außerdem haben wir unsere HEAT-Kenntnisse aufgefrischt und ein Volume mit eingebunden. ",
    "url": "/optimist/guided_tour/step21/#abschluss",
    
    "relUrl": "/optimist/guided_tour/step21/#abschluss"
  },"411": {
    "doc": "21: Eine Instanz von einem SSD-Volume starten",
    "title": "21: Eine Instanz von einem SSD-Volume starten",
    "content": " ",
    "url": "/optimist/guided_tour/step21/",
    
    "relUrl": "/optimist/guided_tour/step21/"
  },"412": {
    "doc": "22: Anlegen eines DNS-Record in Designate",
    "title": "Schritt 22: Anlegen eines DNS-Record in Designate",
    "content": " ",
    "url": "/optimist/guided_tour/step22/#schritt-22-anlegen-eines-dns-record-in-designate",
    
    "relUrl": "/optimist/guided_tour/step22/#schritt-22-anlegen-eines-dns-record-in-designate"
  },"413": {
    "doc": "22: Anlegen eines DNS-Record in Designate",
    "title": "Vorwort",
    "content": "Die Openstackplattform Optimist enthält eine Technologie namens DNS-as-a-Service (DNSaaS), auch bekannt als Designate. DNSaaS enthält eine REST-API für die Domänen- und Datensatzverwaltung, ist multi-tenant und integriert den OpenStack Identity Service (Keystone) für die Authentifizierung. Wir werden in diesem Schritt eine fiktive Zone (Domain) mit MX und A-Records erstellen und die entsprechende IP/CNAME hinterlegen. Um zu starten, lesen wir uns zunächst wie in “Schritt 4: Der Weg vom Horizon auf die Kommandozeile” erklärt die Zugangsdaten ein und sorgen dafür das der python-designateclient installiert ist (pip install python-openstackclient python-designateclient) Anschliessend bedienen wir den Openstack-Client und erstellen zuerst eine Zone für unser Projekt. $ openstack zone create --email webmaster@foobar.cloud foobar.cloud. +----------------+--------------------------------------+ | Field | Value | +----------------+--------------------------------------+ | action | CREATE | attributes | | created_at | 2018-08-15T06:45:24.000000 | description | None | email | webmaster@foobar.cloud | id | 036ae6e6-6318-47e1-920f-be518d845fb5 | masters | | name | foobar.cloud. | pool_id | bb031d0d-b8ca-455a-8963-50ec70fe57cf | project_id | 2b62bc8ff48445f394d0318dbd058967 | serial | 1534315524 | status | PENDING | transferred_at | None | ttl | 3600 | type | PRIMARY | updated_at | None | version | 1 | +----------------+--------------------------------------+ . Man beachte den abschliessenden “.” an der zu erstellenden Zone/Domain. Das Resultat bisher: . $ openstack zone list +--------------------------------------+-----------------------+---------+------------+--------+--------+ | id | name | type | serial | status | action | +--------------------------------------+-----------------------+---------+------------+--------+--------+ | 036ae6e6-6318-47e1-920f-be518d845fb5 | foobar.cloud. | PRIMARY | 1534315524 | ACTIVE | NONE | +--------------------------------------+-----------------------+---------+------------+--------+--------+ $ openstack zone show foobar.cloud. +----------------+--------------------------------------+ | Field | Value | +----------------+--------------------------------------+ | action | NONE | attributes | | created_at | 2018-08-15T06:45:24.000000 | description | None | email | webmaster@foobar.cloud | id | 036ae6e6-6318-47e1-920f-be518d845fb5 | masters | | name | foobar.cloud. | pool_id | bb031d0d-b8ca-455a-8963-50ec70fe57cf | project_id | 2b62bc8ff48445f394d0318dbd058967 | serial | 1534315524 | status | ACTIVE | transferred_at | None | ttl | 3600 | type | PRIMARY | updated_at | 2018-08-15T06:45:30.000000 | version | 2 | +----------------+--------------------------------------+ . Nun ist die Domain “foobar.cloud” für unser Projekt registriert und einsatzbereit (status: ACTIVE). Wir wollen im nächsten Schritt MX-Records (Datensätze für Mailserver in dieser Zone) für diese Domain erstellen. Doch zuerst schauen wir, welche Inhalte (Recordsets) unsere neue Zone bereits jetzt besitzt. $ openstack recordset list foobar.cloud. +--------------------------------------+---------------+------+--------------------------------------------------------------------------------+--------+--------+ | id | name | type | records | status | action | +--------------------------------------+---------------+------+--------------------------------------------------------------------------------+--------+--------+ | 4b666317-6170-4d71-8962-94f1cbe94dda | foobar.cloud. | NS | dns1.ddns.innovo.cloud. | ACTIVE | NONE | | | dns2.ddns.innovo.cloud. | | 7915c9c1-de30-4144-b82b-36295687b0fe | foobar.cloud. | SOA | dns2.ddns.innovo.cloud. webmaster.foobar.cloud. 1534315524 3507 600 86400 3600 | ACTIVE | NONE | +--------------------------------------+---------------+------+--------------------------------------------------------------------------------+--------+--------+ . Hier sehen wir eine “leere Hülle” einer Domain mit automatisch erstellen NS und SOA-Einträgen, die sofort zur Abfrage bereit stehen. $ dig +short @dns1.ddns.innovo.cloud foobar.cloud NS dns1.ddns.innovo.cloud. dns2.ddns.innovo.cloud. $ dig +short @dns1.ddns.innovo.cloud foobar.cloud SOA dns2.ddns.innovo.cloud. webmaster.foobar.cloud. 1534315524 3507 600 86400 3600 . Anlegen eines MX-Records: Nun können wir Datensätze innerhalb dieser Zone hinzufügen, verändern oder löschen (openstack recordset –help). Als nächstes fügen wir einen MX und einen A-Record hinzu. Bei den MX-Records richten wir auch gleich die typischen Mailserver-Prioritäten (10,20) mit ein. Wobei immer der niedrigere Wert als erstes angesteuert wird und der zweite Eintrag als “Backup” dient. $ openstack recordset create --record '10 mx1.foobar.cloud.' --record '20 mx2.foobar.cloud.' --type MX foobar.cloud. foobar.cloud. +-------------+--------------------------------------+ | Field | Value | +-------------+--------------------------------------+ | action | CREATE | created_at | 2018-08-15T07:15:32.000000 | description | None | id | 4197e380-dc61-4e1d-bf92-0dcf5344eafa | name | foobar.cloud. | project_id | 2b62bc8ff48445f394d0318dbd058967 | records | 10 mx1.foobar.cloud. | | 20 mx2.foobar.cloud. | status | PENDING | ttl | None | type | MX | updated_at | None | version | 1 | zone_id | 036ae6e6-6318-47e1-920f-be518d845fb5 | zone_name | foobar.cloud. | +-------------+--------------------------------------+ $ openstack recordset list foobar.cloud. +--------------------------------------+---------------+------+--------------------------------------------------------------------------------+---------+--------+ | id | name | type | records | status | action | +--------------------------------------+---------------+------+--------------------------------------------------------------------------------+---------+--------+ | 4b666317-6170-4d71-8962-94f1cbe94dda | foobar.cloud. | NS | dns1.ddns.innovo.cloud. | ACTIVE | NONE | | | dns2.ddns.innovo.cloud. | | 7915c9c1-de30-4144-b82b-36295687b0fe | foobar.cloud. | SOA | dns2.ddns.innovo.cloud. webmaster.foobar.cloud. 1534317332 3507 600 86400 3600 | PENDING | UPDATE | 4197e380-dc61-4e1d-bf92-0dcf5344eafa | foobar.cloud. | MX | 20 mx2.foobar.cloud. | PENDING | CREATE | | | 10 mx1.foobar.cloud. | | +--------------------------------------+---------------+------+--------------------------------------------------------------------------------+---------+--------+ . $ openstack recordset create --type A --record 1.2.3.4 foobar.cloud. www +-------------+--------------------------------------+ | Field | Value | +-------------+--------------------------------------+ | action | CREATE | created_at | 2018-08-15T07:28:15.000000 | description | None | id | d932688f-21d5-44b1-aa27-030c342788e7 | name | www.foobar.cloud. | project_id | 2b62bc8ff48445f394d0318dbd058967 | records | 1.2.3.4 | status | PENDING | ttl | None | type | A | updated_at | None | version | 1 | zone_id | 036ae6e6-6318-47e1-920f-be518d845fb5 | zone_name | foobar.cloud. | +-------------+--------------------------------------+ . Resultat: . $ openstack recordset list foobar.cloud. +--------------------------------------+-------------------+------+--------------------------------------------------------------------------------+--------+--------+ | id | name | type | records | status | action | +--------------------------------------+-------------------+------+--------------------------------------------------------------------------------+--------+--------+ | 4b666317-6170-4d71-8962-94f1cbe94dda | foobar.cloud. | NS | dns1.ddns.innovo.cloud. | ACTIVE | NONE | | | dns2.ddns.innovo.cloud. | | 7915c9c1-de30-4144-b82b-36295687b0fe | foobar.cloud. | SOA | dns2.ddns.innovo.cloud. webmaster.foobar.cloud. 1534318095 3507 600 86400 3600 | ACTIVE | NONE | 4197e380-dc61-4e1d-bf92-0dcf5344eafa | foobar.cloud. | MX | 20 mx2.foobar.cloud. | ACTIVE | NONE | | | 10 mx1.foobar.cloud. | | d932688f-21d5-44b1-aa27-030c342788e7 | www.foobar.cloud. | A | 1.2.3.4 | ACTIVE | NONE | +--------------------------------------+-------------------+------+--------------------------------------------------------------------------------+--------+--------+ . Wenn die Recordsets aktiv sind können wir die dafür vorgesehenen DNS-Server . | dns1.ddns.innovo.cloud | dns2.ddns.innovo.cloud nach diesen Records abfragen. | . $ dig +short @dns1.ddns.innovo.cloud foobar.cloud NS dns1.ddns.innovo.cloud. dns2.ddns.innovo.cloud. $ dig +short @dns1.ddns.innovo.cloud foobar.cloud MX 10 mx1.foobar.cloud. 20 mx2.foobar.cloud. $ dig +short @dns1.ddns.innovo.cloud foobar.cloud www.foobar.cloud 1.2.3.4 . ACHTUNG! Zu diesem Zeitpunkt ist diese Domaine (foobar.cloud) noch nicht weltweit auflösbar. Damit dieses Konstrukt weltweit benutzt werden kann, muss jede im Designate verwaltete Domain bei dem jeweiligen Registrar die Delegation zu den Nameservern dns1.ddns.innovo.cloud und dns2.ddns.innovo.cloud eingerichtet haben. Details zu unseren authoritativen DNS-Server: . | dns1.ddns.innovo.cloud: ‘185.116.244.45’ / ‘2a00:c320:0:1::d’ | dns2.ddns.innovo.cloud: ‘185.116.244.46’ / ‘2a00:c320:0:1::e’ | . Um die die Mail-Records abzuschliessen, bietet sich noch an für die Mailserver entsprechende A-Records zu hinterlegen . $ openstack recordset create --type A --record 2.3.4.5 foobar.cloud. mx1 +-------------+--------------------------------------+ | Field | Value | +-------------+--------------------------------------+ | action | CREATE | created_at | 2018-08-15T07:42:31.000000 | description | None | id | 630d5103-7c02-4a58-83a5-97f802cf141c | name | mx1.foobar.cloud. | project_id | 2b62bc8ff48445f394d0318dbd058967 | records | 2.3.4.5 | status | PENDING | ttl | None | type | A | updated_at | None | version | 1 | zone_id | 036ae6e6-6318-47e1-920f-be518d845fb5 | zone_name | foobar.cloud. | +-------------+--------------------------------------+ und $ openstack recordset create --type A --record 3.4.5.6 foobar.cloud. mx2 +-------------+--------------------------------------+ | Field | Value | +-------------+--------------------------------------+ | action | CREATE | created_at | 2018-08-15T07:42:56.000000 | description | None | id | 2b47dbe4-70b9-4edb-ac2f-25cb8398bacb | name | mx2.foobar.cloud. | project_id | 2b62bc8ff48445f394d0318dbd058967 | records | 3.4.5.6 | status | PENDING | ttl | None | type | A | updated_at | None | version | 1 | zone_id | 036ae6e6-6318-47e1-920f-be518d845fb5 | zone_name | foobar.cloud. | +-------------+--------------------------------------+ . Das Resultat nach einigen Sekunden: . $ openstack recordset list foobar.cloud. +--------------------------------------+-------------------+------+--------------------------------------------------------------------------------+--------+--------+ | id | name | type | records | status | action | +--------------------------------------+-------------------+------+--------------------------------------------------------------------------------+--------+--------+ | 4b666317-6170-4d71-8962-94f1cbe94dda | foobar.cloud. | NS | dns1.ddns.innovo.cloud. | ACTIVE | NONE | | | dns2.ddns.innovo.cloud. | | 7915c9c1-de30-4144-b82b-36295687b0fe | foobar.cloud. | SOA | dns2.ddns.innovo.cloud. webmaster.foobar.cloud. 1534318976 3507 600 86400 3600 | ACTIVE | NONE | 4197e380-dc61-4e1d-bf92-0dcf5344eafa | foobar.cloud. | MX | 20 mx2.foobar.cloud. | ACTIVE | NONE | | | 10 mx1.foobar.cloud. | | d932688f-21d5-44b1-aa27-030c342788e7 | www.foobar.cloud. | A | 1.2.3.4 | ACTIVE | NONE | 630d5103-7c02-4a58-83a5-97f802cf141c | mx1.foobar.cloud. | A | 2.3.4.5 | ACTIVE | NONE | 2b47dbe4-70b9-4edb-ac2f-25cb8398bacb | mx2.foobar.cloud. | A | 3.4.5.6 | ACTIVE | NONE | +--------------------------------------+-------------------+------+--------------------------------------------------------------------------------+--------+--------+ . ",
    "url": "/optimist/guided_tour/step22/#vorwort",
    
    "relUrl": "/optimist/guided_tour/step22/#vorwort"
  },"414": {
    "doc": "22: Anlegen eines DNS-Record in Designate",
    "title": "Abschluss",
    "content": "In diesem Schritt haben wir gelernt, wie man eine Zone anlegt, einen Recordset konfiguriert und diesen abfragen kann. ",
    "url": "/optimist/guided_tour/step22/#abschluss",
    
    "relUrl": "/optimist/guided_tour/step22/#abschluss"
  },"415": {
    "doc": "22: Anlegen eines DNS-Record in Designate",
    "title": "22: Anlegen eines DNS-Record in Designate",
    "content": " ",
    "url": "/optimist/guided_tour/step22/",
    
    "relUrl": "/optimist/guided_tour/step22/"
  },"416": {
    "doc": "23: Der Object Storage (S3 kompatibel)",
    "title": "Schritt 23: Der Object Storage (S3 kompatibel)",
    "content": " ",
    "url": "/optimist/guided_tour/step23/#schritt-23-der-object-storage-s3-kompatibel",
    
    "relUrl": "/optimist/guided_tour/step23/#schritt-23-der-object-storage-s3-kompatibel"
  },"417": {
    "doc": "23: Der Object Storage (S3 kompatibel)",
    "title": "Vorwort",
    "content": "In den vorigen Schritten haben wir bereits einige interessante Bausteine kennengelernt. Als nächstes widmen wir uns dem Object Storage, der uns interessante Möglichkeiten bietet, Dateien zu speichern. ",
    "url": "/optimist/guided_tour/step23/#vorwort",
    
    "relUrl": "/optimist/guided_tour/step23/#vorwort"
  },"418": {
    "doc": "23: Der Object Storage (S3 kompatibel)",
    "title": "Der Start (Benutzerdaten)",
    "content": "Damit wir auf den Object Storage zugreifen können, benötigen wir zunächst Login Daten(Credentials), um auf diesen zugreifen zu können. Dafür benötigen wir den OpenStackClienten(siehe Schritt 4), damit wir per OpenstackAPI die entsprechenden Daten erstellen. Der Befehl in der Kommandozeile dafür lautet: . openstack ec2 credentials create . Wenn die Daten korrekt erstellt worden sind, sieht die Ausgabe in etwa so aus: . $ openstack ec2 credentials create +------------+-----------------------------------------------------------------+ | Field | Value | +------------+-----------------------------------------------------------------+ | access | aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa | links | {u'self': u'https://identity.optimist.gec.io/v3/users/bbb | | bbbbbbbbbbbbbbbbbbbbbbbbbbbbb/credentials/OS- | | EC2/aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa'} | project_id | cccccccccccccccccccccccccccccccc | secret | dddddddddddddddddddddddddddddddd | trust_id | None | user_id | bbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbb | +------------+-----------------------------------------------------------------+ . Nachdem die Zugangsdaten (Credentials) vorliegen, brauchen wir eine Möglichkeit auf den S3 kompatiblen ObjectStorage zuzugreifen. ",
    "url": "/optimist/guided_tour/step23/#der-start-benutzerdaten",
    
    "relUrl": "/optimist/guided_tour/step23/#der-start-benutzerdaten"
  },"419": {
    "doc": "23: Der Object Storage (S3 kompatibel)",
    "title": "Zugriff auf den S3 kompatiblen ObjectStorage",
    "content": "Es gibt natürlich verschiedene Optionen auf den ObjectStorage zuzugreifen. Wir empfehlen dafür die Nutzung von s3cmd Dieses kleine Tool ist einfach zu bedienen und zu nutzen. Da wir bereits in Schritt 4 “pip” als Paketmanager installiert haben und nutzen, können wir S3cmd auch über “pip” installieren: . pip install s3cmd . Da jetzt S3cmd installiert ist, müssen die vorher erstellten Zugangsdaten (Credentials) eingetragen werden, nur so lässt sich S3cmd auch korrekt nutzen. Alle wichtigen Informationen finden wir in der .s3cfg , sollte diese noch nicht existieren, erstellen wir diese vorher. Folgende Daten tragen wir dann in der .s3cfg ein: . access_key = aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa check_ssl_certificate = True check_ssl_hostname = True host_base = s3.es1.fra.optimist.gec.io host_bucket = s3.es1.fra.optimist.gec.io secret_key = dddddddddddddddddddddddddddddddd use_https = True . ",
    "url": "/optimist/guided_tour/step23/#zugriff-auf-den-s3-kompatiblen-objectstorage",
    
    "relUrl": "/optimist/guided_tour/step23/#zugriff-auf-den-s3-kompatiblen-objectstorage"
  },"420": {
    "doc": "23: Der Object Storage (S3 kompatibel)",
    "title": "Der Bucket",
    "content": "Da wir Zugriff auf den S3 kompatiblen Object Storage haben, ist es an der Zeit damit auch zu arbeiten. Alle verfügbaren Befehle von s3cmd können mit folgendem Befehl angezeigt werden: . s3cmd --help . Als Nächstes erstellen wir einen Bucket. Buckets entsprechen dabei im weitesten Sinne Ordnern, die wir für eine Struktur benötigen. Eine Datei kann also nur in einem existierenden Bucket gespeichert werden und der Name vom Bucket selber ist einzigartig (über den gesamten Optimist). Wenn also bereits ein Bucket mit dem Namen “Test” besteht, kann dieser nicht erneut angelegt werden. Daher ist es aus unserer Sicht eine gute Option, eine UUID zu nutzen und diese dann in der entsprechenden Applikation aufzulösen. Auch gibt es die Möglichkeit, bei Buckets und auch bei Dateien, zwischen public und private zu unterscheiden. Alle Buckets die erstellt und Dateien die hochgeladen werden, sind per default private, d.h. wenn keine weiteren Einstellungen vorgenommen werden, kann nur der Ersteller auf den Bucket und den Inhalt zugreifen. Dies lässt sich zum Beispiel per Access Control List (ACL) ändern. WICHTIG: Sollte man einen kompletten Bucket auf public stellen, können auch Informationen über Dateien, in diesem Bucket, die auf private gesetzt sind, abgerufen werden. Da wir die wichtigsten Details kennen, ist es Zeit, einen Bucket mit einer UUID zu erstellen: . $ s3cmd mb s3://e4d05df3-aa8e-4a37-b1b5-2745d189b189 Bucket 's3://e4d05df3-aa8e-4a37-b1b5-2745d189b189/' created . ",
    "url": "/optimist/guided_tour/step23/#der-bucket",
    
    "relUrl": "/optimist/guided_tour/step23/#der-bucket"
  },"421": {
    "doc": "23: Der Object Storage (S3 kompatibel)",
    "title": "Eine Datei hochladen",
    "content": "Da wir jetzt einen Bucket erstellt haben, ist der nächste Schritt, eine oder auch mehrere Dateien hochzuladen. Dafür nehmen wir den Befehl s3cmd put Dateiname s3://Name_des_Buckets und eine Ausgabe kann dann so aussehen: . $ s3cmd put test.yaml s3://e4d05df3-aa8e-4a37-b1b5-2745d189b189 upload: 'test.yaml' -&gt; 's3://e4d05df3-aa8e-4a37-b1b5-2745d189b189/test.yaml' [1 of 1] 4218 of 4218 100% in 0s 4.61 kB/s done . ",
    "url": "/optimist/guided_tour/step23/#eine-datei-hochladen",
    
    "relUrl": "/optimist/guided_tour/step23/#eine-datei-hochladen"
  },"422": {
    "doc": "23: Der Object Storage (S3 kompatibel)",
    "title": "Zugriff auf die Datei erhalten",
    "content": "Die generelle URL für den Zugriff auf Dateien lautet im Optimisten https://s3.es1.fra.optimist.gec.io/Name_des_Buckets/Dateiname. Damit auf die Datei aus unserem Beispiel zugegriffen werden kann, ist es notwendig, die Einstellung von private auf public zu ändern. Dafür können wir, wie bereits unter dem Punkt “Der Bucket” erwähnt, die Access Control List (ACL) nutzen: . $ s3cmd setacl s3://e4d05df3-aa8e-4a37-b1b5-2745d189b189/test.yaml --acl-public s3://e4d05df3-aa8e-4a37-b1b5-2745d189b189/test.yaml: ACL set to Public [1 of 1] . Die Datei kann jetzt über folgenden Link aufgerufen werden: https://s3.es1.fra.optimist.gec.io/e4d05df3-aa8e-4a37-b1b5-2745d189b189/test.yaml . Um sie wieder auf private zu stellen, nutzen wir folgenden Befehl: . $ s3cmd setacl s3://e4d05df3-aa8e-4a37-b1b5-2745d189b189/test.yaml --acl-private s3://e4d05df3-aa8e-4a37-b1b5-2745d189b189/test.yaml: ACL set to Private [1 of 1] . ",
    "url": "/optimist/guided_tour/step23/#zugriff-auf-die-datei-erhalten",
    
    "relUrl": "/optimist/guided_tour/step23/#zugriff-auf-die-datei-erhalten"
  },"423": {
    "doc": "23: Der Object Storage (S3 kompatibel)",
    "title": "Abschluss",
    "content": "In diesem Schritt haben wir den S3 kompatiblen Storage kennengelernt und die ersten Schritte im Umgang damit geübt. ",
    "url": "/optimist/guided_tour/step23/#abschluss",
    
    "relUrl": "/optimist/guided_tour/step23/#abschluss"
  },"424": {
    "doc": "23: Der Object Storage (S3 kompatibel)",
    "title": "23: Der Object Storage (S3 kompatibel)",
    "content": " ",
    "url": "/optimist/guided_tour/step23/",
    
    "relUrl": "/optimist/guided_tour/step23/"
  },"425": {
    "doc": "Netzwerke",
    "title": "Netzwerke",
    "content": " ",
    "url": "/optimist/networking/",
    
    "relUrl": "/optimist/networking/"
  },"426": {
    "doc": "Port Forwarding auf Floating IPs",
    "title": "Port Forwarding auf Floating IPs",
    "content": "Floating IP Port Forwarding erlaubt die Weiterleitung eines beliebigen TCP/UDP/anderen Protokoll-Ports einer Floating IP-Adresse an einen TCP/UDP/anderen Protokoll-Port, der mit einer festen IP-Adresse eines Neutron-Ports verbunden ist. ",
    "url": "/optimist/networking/port_forwarding/",
    
    "relUrl": "/optimist/networking/port_forwarding/"
  },"427": {
    "doc": "Port Forwarding auf Floating IPs",
    "title": "Port Forwarding auf einer Floating IP erstellen",
    "content": "Um ein Port forwarding auf eine Floating IP anzuwenden, sind die folgenden Informationen erforderlich: . | Die zu verwendende interne IP-Adresse | Die UUID des Ports, der mit der Floating IP assoziiert werden soll | Die Portnummer des Netzwerkports der festen IPv4-Adresse | Die externe Portnummer der Floating IP-Adresse | Das spezifische Protokoll, das bei der Port-Weiterleitung zu verwenden ist (in diesem Beispiel TCP) | Die Floating IP, auf der dieser Port freigeschalten werden soll. (in diesem Beispiel 185.116.244.141) | . Das folgende Beispiel zeigt die Erstellung eines Port Forwarding auf einer Floating IP unter Verwendung der erforderlichen Optionen: . $ openstack floating ip port forwarding create \\ --internal-ip-address 10.0.0.14 \\ --port 12c29300-0f8a-4c54-a9dc-bee4c12c6ad2 \\ --internal-protocol-port 80 \\ --external-protocol-port 8080 \\ --protocol tcp 185.116.244.141 . ",
    "url": "/optimist/networking/port_forwarding/#port-forwarding-auf-einer-floating-ip-erstellen",
    
    "relUrl": "/optimist/networking/port_forwarding/#port-forwarding-auf-einer-floating-ip-erstellen"
  },"428": {
    "doc": "Port Forwarding auf Floating IPs",
    "title": "Anzeigen der Port Forwarding Einstellungen bestimmter Floating IPs",
    "content": "Innerhalb eines Projekts kann eine Liste der Port Forwarding-Regeln, die für eine bestimmte Floating IP gelten, mit dem folgenden Befehl abgerufen werden: . $ openstack floating ip port forwarding list 185.116.244.141 . Der obige Befehl kann weiter gefiltert werden, indem vor der Floating IP die Flags --sort-column, --port, --external-protcol-port und/oder --protocol verwendet werden. ",
    "url": "/optimist/networking/port_forwarding/#anzeigen-der-port-forwarding-einstellungen-bestimmter-floating-ips",
    
    "relUrl": "/optimist/networking/port_forwarding/#anzeigen-der-port-forwarding-einstellungen-bestimmter-floating-ips"
  },"429": {
    "doc": "Port Forwarding auf Floating IPs",
    "title": "Anzeigen der Details einer port forwarding-Regel",
    "content": "Um Details zu einer bestimmten Port Forwarding-Regel anzuzeigen, kann der folgende Befehl verwendet werden: . $ openstack floating ip port forwarding show &lt;floating-ip&gt; &lt;port-forwarding-id&gt; . ",
    "url": "/optimist/networking/port_forwarding/#anzeigen-der-details-einer-port-forwarding-regel",
    
    "relUrl": "/optimist/networking/port_forwarding/#anzeigen-der-details-einer-port-forwarding-regel"
  },"430": {
    "doc": "Port Forwarding auf Floating IPs",
    "title": "Ändern von Floating IP Port Forwarding-Eigenschaften",
    "content": "Wenn eine Port Forwarding-Konfiguration auf einer Floating IP bereits mit $ openstack floating ip port forwarding create erstellt wurde, können Änderungen an der bestehenden Konfiguration mit $ openstack floating ip port forwarding set ... vorgenommen werden. Die folgenden Parameter eines Port Forwardings können geändert werden: . | --port: Die UUID des Ports | --internal-ip-address: Die zum Zielport der Forwarding-Regel gehoerende feste interne IPv4-Adresse | --internal-protocol-port: Die interne TCP/UDP/etc. Portnummer auf die die Floating IPs Port Forwarding-Regel weiterleitet | --external-protocol-port: Die TCP/UDP/etc. Portnummer der Floating-IP-Adresse des Port Forwardings | --protocol: Das IP-Protokoll, das in der Floating IP Port Forwarding-Regel verwendet wird (TCP/UDP/andere) | --description: Text zur Beschreibung der Verwendung der Port Forwarding-Konfiguration | . Die Konfiguration jeder der oben genannten Parameter kann mit einer Variation des folgenden Befehls geändert werden: . $ openstack floating ip port forwarding set \\ --port &lt;port&gt; \\ --internal-ip-address &lt;internal-ip-address&gt; \\ --internal-protocol-port &lt;port-number&gt; \\ --extern-protocol-port &lt;port-number&gt; \\ --protocol &lt;protocol&gt; \\ --description &lt;description&gt; \\ &lt;Floating-ip&gt; &lt;port-forwarding-id&gt; . ",
    "url": "/optimist/networking/port_forwarding/#%C3%A4ndern-von-floating-ip-port-forwarding-eigenschaften",
    
    "relUrl": "/optimist/networking/port_forwarding/#ändern-von-floating-ip-port-forwarding-eigenschaften"
  },"431": {
    "doc": "Port Forwarding auf Floating IPs",
    "title": "Löschen der Port Forwarding-Konfiguration zu einer Floating IP",
    "content": "Um eine Port Forwarding-Regel von einer Floating IP zu entfernen, benötigen wir die folgenden Informationen: . | Die Floating IP dessen Port Forwarding-Regel entfernt werden soll | Die Port Forwarding ID (Diese ID wird bei der Erstellung erzeugt und kann mit dem Befehl $ openstack Floating ip port forwarding list ... angezeigt werden) | . Mit dem folgenden Befehl lässt sich die Konfiguration für ein Floating IP Port Forwarding löschen: . $ openstack floating ip port forwarding delete &lt;Floating-ip&gt; &lt;port-forwarding-id&gt; . ",
    "url": "/optimist/networking/port_forwarding/#l%C3%B6schen-der-port-forwarding-konfiguration-zu-einer-floating-ip",
    
    "relUrl": "/optimist/networking/port_forwarding/#löschen-der-port-forwarding-konfiguration-zu-einer-floating-ip"
  },"432": {
    "doc": "Geteilte Netzwerke",
    "title": "Geteilte Netzwerke",
    "content": " ",
    "url": "/optimist/networking/shared_networks/",
    
    "relUrl": "/optimist/networking/shared_networks/"
  },"433": {
    "doc": "Geteilte Netzwerke",
    "title": "Motivation",
    "content": "Es kommt oft die Frage auf, ob es möglich ist ein Netzwerk zwischen zwei Projekten im OpenStack zu teilen. ",
    "url": "/optimist/networking/shared_networks/#motivation",
    
    "relUrl": "/optimist/networking/shared_networks/#motivation"
  },"434": {
    "doc": "Geteilte Netzwerke",
    "title": "Netzwerk teilen",
    "content": "Zugriff auf beide Projekte ist vorhanden: . Damit das Netzwerk geteilt werden kann, brauchen wir zum einen den OpenStackClient, sowie die Projekt-ID in welches das Netzwerk geteilt werden soll und die Netzwerk-ID des zu teilenden Netzwerks. Die Projekt-ID findet sich in der Ausgabe unter “id” wenn wir folgenden Befehl benutzen: . openstack project show &lt;Name des Projekts&gt; -f value -c id . Als nächstes benötigen wir noch die Netzwerk-ID des zu teilenden Netzwerks, diese finden wir in der Ausgabe unter “id” wenn folgender Befehl genutzt wird: . openstack network show &lt;Name des Netzwerks&gt; -f value -c id . Mit den erhaltenen IDs kann nun das Netzwerk in das entsprechende Projekt geteilt werden, dafür benutzen wir die rollenbasierte Zugriffskontrolle (RBAC): . openstack network rbac create --type network --action access_as_shared --target-project &lt;ID des Projekts&gt; &lt;ID des zu teilenden Netzwerks&gt; . Zugriff auf beide Projekte ist nicht vorhanden: . In diesem Fall kann das Netzwerk nur vom Support, nach voriger Freigabe des anderen Projekt Inhabers, geteilt werden. Um ein Netzwerk mit einem Projekt zu teilen, schreiben Sie uns bitte eine E-Mail an support@gec.io mit folgenden Angaben: . | Name und ID des Netzwerks, welches geteilt werden soll | Name und ID des Projekts, in welchem das Netzwerk sichtbar sein soll | . ",
    "url": "/optimist/networking/shared_networks/#netzwerk-teilen",
    
    "relUrl": "/optimist/networking/shared_networks/#netzwerk-teilen"
  },"435": {
    "doc": "Geteilte Netzwerke",
    "title": "Wichtige Informationen zum geteilten Netzwerk",
    "content": "Wenn man auf ein geteiltes Netzwerk zugreift, gibt es Einschränkungen, die beachtet werden müssen. Eine Einschränkung ist, dass keine Remote Security-Groups benutzt werden können. Auch gibt es keinen Einblick in Ports und IP Adressen vom anderen Projekt gibt. Daher kann man auch keine konkrete IP Adressen für neue Ports in einem Subnetz (im geteilten Netzwerk) angeben, da es so möglich wäre, IPs zu finden die bereits genutzt werden. Damit man das geteilte Netzwerk sinnvoll nutzen kann, gibt es die Möglichkeit einen neuen Port zu erstellen. Dieser erhält dann eine beliebige IP-Adresse und kann weiter genutzt werden, um zum Beispiel einen Router über diesen Port hinzuzufügen. Im Dashboard (Horizon) ist dies nicht möglich und der OpenStackClient wird benötigt. Bitte achten Sie darauf bei den Bezeichnungen keine Leer- und/oder Sonderzeichen zu nutzen, da die Nutzung selbiger zu Problem führen kann. Zuerst erstellen wir den Port und geben dort das geteilte Netzwerk an: . openstack port create --network &lt;ID des geteilten Netzwerks&gt; &lt;Name des Ports&gt; . Jetzt kann zum Beispiel ein Router erstellt und dann dem neu erstellten Port zugeordnet werden: . ##Erstellung des Routers $ openstack router create &lt;Name des Routers&gt; ##Port dem Router zuordnen $ openstack router add port &lt;Name des Routers&gt; &lt;Name des Ports&gt; . ",
    "url": "/optimist/networking/shared_networks/#wichtige-informationen-zum-geteilten-netzwerk",
    
    "relUrl": "/optimist/networking/shared_networks/#wichtige-informationen-zum-geteilten-netzwerk"
  },"436": {
    "doc": "Geteilte Netzwerke",
    "title": "Netzwerk Topology Projekt 1",
    "content": ". Das Netzwerk “shared” aus dem Projekt 1 wird mit Projekt 2 geteilt. In diesem Netzwerk steht der Service “Example” zur Verfügung der dort auf einer Instanz läuft. ",
    "url": "/optimist/networking/shared_networks/#netzwerk-topology-projekt-1",
    
    "relUrl": "/optimist/networking/shared_networks/#netzwerk-topology-projekt-1"
  },"437": {
    "doc": "Geteilte Netzwerke",
    "title": "Netzwerk Topology Projekt 2",
    "content": ". Das Netzwerk “shared” ist auch in Projekt 2 sichtbar und wurde dort an den Router “router2” angehangen. Zusätzlich existiert dort das Netzwerk “network”, aus dem auf die Services in dem Netzwerk “shared” zugegriffen wird. Dabei muss berücksichtigt werden, das im Subnet des “shared” Networks in Projekt 1 die entsprechende Route unter dem Eintrag “Host Routes” gesetzt wird, um einen korrekten Rücktransport der Pakete zu ermöglichen. Im unserem Beispiel ist die folgende Route notwendig: 10.0.1.0/24,10.0.0.1 . ",
    "url": "/optimist/networking/shared_networks/#netzwerk-topology-projekt-2",
    
    "relUrl": "/optimist/networking/shared_networks/#netzwerk-topology-projekt-2"
  },"438": {
    "doc": "Octavia Loadbalancers",
    "title": "Der Octavia Loadbalancer",
    "content": " ",
    "url": "/optimist/networking/octavia_loadbalancer/#der-octavia-loadbalancer",
    
    "relUrl": "/optimist/networking/octavia_loadbalancer/#der-octavia-loadbalancer"
  },"439": {
    "doc": "Octavia Loadbalancers",
    "title": "Vorwort",
    "content": "Octavia ist eine hochverfügbare und skalierbare Open-Source Load-Balancing-Lösung, die für die Arbeit mit OpenStack entwickelt wurde. Octavia erledigt das Load-Balancing nach Bedarf, indem es virtuelle Maschinen – auch Amphoren genannt – in seinem Projekt verwaltet und konfiguriert. In diesen Amphoren wirkt schlussendlich ein HAproxy. ",
    "url": "/optimist/networking/octavia_loadbalancer/#vorwort",
    
    "relUrl": "/optimist/networking/octavia_loadbalancer/#vorwort"
  },"440": {
    "doc": "Octavia Loadbalancers",
    "title": "Der Start",
    "content": "Für die Nutzung von Octavia ist es notwendig, dass der Client auf dem eigenen System installiert ist. Eine Anleitung für sein System findet man unter Schritt 4) . ",
    "url": "/optimist/networking/octavia_loadbalancer/#der-start",
    
    "relUrl": "/optimist/networking/octavia_loadbalancer/#der-start"
  },"441": {
    "doc": "Octavia Loadbalancers",
    "title": "Erstellung eines Octavia-Ladbalancer",
    "content": "Für unser Beispiel nutzen wir das aus Schritt 10 schon bestehende BeispielSubnet. $ openstack loadbalancer create --name Beispiel-LB --vip-subnet-id 32259126-dd37-44d5-922c-99d68ee870cd +---------------------+--------------------------------------+ | Field | Value | +---------------------+--------------------------------------+ | admin_state_up | True | created_at | 2019-05-01T09:00:00 | description | | flavor_id | | id | e94827f0-f94d-40c7-a7fd-b91bf2676177 | listeners | | name | Beispiel-LB | operating_status | OFFLINE | pools | | project_id | b15cde70d85749689e08106f973bb002 | provider | amphora | provisioning_status | PENDING_CREATE | updated_at | None | vip_address | 10.0.0.10 | vip_network_id | f2a8f00e-204b-4c37-9d19-1d5c8e4efbf6 | vip_port_id | 37fc5b34-ee07-49c8-b054-a8d591a9679f | vip_qos_policy_id | None | vip_subnet_id | 32259126-dd37-44d5-922c-99d68ee870cd | +---------------------+--------------------------------------+ . Nun geht Octavia her und spawned seine Amphoreninstanzen im Hintergrund. $ openstack loadbalancer list +--------------------------------------+-------------+----------------------------------+--------------+---------------------+----------+ | id | name | project_id | vip_address | provisioning_status | provider | +--------------------------------------+-------------+----------------------------------+--------------+---------------------+----------+ | e94827f0-f94d-40c7-a7fd-b91bf2676177 | Beispiel-LB | b15cde70d85749689e08106f973bb002 | 10.0.0.10 | PENDING_CREATE | amphora | +--------------------------------------+-------------+----------------------------------+--------------+---------------------+----------+ . Mit dem provisioning_status ACTIVE ist dieser Vorgang erfolgreich abgeschlossen und der erste Octavia-Loadbalancer kann weiter konfiguiert werden. $ openstack loadbalancer list +--------------------------------------+-------------+----------------------------------+--------------+---------------------+----------+ | id | name | project_id | vip_address | provisioning_status | provider | +--------------------------------------+-------------+----------------------------------+--------------+---------------------+----------+ | e94827f0-f94d-40c7-a7fd-b91bf2676177 | Beispiel-LB | b15cde70d85749689e08106f973bb002 | 10.0.0.10 | ACTIVE | amphora | +--------------------------------------+-------------+----------------------------------+--------------+---------------------+----------+ . ",
    "url": "/optimist/networking/octavia_loadbalancer/#erstellung-eines-octavia-ladbalancer",
    
    "relUrl": "/optimist/networking/octavia_loadbalancer/#erstellung-eines-octavia-ladbalancer"
  },"442": {
    "doc": "Octavia Loadbalancers",
    "title": "Erstellen eines LB-Listener",
    "content": "In unserem Beispiel wollen wir einen Listener für den HTTP-Port 80 erstellen. Als Listener ist hier - vergleichbar mit anderen LB-Lösungen - der Port des Frontends gemeint. $ openstack loadbalancer listener create --name Beispiel-listener --protocol HTTP --protocol-port 80 Beispiel-LB +-----------------------------+--------------------------------------+ | Field | Value | +-----------------------------+--------------------------------------+ | admin_state_up | True | connection_limit | -1 | created_at | 2019-05-01T09:00:00 | default_pool_id | None | default_tls_container_ref | None | description | | id | 0a3312d1-8cf7-41a8-8d24-181246468cd7 | insert_headers | None | l7policies | | loadbalancers | e94827f0-f94d-40c7-a7fd-b91bf2676177 | name | Beispiel-listener | operating_status | OFFLINE | project_id | b15cde70d85749689e08106f973bb002 | protocol | HTTP | protocol_port | 80 | provisioning_status | PENDING_CREATE | sni_container_refs | [] | timeout_client_data | 50000 | timeout_member_connect | 5000 | timeout_member_data | 50000 | timeout_tcp_inspect | 0 | updated_at | None | client_ca_tls_container_ref | | client_authentication | | client_crl_container_ref | +-----------------------------+--------------------------------------+ . Der Befehl war erfolgreich, wenn der admin_state_up True ist. $ openstack loadbalancer listener list +--------------------------------------+-----------------+-------------------+----------------------------------+----------+---------------+----------------+ | id | default_pool_id | name | project_id | protocol | protocol_port | admin_state_up | +--------------------------------------+-----------------+-------------------+----------------------------------+----------+---------------+----------------+ | 0a3312d1-8cf7-41a8-8d24-181246468cd7 | None | Beispiel-listener | b15cde70d85749689e08106f973bb002 | HTTP | 80 | True | +--------------------------------------+-----------------+-------------------+----------------------------------+----------+---------------+----------------+ . ",
    "url": "/optimist/networking/octavia_loadbalancer/#erstellen-eines-lb-listener",
    
    "relUrl": "/optimist/networking/octavia_loadbalancer/#erstellen-eines-lb-listener"
  },"443": {
    "doc": "Octavia Loadbalancers",
    "title": "Erstellen eines LB-Pools",
    "content": "Als LB-Pool ist hier eine Ansammlung aller Objekte (Listeners, Member, etc.) für zum Beispiel eine Region gemeint - vergleichbar mit einem Pool an öffentlichen IP-Adressen aus derer man sich eine belegen kann. Einen Pool für unser Beispiel erstellt man wie folgt: . $ openstack loadbalancer pool create --name Beispiel-pool --lb-algorithm ROUND_ROBIN --listener Beispiel-listener --protocol HTTP +----------------------+--------------------------------------+ | Field | Value | +----------------------+--------------------------------------+ | admin_state_up | True | created_at | 2019-05-01T09:00:00 | description | | healthmonitor_id | | id | 4053e88e-c2b5-47c6-987e-4387d837c88d | lb_algorithm | ROUND_ROBIN | listeners | 0a3312d1-8cf7-41a8-8d24-181246468cd7 | loadbalancers | e94827f0-f94d-40c7-a7fd-b91bf2676177 | members | | name | Beispiel-pool | operating_status | OFFLINE | project_id | b15cde70d85749689e08106f973bb002 | protocol | HTTP | provisioning_status | PENDING_CREATE | session_persistence | None | updated_at | None | tls_container_ref | | ca_tls_container_ref | | crl_container_ref | | tls_enabled | +----------------------+--------------------------------------+ . Hier sei erwähnt, dass man mit openstack loadbalancer pool create --help sich alle möglichen Einstellungen anzeigen lassen kann. Die häufigsten Einstellungen und deren Auswahlmöglichkeiten: . --protocol: {TCP,HTTP,HTTPS,TERMINATED_HTTPS,PROXY,UDP} --lb-algorithm {SOURCE_IP,ROUND_ROBIN,LEAST_CONNECTIONS} . Der Pool ist erfolgreich erstellt, wenn der provisioning_status den Status ACTIVE erreicht hat. $ openstack loadbalancer pool list +--------------------------------------+---------------+----------------------------------+---------------------+----------+--------------+----------------+ | id | name | project_id | provisioning_status | protocol | lb_algorithm | admin_state_up | +--------------------------------------+---------------+----------------------------------+---------------------+----------+--------------+----------------+ | 4053e88e-c2b5-47c6-987e-4387d837c88d | Beispiel-pool | b15cde70d85749689e08106f973bb002 | ACTIVE | HTTP | ROUND_ROBIN | True | +--------------------------------------+---------------+----------------------------------+---------------------+----------+--------------+----------------+ . ",
    "url": "/optimist/networking/octavia_loadbalancer/#erstellen-eines-lb-pools",
    
    "relUrl": "/optimist/networking/octavia_loadbalancer/#erstellen-eines-lb-pools"
  },"444": {
    "doc": "Octavia Loadbalancers",
    "title": "Erstellen der LB-member",
    "content": "Damit unser Loadbalancer weiß, an welche Backends er weiterleiten darf, fehlen uns noch sogenannte member, die wir wie folgt definieren: . $ openstack loadbalancer member create --subnet-id 32259126-dd37-44d5-922c-99d68ee870cd --address 10.0.0.11 --protocol-port 80 Beispiel-pool +---------------------+--------------------------------------+ | Field | Value | +---------------------+--------------------------------------+ | address | 10.0.0.11 | admin_state_up | True | created_at | 2019-05-01T09:00:00 | id | 703e27e0-e7fe-474b-b32d-68f9a8aeef07 | name | | operating_status | NO_MONITOR | project_id | b15cde70d85749689e08106f973bb002 | protocol_port | 80 | provisioning_status | PENDING_CREATE | subnet_id | 32259126-dd37-44d5-922c-99d68ee870cd | updated_at | None | weight | 1 | monitor_port | None | monitor_address | None | backup | False | +---------------------+--------------------------------------+ . und . $ openstack loadbalancer member create --subnet-id 32259126-dd37-44d5-922c-99d68ee870cd --address 10.0.0.12 --protocol-port 80 Beispiel-pool +---------------------+--------------------------------------+ | Field | Value | +---------------------+--------------------------------------+ | address | 10.0.0.12 | admin_state_up | True | created_at | 2019-05-01T09:00:00 | id | 2add1e17-73a6-4002-82af-538a3374e5dc | name | | operating_status | NO_MONITOR | project_id | b15cde70d85749689e08106f973bb002 | protocol_port | 80 | provisioning_status | PENDING_CREATE | subnet_id | 32259126-dd37-44d5-922c-99d68ee870cd | updated_at | None | weight | 1 | monitor_port | None | monitor_address | None | backup | False | +---------------------+--------------------------------------+ . Hier sei erwähnt, dass die beiden IP’s aus 10.0.0.* bereits vorhandene, auf Port 80 lauschende Webserver sind, die eine einfache Webseite mit Info’s über ihren Servicenamen ausliefern. Unter der Vorraussetzung es handelt sich bei diesen Webservern im folgenden Beispiel um ein Ubuntu/Debian und man hat root-Berechtigungen, könnte man die einfache Webseite schnell erstellen mit: . root@BeispielInstanz1:~# apt-get update; apt-get -y install apache2; echo \"you hit: you hit: webserver1\" &gt; /var/www/html/index.html . root@BeispielInstanz2:~# apt-get update; apt-get -y install apache2; echo \"you hit: you hit: webserver2\" &gt; /var/www/html/index.html . Das Resultat vom Anlegen der Member können wir wie folgt überprüfen: . $ openstack loadbalancer member list Beispiel-pool +--------------------------------------+------+----------------------------------+---------------------+--------------+---------------+------------------+--------+ | id | name | project_id | provisioning_status | address | protocol_port | operating_status | weight | +--------------------------------------+------+----------------------------------+---------------------+--------------+---------------+------------------+--------+ | 703e27e0-e7fe-474b-b32d-68f9a8aeef07 | b15cde70d85749689e08106f973bb002 | ACTIVE | 10.0.0.11 | 80 | NO_MONITOR | 1 | 2add1e17-73a6-4002-82af-538a3374e5dc | b15cde70d85749689e08106f973bb002 | ACTIVE | 10.0.0.12 | 80 | NO_MONITOR | 1 | +--------------------------------------+------+----------------------------------+---------------------+--------------+---------------+------------------+--------+ . Nun ist das “interne” Konstrukt des Loadbalancers konfiguriert. Wir haben nun: . | 2 member die über Port 80 den tatsächlichen Service bereitstellen und zwischen denen das Loadbalancing stattfindet, | einen pool für diese member, | einen listener, der auf Port TCP/80 lauscht und ein ROUND_ROBIN zu den beiden Endpunkten macht und | einen Loadbalancer, über den wir alle Komponenten vereint haben. | . Der operating_status NO_MONITOR wird unter healthmonitor korrigiert. ",
    "url": "/optimist/networking/octavia_loadbalancer/#erstellen-der-lb-member",
    
    "relUrl": "/optimist/networking/octavia_loadbalancer/#erstellen-der-lb-member"
  },"445": {
    "doc": "Octavia Loadbalancers",
    "title": "Erstellen und konfigurieren der Floating-IP",
    "content": "Damit wir auch den Loadbalancer außerhalb unseres Beispiel-Netzwerk einsetzen können, müssen wir eine FloatingIP reservieren und diese dann mit dem vip_port_id des Beispiel-LB verknüpfen. Mit folgendem Befehl erstellen wir uns eine Floating IP aus dem provider-Netz: . $ openstack floating ip create provider +---------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | Field | Value | +---------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | created_at | 2019-05-01T09:00:00Z | description | | dns_domain | None | dns_name | None | fixed_ip_address | None | floating_ip_address | 185.116.247.133 | floating_network_id | 54258498-a513-47da-9369-1a644e4be692 | id | 46c0e8cf-783d-44a0-8256-79f8ae0be7fe | location | Munch({'project': Munch({'domain_id': 'default', 'id': u'b15cde70d85749689e08106f973bb002', 'name': 'beispiel-tenant', 'domain_name': None}), 'cloud': '', 'region_name': 'fra', 'zone': None}) | name | 185.116.247.133 | port_details | None | port_id | None | project_id | b15cde70d85749689e08106f973bb002 | qos_policy_id | None | revision_number | 0 | router_id | None | status | DOWN | subnet_id | None | tags | [] | updated_at | 2019-05-01T09:00:00Z | +---------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ . Im nächsten Schritt benötigen wir die vip_port_id des Loadbalancers. Diese bekommt man mit folgendem Befehl heraus: . $ openstack loadbalancer show Beispiel-LB -f value -c vip_port_id 37fc5b34-ee07-49c8-b054-a8d591a9679f . Mit dem folgendem Befehl weisen wir dem Loadbalancer nun die öffentliche IP Adresse zu. Damit ist der LB (und somit auch die Endpunkte dahinter) aus dem Internet erreichbar. openstack floating ip set --port 37fc5b34-ee07-49c8-b054-a8d591a9679f 185.116.247.133 . Wir sind soweit, dass wir unser Loadbalancer-Deployment testen können. Mit folgendem Befehl fragen wir unseren Loadbalancer über Port TCP/80 an und bekommen anschließend eine entsprechende Antwort von den einzelnen member zurück: . $ for ((i=1;i&lt;=10;i++)); do curl http://185.116.247.133; sleep 1; done you hit: webserver1 you hit: webserver2 you hit: webserver1 you hit: webserver2 you hit: webserver1 you hit: webserver2 you hit: webserver1 ... (usw.) . ",
    "url": "/optimist/networking/octavia_loadbalancer/#erstellen-und-konfigurieren-der-floating-ip",
    
    "relUrl": "/optimist/networking/octavia_loadbalancer/#erstellen-und-konfigurieren-der-floating-ip"
  },"446": {
    "doc": "Octavia Loadbalancers",
    "title": "Erstellen eines healthmonitor",
    "content": "Mit dem folgenden Befehl erstellen wir einen Monitor, der bei einem Ausfall eines der Backends genau dieses fehlerhafte Backend aus der Lastverteilung nimmt und somit die Webseite oder Applikation weiterhin sauber ausgeliefert wird. $ openstack loadbalancer healthmonitor create --delay 5 --max-retries 2 --timeout 10 --type HTTP --name Beispielmonitor --url-path / Beispiel-pool +---------------------+--------------------------------------+ | Field | Value | +---------------------+--------------------------------------+ | project_id | b15cde70d85749689e08106f973bb002 | name | Beispielmonitor | admin_state_up | True | pools | 4053e88e-c2b5-47c6-987e-4387d837c88d | created_at | 2019-05-01T09:00:00 | provisioning_status | PENDING_CREATE | updated_at | None | delay | 5 | expected_codes | 200 | max_retries | 2 | http_method | GET | timeout | 10 | max_retries_down | 3 | url_path | / | type | HTTP | id | 368f9baa-708c-4e7f-ace1-02de15598c5d | operating_status | OFFLINE | http_version | | domain_name | +---------------------+--------------------------------------+ . In diesem Beispiel entfernt der Monitor das fehlerhafte Backend aus dem Pool, wenn die Integritätsprüfung (–type HTTP, –url-path / ) alle zwei Fünf-Sekunden-Intervalle fehlschlägt(–delay 5, –max-retries 2, –timeout 10). Wenn der Server wiederhergestellt wird und erneut auf TCP/80 reagiert, wird er erneut zum Pool hinzugefügt. Ein manueller Failover kann erzwungen werden, indem der Statuscodes des Webservers ungleich “200” ist, oder gar keine Antwort des Webservers erfolgt. $ openstack loadbalancer healthmonitor show Beispielmonitor +---------------------+--------------------------------------+ | Field | Value | +---------------------+--------------------------------------+ | project_id | b15cde70d85749689e08106f973bb002 | name | Beispielmonitor | admin_state_up | True | pools | 4053e88e-c2b5-47c6-987e-4387d837c88d | created_at | 2019-05-01T09:00:00 | provisioning_status | ACTIVE | updated_at | 2019-05-01T09:00:00 | delay | 5 | expected_codes | 200 | max_retries | 2 | http_method | GET | timeout | 10 | max_retries_down | 3 | url_path | / | type | HTTP | id | 368f9baa-708c-4e7f-ace1-02de15598c5d | operating_status | ONLINE | http_version | | domain_name | +---------------------+--------------------------------------+ . Aus unserem Deployment-Beispiel würde das Resultat ungefähr so aussehen: . you hit: webserver1 Mi 22 Mai 2019 17:09:39 CEST you hit: webserver2 Mi 22 Mai 2019 17:09:40 CEST you hit: webserver1 Mi 22 Mai 2019 17:09:41 CEST you hit: webserver2 Mi 22 Mai 2019 17:09:42 CEST you hit: webserver1 Mi 22 Mai 2019 17:09:43 CEST - bis hier waren beide Webserver online, doch nun ist der webserver2 offline gegangen. you hit: webserver1 Mi 22 Mai 2019 17:09:44 CEST - noch erwarteter hit durch ROUND_ROBIN you hit: webserver1 Mi 22 Mai 2019 17:09:50 CEST - 1. retry zu webserver2 schlägt fehl you hit: webserver1 Mi 22 Mai 2019 17:09:56 CEST - 2. retry zu webserver2 schlägt fehl you hit: webserver1 Mi 22 Mai 2019 17:10:01 CEST - das Backend (webserver2) wurde aus dem Pool genommen. you hit: webserver1 Mi 22 Mai 2019 17:10:02 CEST you hit: webserver1 Mi 22 Mai 2019 17:10:03 CEST you hit: webserver1 Mi 22 Mai 2019 17:10:04 CEST you hit: webserver1 Mi 22 Mai 2019 17:10:05 CEST you hit: webserver1 Mi 22 Mai 2019 17:10:06 CEST . ",
    "url": "/optimist/networking/octavia_loadbalancer/#erstellen-eines-healthmonitor",
    
    "relUrl": "/optimist/networking/octavia_loadbalancer/#erstellen-eines-healthmonitor"
  },"447": {
    "doc": "Octavia Loadbalancers",
    "title": "Monitoring mit Prometheus",
    "content": "Der Octavia Amphora-Driver bietet einen Prometheus-Endpunkt. Auf diese Weise können Sie Metriken von Octavia-Loadbalancern sammeln. Um einen Prometheus-Endpunkt zu einem vorhandenen Octavia-Load-Balancer hinzuzufügen, erstellen Sie einen Listener mit dem Protokoll PROMETHEUS. Dadurch wird der Endpunkt als /metrics auf dem Listener aktiviert. Der Listener unterstützt alle Funktionen eines Octavia-Load-Balancers, z. B. allowed_cidrs, unterstützt jedoch nicht das Anhängen von Pools oder L7-Richtlinien. Alle Metriken werden durch die Octavia-Objekt-ID (UUID) der Ressourcen identifiziert. Hinweis: Derzeit werden UDP- und SCTP-Metriken nicht über Prometheus-Endpunkte gemeldet, wenn der Amphora-Provider verwendet wird. Um beispielsweise einen Prometheus-Endpunkt auf Port 8088 für den Load Balancer lb1 zu erstellen, führen Sie den folgenden Befehl aus: . $ openstack loadbalancer listener create --name stats-listener --protocol PROMETHEUS --protocol-port 8088 lb1 +-----------------------------+--------------------------------------+ | Field | Value | +-----------------------------+--------------------------------------+ | admin_state_up | True | connection_limit | -1 | created_at | 2021-10-03T01:44:25 | default_pool_id | None | default_tls_container_ref | None | description | | id | fb57d764-470a-4b6b-8820-627452f55b96 | insert_headers | None | l7policies | | loadbalancers | b081ed89-f6f8-48cb-a498-5e12705e2cf9 | name | stats-listener | operating_status | OFFLINE | project_id | 4c1caeee063747f8878f007d1a323b2f | protocol | PROMETHEUS | protocol_port | 8088 | provisioning_status | PENDING_CREATE | sni_container_refs | [] | timeout_client_data | 50000 | timeout_member_connect | 5000 | timeout_member_data | 50000 | timeout_tcp_inspect | 0 | updated_at | None | client_ca_tls_container_ref | None | client_authentication | NONE | client_crl_container_ref | None | allowed_cidrs | None | tls_ciphers | None | tls_versions | None | alpn_protocols | None | tags | +-----------------------------+--------------------------------------+ . Sobald der PROMETHEUS-Listener AKTIV ist, können Sie Prometheus so konfigurieren, dass es Metriken vom Load Balancer sammelt, indem Sie die Datei prometheus.yml aktualisieren. [scrape_configs] - job_name: 'Octavia LB1' static_configs: - targets: ['192.0.2.10:8088'] . ",
    "url": "/optimist/networking/octavia_loadbalancer/#monitoring-mit-prometheus",
    
    "relUrl": "/optimist/networking/octavia_loadbalancer/#monitoring-mit-prometheus"
  },"448": {
    "doc": "Octavia Loadbalancers",
    "title": "Bekannte Probleme",
    "content": "Wenn sie bei der Zuweisung der öffentliche IP Adresse zum Loadbalancer folgenden Fehler bekommen: . ResourceNotFound: 404: Client Error for url: https://network.fra.optimist.gec.io/v2.0/floatingips/46c0e8cf-783d-44a0-8256-79f8ae0be7fe, External network 54258498-a513-47da-9369-1a644e4be692 is not reachable from subnet 32259126-dd37-44d5-922c-99d68ee870cd. Therefore, cannot associate Port 37fc5b34-ee07-49c8-b054-a8d591a9679f with a Floating IP. dann fehlt eine Verbindung zwischen ihrem Beispiel-Netz (Router) und dem Provider-Netz (Schritt 10) . Die default-connect-Einstellung der haproxy-Prozesse innerhalb einer Amphore liegen bei 50 Sekunden, d.h. wenn eine Verbindung länger als 50 Sekunden anhalten soll, müssen sie am Listener diese Werte entsprechend konfigurieren. Beispiel für einen Connect mit Timeout: . $ time kubectl -n kube-system exec -ti machine-controller-5f649c5ff4-pksps /bin/sh ~ $ 50.69 real 0.08 user 0.05 sys . Um in diesem Beispiel den Timeout auf 4h zu erweitern: . openstack loadbalancer listener set --timeout_client_data 14400000 &lt;Listener ID&gt; openstack loadbalancer listener set --timeout_member_data 14400000 &lt;Listener ID&gt; . Wenn Octavia versucht, einen LB mit port_security_enabled = False in einem Netzwerk zu starten, wird der LB in den Status ERROR versetzt. ",
    "url": "/optimist/networking/octavia_loadbalancer/#bekannte-probleme",
    
    "relUrl": "/optimist/networking/octavia_loadbalancer/#bekannte-probleme"
  },"449": {
    "doc": "Octavia Loadbalancers",
    "title": "Abschluss",
    "content": "Es macht durchaus Sinn immer einen Monitor für seinen Pool zu etablieren. ",
    "url": "/optimist/networking/octavia_loadbalancer/#abschluss",
    
    "relUrl": "/optimist/networking/octavia_loadbalancer/#abschluss"
  },"450": {
    "doc": "Octavia Loadbalancers",
    "title": "Octavia Loadbalancers",
    "content": " ",
    "url": "/optimist/networking/octavia_loadbalancer/",
    
    "relUrl": "/optimist/networking/octavia_loadbalancer/"
  },"451": {
    "doc": "VPN as a Service",
    "title": "VPN as a Service (VPNaaS)",
    "content": "OpenStack unterstützt bei Bedarf Site-to-Site VPNs as a Service. Damit können Benutzer zwei private Netzwerke miteinander verbinden. Dazu werden von OpenStack voll funktionale IPsec VPNs innerhalb eines Projekts konfiguriert, ohne dass weitere netzwerkfähige VMs benötigt werden. ",
    "url": "/optimist/networking/vpnaas/#vpn-as-a-service-vpnaas",
    
    "relUrl": "/optimist/networking/vpnaas/#vpn-as-a-service-vpnaas"
  },"452": {
    "doc": "VPN as a Service",
    "title": "Anlegen eines Site-to-Site IPSec VPN",
    "content": "Erzeugen von “linken” und “rechten” Netzwerken und Subnetzen . Bevor wir ein VPN anlegen können, benötigen wir zwei getrennte Netzwerke, die miteinander verbunden werden sollen. In dieser Anleitung erzeugen wir diese Netzwerke in zwei verschiedenen OpenStack Projekten und nennen sie “links” (left) und “rechts” (right). Die folgenden Schritte müssen für beide Netzwerke (“links” und “rechts”) durchgeführt werden, damit die zwei verschiedenen OpenStack Cluster miteinander verbunden werden können. Der Einfachheit halber zeigen wir in dieser Anleitung nur, wie wir das linke Netzwerk erzeugen. Die Schritte für das rechte Netzwerk sind bis auf den Namen und das Subnetz-Präfix für OpenStack identisch. In diesem Beispiel verwenden wir das Subnetz-Präfix 2001:db8:1:33bc::/64 für das linke Netzwerk und 2001:db8:1:33bd::/64 für das rechte Netzwerk. Falls Sie bereits über zwei Netzwerke verfügen, die Sie über einen Site-to-Site VPN verbinden möchten, können Sie den Schritt Erzeugen von IKE und IPSec Policies auf beiden Seiten überspringen. Verwenden von Horizon (GUI) . | Erzeugen Sie das linke Netzwerk mit einem neuen Subnetz. | . Navigieren Sie innerhalb Ihres Projekts zu Network → Networks und klicken Sie auf Create Network. Geben Sie dem neuen Netzwerk einen Namen und wählen Sie Enable Admin State , um das Netzwerk zu aktivieren. Wählen Sie anschließend Create Subnet aus, um das Netzwerk und Subnetz in einem Schritt zu erzeugen. Klicken Sie danach auf Next. Geben Sie Ihrem neuen Netzwerk-Subnetz einen Namen und wählen Sie Enter Network Address manually aus. Falls Sie Ihr eigenes Subnetz verwenden möchten, geben Sie Ihr gewünschtes Subnetz in Network Address ein. Falls Sie ein Subnetz von einem vordefinierten Pool verwenden möchten, wählen Sie Allocate Network Address from a pool und wählen Sie einen Pool aus. Klicken Sie anschließend auf Next. Für Dokumentationszwecke verwenden wir die vorher genannten Präfixe. Wählen Sie Enable DHCP und unter IPv6 Address Configuration Mode DHCPV6 STATEFUL aus. Die Allokationspools werden automatisch erzeugt. Klicken Sie auf Create. Verwenden von CLI . | Erzeugen Sie das linke Netzwerk mit dem Befehl openstack network create. | . $ openstack network create vpnaas-left-network +---------------------------+--------------------------------------+ | Field | Value | +---------------------------+--------------------------------------+ | admin_state_up | UP | availability_zone_hints | | availability_zones | | created_at | 2022-09-12T12:45:42Z | description | | dns_domain | | id | ff7c61f1-4dcb-49bf-be9f-efdcaa1e0aaa | ipv4_address_scope | None | ipv6_address_scope | None | is_default | False | is_vlan_transparent | None | mtu | 1500 | name | vpnaas-left-network | port_security_enabled | True | project_id | 281fa14f782e4d4cbfd4e34a121c2680 | provider:network_type | None | provider:physical_network | None | provider:segmentation_id | None | qos_policy_id | None | revision_number | 1 | router:external | Internal | segments | None | shared | False | status | ACTIVE | subnets | | tags | | updated_at | 2022-09-12T12:45:42Z | +---------------------------+--------------------------------------+ . | Erzeugen Sie ein neues Subnetz und weisen Sie es mit dem Befehl openstack subnet create dem neuen Netzwerk zu. | . $ openstack subnet create \\ vpnaas-left-network-subnet \\ --subnet-range 2001:db8:1:33bc::/64 --ip-version 6 \\ --network vpnaas-left-network +----------------------+--------------------------------------------------------+ | Field | Value | +----------------------+--------------------------------------------------------+ | allocation_pools | 2001:db8:1:33bc::1-2001:db8:1:33bc:ffff:ffff:ffff:ffff | cidr | 2001:db8:1:33bc::/64 | created_at | 2022-09-12T12:47:51Z | description | | dns_nameservers | | dns_publish_fixed_ip | None | enable_dhcp | True | gateway_ip | 2001:db8:1:33bc:: | host_routes | | id | e217a377-48c7-4c18-93b5-cfd805bde40a | ip_version | 6 | ipv6_address_mode | None | ipv6_ra_mode | None | name | vpnaas-left-network-subnet | network_id | ff7c61f1-4dcb-49bf-be9f-efdcaa1e0aaa | project_id | 281fa14f782e4d4cbfd4e34a121c2680 | revision_number | 0 | segment_id | None | service_types | | subnetpool_id | None | tags | | updated_at | 2022-09-12T12:47:51Z | +----------------------+--------------------------------------------------------+ . Erzeugen des linken und rechten Routers . Verwenden von Horizon (GUI) . | Erzeugen Sie einen Router mit dem Provider-Netzwerk als externes Gateway. | . Navigieren Sie innerhalb Ihres Projekts zu Network → Routers und klicken Sie auf Create Router. Geben Sie dem neuen Router einen Namen. Wählen Sie Enable Admin State, um den Router zu aktivieren. Wählen Sie “PROVIDER” als External Network und klicken Sie auf Create Router. Verwenden von CLI . | Erzeugen Sie einen Router mit dem Befehl openstack router create. | . $ openstack router create vpnaas-left-router +-------------------------+--------------------------------------+ | Field | Value | +-------------------------+--------------------------------------+ | admin_state_up | UP | availability_zone_hints | | availability_zones | | created_at | 2022-09-12T12:48:15Z | description | | enable_ndp_proxy | None | external_gateway_info | null | flavor_id | None | id | 052e968a-a63b-4824-b904-eb70c42c53e5 | name | vpnaas-left-router | project_id | 281fa14f782e4d4cbfd4e34a121c2680 | revision_number | 2 | routes | | status | ACTIVE | tags | | tenant_id | 281fa14f782e4d4cbfd4e34a121c2680 | updated_at | 2022-09-12T12:48:15Z | +-------------------------+--------------------------------------+ . | Verwenden Sie den Befehl openstack router set, um das Provider-Netzwerk als externes Gateway für den Router anzulegen. | . $ openstack router set vpnaas-left-router --external-gateway provider . Zuordnen des Subnetzes an den Router . Verwenden von Horizon (GUI) . Navigieren Sie innerhalb Ihres Projekts zu Network → Routers und wählen Sie den zuvor erzeugten Router aus. Wählen Sie Interfaces und klicken Sie auf Add Interface. Wählen Sie Ihr Subnetz aus und klicken Sie auf Submit. Verwenden von CLI . Verwenden Sie den Befehl openstack router add subnet, um das Subnetz mit dem Router zu verbinden. $ openstack router add subnet vpnaas-left-router vpnaas-left-network-subnet . Erzeugen von IKE und IPSec Policies auf beiden Seiten . Die IKE und IPSec Policies müsssen auf beiden Seiten identisch konfiguriert werden. In dieser Anleitung verwenden wir die folgenden Parameter: . | Parameter | IKE Policy | IPSec Policy | . | Authorization algorithm | SHA256 | SHA256 | . | Encryption algorithm | AES-256 | AES-256 | . | Encapsulation mode | N/A | TUNNEL | . | IKE Version | V2 | N/A | . | Perfect Forward Secrecy | GROUP14 | GROUP14 | . | Transform Protocol | N/A | ESP | . Verwenden von Horizon (GUI) . | Erzeugen Sie die IKE Policy. | . Navigieren Sie innerhalb Ihres Projekts zu Network → VPN, wählen Sie IKE Policies aus und klicken Sie auf Add IKE Policy. Geben Sie Ihrer IKE Policy einen Namen und geben Sie die IKE Policy Parameter ein. Klicken Sie anschließend auf Add. | Erzeugen Sie die IPSec Policy. | . Sie befinden sich noch immer innerhalb von Network → VPN. Wählen Sie IPSec Policies aus und klicken Sie auf Add IPsec Policy. Geben Sie Ihrer IPSec Policy einen Namen und geben Sie die IPSec Policy Parameter ein. Klicken Sie anschließend auf Add. Verwenden von CLI . | Erzeugen Sie die IKE Policy mit dem Befehl openstack vpn ike policy create. | . $ openstack vpn ike policy create \\ vpnaas-left-ike-policy \\ --auth-algorithm sha256 \\ --encryption-algorithm aes-256 \\ --ike-version v2 \\ --pfs group14 +-------------------------------+--------------------------------------+ | Field | Value | +-------------------------------+--------------------------------------+ | Authentication Algorithm | sha256 | Description | | Encryption Algorithm | aes-256 | ID | 561387b8-b5c1-415e-abc9-79ba93dd48ff | IKE Version | v2 | Lifetime | {'units': 'seconds', 'value': 3600} | Name | vpnaas-left-ike-policy | Perfect Forward Secrecy (PFS) | group14 | Phase1 Negotiation Mode | main | Project | 281fa14f782e4d4cbfd4e34a121c2680 | project_id | 281fa14f782e4d4cbfd4e34a121c2680 | +-------------------------------+--------------------------------------+ . | Erzeugen Sie die IPSec Policy mit dem Befehl openstack vpn ipsec policy create. | . $ openstack vpn ipsec policy create \\ vpnaas-left-ipsec-policy \\ --auth-algorithm sha256 \\ --encryption-algorithm aes-256 \\ --pfs group14 \\ --transform-protocol esp +-------------------------------+--------------------------------------+ | Field | Value | +-------------------------------+--------------------------------------+ | Authentication Algorithm | sha256 | Description | | Encapsulation Mode | tunnel | Encryption Algorithm | aes-256 | ID | 553a600e-f39d-47a0-9550-97f2b4033685 | Lifetime | {'units': 'seconds', 'value': 3600} | Name | vpnaas-left-ipsec-policy | Perfect Forward Secrecy (PFS) | group14 | Project | 281fa14f782e4d4cbfd4e34a121c2680 | Transform Protocol | esp | project_id | 281fa14f782e4d4cbfd4e34a121c2680 | +-------------------------------+--------------------------------------+ . Erzeugen der VPN Services auf beiden Seiten . Verwenden von Horizon (GUI) . Navigieren Sie innerhalb Ihres Projekts zu Network → VPN. Wählen Sie VPN Services aus und klicken Sie auf Add VPN Service. Geben Sie Ihrem VPN Service einen Namen. Wählen Sie Ihren Router aus und selektieren Sie Enable Admin State. Ein Subnetz wird nicht benötigt, da wir die Endpoint Gruppen verwenden. Klicken Sie anschließend auf Add. Verwenden von CLI . Verwenden Sie den Befehl openstack vpn service create, um den VPN Service zu erzeugen. $ openstack vpn service create vpnaas-left-vpn --router vpnaas-left-router +----------------+--------------------------------------+ | Field | Value | +----------------+--------------------------------------+ | Description | | Flavor | None | ID | cc258fd7-0e87-4058-ad7d-355f32c1ab5e | Name | vpnaas-left-vpn | Project | 281fa14f782e4d4cbfd4e34a121c2680 | Router | 052e968a-a63b-4824-b904-eb70c42c53e5 | State | True | Status | PENDING_CREATE | Subnet | None | external_v4_ip | 185.116.244.85 | external_v6_ip | 2a00:c320:1003::23a | project_id | 281fa14f782e4d4cbfd4e34a121c2680 | +----------------+--------------------------------------+ . Erzeugen der Endpoint Gruppen . Bei Verwendung mehrerer Subnetze muss sichergestellt werden, dass der VPN-Endpunkt das Routing mehrerer Subnetze über dieselbe Verbindung unterstützt. Während OpenStack dies tut, müssen für Implementierungen, welche dies nicht unterstützen, mehrere Endpunktgruppen erstellt werden, eine für jedes Subnetz. Verwenden von Horizon (GUI) . | Erzeugen Sie die lokale Endpoint Gruppe für die linke Seite. | . Navigieren Sie innerhalb Ihres Projekts zu Network → VPN. Wählen Sie Endpoint Groups aus und klicken Sie auf Add Endpoint Group. Geben Sie Ihrer Endpoint Gruppe einen Namen. Wählen Sie als Typ Subnet und wählen Sie unter Local System Subnets Ihr Subnetz aus. Klicken Sie anschließend auf Add. | Erzeugen Sie die Peer Endpoint Gruppe für die linke Seite. | . Sie befinden sich noch immer innerhalb von Network → VPN und Endpoint Groups. Klicken Sie erneut auf Add Endpoint Group. Geben Sie Ihrer Endpoint Gruppe einen Namen. Wählen Sie den Typ CIDR und geben Sie das Subnetz für die rechte Seite ein. Klicken Sie anschließend auf Add. Verwenden von CLI . | Verwenden Sie den Befehl openstack vpn endpoint group create, um die lokale Endpoint Gruppe für die linke Seite zu erzeugen. | . $ openstack vpn endpoint group create \\ vpnaas-left-local \\ --type subnet \\ --value vpnaas-left-network-subnet +-------------+------------------------------------------+ | Field | Value | +-------------+------------------------------------------+ | Description | | Endpoints | ['e217a377-48c7-4c18-93b5-cfd805bde40a'] | ID | 949ccc53-5dc6-457d-95bf-278fdf9a3e5d | Name | vpnaas-left-local | Project | 281fa14f782e4d4cbfd4e34a121c2680 | Type | subnet | project_id | 281fa14f782e4d4cbfd4e34a121c2680 | +-------------+------------------------------------------+ . | Verwenden Sie erneut den Befehl openstack vpn endpoint group create, um die Peer Endpoint Gruppe für die linke Seite zu erzeugen. | . $ openstack vpn endpoint group create \\ vpnaas-left-remote \\ --type cidr \\ --value 2001:db8:1:33bd::/64 +-------------+--------------------------------------+ | Field | Value | +-------------+--------------------------------------+ | Description | | Endpoints | ['2001:db8:1:33bd::/64'] | ID | 9146346d-1306-4b03-a3ce-04ee51832ed8 | Name | vpnaas-left-remote | Project | 281fa14f782e4d4cbfd4e34a121c2680 | Type | cidr | project_id | 281fa14f782e4d4cbfd4e34a121c2680 | +-------------+--------------------------------------+ . Erzeugen der Site-Verbindungen . Wie auch bei den Endpunktgruppen müssen Sie, wenn Ihr VPN-Endpunkt das Routing mehrerer Subnetze über dieselbe Verbindung nicht unterstützt, mehrere Site-Verbindungen erstellen, eine für jedes Subnetz/Endpunktgruppe. Verwenden von Horizon (GUI) . Navigieren Sie innerhalb Ihres Projekts zu Network → VPN. Wählen Sie IPSec Site Connections aus und klicken Sie auf Add IPSec Site Connection. Geben Sie Ihrer Verbindung einen Namen. Wählen Sie den vorher erzeugten VPN Service, die lokale Endpoint Guppe und die IKE und IPSec Policy, den Pre-Shared Key, die Peer IP und die Router Identity. In dieser Anleitung nutzen wir 2001:db8::4:703 als IP Addresse des rechten Routers. Verwenden von CLI . Verwenden Sie den Befehl openstack vpn ipsec site connection create, um den VPN Service zu erzeugen. $ openstack vpn ipsec site connection create \\ vpnaas-left-connection \\ --vpnservice vpnaas-left-vpn \\ --ikepolicy vpnaas-left-ike-policy \\ --ipsecpolicy vpnaas-left-ipsec-policy \\ --local-endpoint-group vpnaas-left-local \\ --peer-address 2001:db8::4:703 \\ --peer-id 2001:db8::4:703 \\ --peer-endpoint-group vpnaas-left-remote \\ --psk 1gHAsAeR8lFEDDu7 +--------------------------+----------------------------------------------------+ | Field | Value | +--------------------------+----------------------------------------------------+ | Authentication Algorithm | psk | Description | | ID | d81dbe28-ccda-4ee3-ba96-145fadc74e0f | IKE Policy | 561387b8-b5c1-415e-abc9-79ba93dd48ff | IPSec Policy | 553a600e-f39d-47a0-9550-97f2b4033685 | Initiator | bi-directional | Local Endpoint Group ID | 949ccc53-5dc6-457d-95bf-278fdf9a3e5d | Local ID | | MTU | 1500 | Name | vpnaas-left-connection | Peer Address | 2001:db8::4:703 | Peer CIDRs | | Peer Endpoint Group ID | 9146346d-1306-4b03-a3ce-04ee51832ed8 | Peer ID | 2001:db8::4:703 | Pre-shared Key | 1gHAsAeR8lFEDDu7 | Project | 281fa14f782e4d4cbfd4e34a121c2680 | Route Mode | static | State | True | Status | PENDING_CREATE | VPN Service | cc258fd7-0e87-4058-ad7d-355f32c1ab5e | dpd | {'action': 'hold', 'interval': 30, 'timeout': 120} | project_id | 281fa14f782e4d4cbfd4e34a121c2680 | +--------------------------+----------------------------------------------------+ . ",
    "url": "/optimist/networking/vpnaas/#anlegen-eines-site-to-site-ipsec-vpn",
    
    "relUrl": "/optimist/networking/vpnaas/#anlegen-eines-site-to-site-ipsec-vpn"
  },"453": {
    "doc": "VPN as a Service",
    "title": "VPN as a Service",
    "content": " ",
    "url": "/optimist/networking/vpnaas/",
    
    "relUrl": "/optimist/networking/vpnaas/"
  },"454": {
    "doc": "Storage",
    "title": "Storage",
    "content": " ",
    "url": "/optimist/storage/",
    
    "relUrl": "/optimist/storage/"
  },"455": {
    "doc": "S3 Kompatiblen Objekt Storage",
    "title": "Einführung in den S3 Kompatiblen Objekt Storage",
    "content": " ",
    "url": "/optimist/storage/s3_documentation/#einf%C3%BChrung-in-den-s3-kompatiblen-objekt-storage",
    
    "relUrl": "/optimist/storage/s3_documentation/#einführung-in-den-s3-kompatiblen-objekt-storage"
  },"456": {
    "doc": "S3 Kompatiblen Objekt Storage",
    "title": "Was genau ist Object Storage?",
    "content": "Object Storage ist eine alternative zum klassischen Block Storage. Dabei werden die einzelnen Daten nicht einzelnen Blöcken auf einem Gerät zugeordnet, sondern als binäre Objekte innerhalb eines Storage Clusters gespeichert. Die notwendigen Metadaten werden in einer separaten DB gespeichert. Der Storage Cluster besteht dabei aus mehreren einzelnen Servern (Nodes), die wiederum mehrere Storage Devices verbaut haben. Bei den Storage Devices haben wir einen mix aus klassischen HDDs, SSDs und modernen NVMe Lösungen. Auf welchem Gerät letztendlich ein Object landet, wird durch den CRUSH Algorithmus entschieden, der beim Object Storage serverseitig implementiert ist. ",
    "url": "/optimist/storage/s3_documentation/#was-genau-ist-object-storage",
    
    "relUrl": "/optimist/storage/s3_documentation/#was-genau-ist-object-storage"
  },"457": {
    "doc": "S3 Kompatiblen Objekt Storage",
    "title": "Wie kann ich darauf zugreifen?",
    "content": "Der Zugriff auf diese Art von Storage erfolgt ausschließlich über HTTPS Zugriffe. Dafür stellen wir einen hochverfügbaren Endpunkt zur Verfügung, über den die einzelnen Operationen ausgeführt werden können. Dabei unterstützen wir zwei unabhängige Protokolle: . | S3 | Swift | . S3 ist ein von Amazon ins leben gerufene Protokoll, um mit dieser Art von Daten arbeiten zu können. Swift ist das Protokoll, welches vom gleichnamigen OpenStack Service bereitgestellt wird. Unabhängig davon, welches Protokoll man nutzt, hat man immer Zugriff auf seine gesamten Daten. Man kann also beide Protokolle in Kombination nutzen. Es gibt für alle gängigen Plattformen Tools, um mit den Daten im Object Storage arbeiten zu können: . | Windows: s3Browser, Cyberduck | MacOS: s3cmd, Cyberduck | Linux: s3cmd | . Darüberhinaus gibt es Integrationen in allen gängigen Programmiersprachen. ",
    "url": "/optimist/storage/s3_documentation/#wie-kann-ich-darauf-zugreifen",
    
    "relUrl": "/optimist/storage/s3_documentation/#wie-kann-ich-darauf-zugreifen"
  },"458": {
    "doc": "S3 Kompatiblen Objekt Storage",
    "title": "Wie sicher sind meine Daten?",
    "content": "Basis für den Object Storage ist unsere zugrunde liegende Openstack Cloud Plattform mit dem verteilten Ceph Storage Cluster. Die Objekte werden serverseitig über mehrere Storage Devices hinweg verteilt und repliziert. Dabei sorgt Ceph für die Sicherstellung von Replikation und Integrität der Datensätze. Fällt ein Server oder eine Festplatte aus, werden die betroffenen Datensätze auf verfügbare Server repliziert und das gewünschte Replikationslevel automatisch wiederhergestellt. Darüberhinaus werden die Daten in ein weiteres RZ auf einen weiteren dedizierten Storage Cluster gespiegelt und können von dort im K-Fall weitergenutzt werden. ",
    "url": "/optimist/storage/s3_documentation/#wie-sicher-sind-meine-daten",
    
    "relUrl": "/optimist/storage/s3_documentation/#wie-sicher-sind-meine-daten"
  },"459": {
    "doc": "S3 Kompatiblen Objekt Storage",
    "title": "Vorteile im Überblick",
    "content": ". | Bereitstellung per API: Die HTTPS-Schnittstelle ist sowohl kompatibel zur Amazon S3 API, also auch zur OpenStack Swift API. | unterstützt alle gängigen Betriebssysteme und Programmiersprachen. | Volle Skalierbarkeit - Der Storage kann dynamisch genutzt werden. | Höchste Ausfallsicherheit dank integrierter Replikation und Spiegelung über 2 unabhängige Rechenzentren. | Zugriff ist nahezu von jedem internetfähigen Gerät aus möglich - Somit eine super alternative zu NFS und Co. | PAYG Abrechnung nach genutztem Monatsdurchschnitt. | Transparente Abrechnung und somit gute Planbarkeit - Keine extra Traffic Kosten oder Kosten für Zugriffe auf die Daten. | Automatisierte Verwaltung der Objekte in Buckets mit s3 Lifecycle Policies möglich. | . ",
    "url": "/optimist/storage/s3_documentation/#vorteile-im-%C3%BCberblick",
    
    "relUrl": "/optimist/storage/s3_documentation/#vorteile-im-überblick"
  },"460": {
    "doc": "S3 Kompatiblen Objekt Storage",
    "title": "S3 Kompatiblen Objekt Storage",
    "content": " ",
    "url": "/optimist/storage/s3_documentation/",
    
    "relUrl": "/optimist/storage/s3_documentation/"
  },"461": {
    "doc": "S3 Kennung erstellen und einlesen",
    "title": "S3 Kennung erstellen und einlesen",
    "content": " ",
    "url": "/optimist/storage/s3_documentation/createanduses3credentials/",
    
    "relUrl": "/optimist/storage/s3_documentation/createanduses3credentials/"
  },"462": {
    "doc": "S3 Kennung erstellen und einlesen",
    "title": "Inhalt:",
    "content": ". | Credentials erstellen . | S3cmd | S3Browser | Cyberduck | Boto3 | . | Credentials anzeigen | Credentials löschen | . ",
    "url": "/optimist/storage/s3_documentation/createanduses3credentials/#inhalt",
    
    "relUrl": "/optimist/storage/s3_documentation/createanduses3credentials/#inhalt"
  },"463": {
    "doc": "S3 Kennung erstellen und einlesen",
    "title": "Credentials erstellen",
    "content": "Damit wir auf den Object Storage zugreifen können, benötigen wir zunächst Login Daten(Credentials). Um diese Daten per OpenStackAPI erzeugen zu können, benötigen wir den OpenStackClient und führen dort folgenden Befehl aus: . $ openstack ec2 credentials create . Wenn die Daten korrekt erstellt worden sind, sieht die Ausgabe in etwa so aus: . $ openstack ec2 credentials create +------------+-----------------------------------------------------------------+ | Field | Value | +------------+-----------------------------------------------------------------+ | access | aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa | links | {u'self': u'https://identity.optimist.gec.io/v3/users/bbb | | bbbbbbbbbbbbbbbbbbbbbbbbbbbbb/credentials/OS- | | EC2/aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa'} | project_id | cccccccccccccccccccccccccccccccc | secret | dddddddddddddddddddddddddddddddd | trust_id | None | user_id | bbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbb | +------------+-----------------------------------------------------------------+ . Nachdem die Zugangsdaten (Credentials) vorliegen, brauchen wir eine Möglichkeit auf den S3 kompatiblen ObjectStorage zuzugreifen. Hierfür gibt es die unterschiedliche Möglichkeiten, in der Dokumentation stellen wir hierfür vier Möglichkeiten vor, genauer: S3cmd für Linux/Mac, S3Browser für Windows, Cyberduck und Boto3. ",
    "url": "/optimist/storage/s3_documentation/createanduses3credentials/#credentials-erstellen",
    
    "relUrl": "/optimist/storage/s3_documentation/createanduses3credentials/#credentials-erstellen"
  },"464": {
    "doc": "S3 Kennung erstellen und einlesen",
    "title": "Benutzerdaten in die Konfigurationsdatei eintragen",
    "content": "S3cmd . Um s3cmd zu installieren, brauchen wir einen Paketmanager wie zum Beispiel “pip”. Die Installation und Nutzung erklären wir im Schritt 4: “Der Weg vom Horizon auf die Kommandozeile” unserer Guided Tour. Der Befehl für die Installation lautet dann: . pip install s3cmd . Nach der erfolgreichen Installation von s3cmd, müssen die vorher erstellten Zugangsdaten (Credentials) in die Konfigurationsdatei von s3cmd eingetragen werden. Die dafür zuständige Datei ist die “.s3cfg”, welche sich standardgemäß im Homeverzeichnis befindet. Sollte diese noch nicht existieren, muss diese vorher erstellt werden. Folgende Daten tragen wir dann in der .s3cfg ein und speichern diese: . access_key = aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa check_ssl_certificate = True check_ssl_hostname = True host_base = s3.es1.fra.optimist.gec.io host_bucket = s3.es1.fra.optimist.gec.io secret_key = dddddddddddddddddddddddddddddddd use_https = True . S3Browser . Für den S3Browser genügt es, diese heruterzuladen und zu installieren. Nach der erfolgreichen Installation, gilt es nun die entsprechenden Daten zu hinterlegen. Hierfür öffnen wir den S3Browser und es öffnet sich beim ersten Starten automatisch folgendes Fenster: . Dort tragen wir nun folgende Werte ein und klicken auf “Add new account” . * Account Name: Frei wählbarer Name für den Account * Account Type: S3 Compatible Storage * REST Endpoint: s3.es1.fra.optimist.gec.io * Signature Version: Signature V2 * Access Key ID: Den entsprechenden Access Key (Im Beispiel: aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa) * Secret Access Key: Das entsprechende Secret (Im Beispiel: dddddddddddddddddddddddddddddddd) . Cyberduck . Um Cyberduck zu nutzen, ist es notwendig diese herunterzuladen. Nach der Installation und dem ersten öffnen, ist es notwendig auf “Neue Verbindung” zu klicken. (1) Danach öffnet sich ein neues Fenster in dem im Dropdown Menü(2) “Amazon S3” ausgewählt wird und danach werden folgende Daten benötigt: . | Server(3): s3.es1.fra.optimist.gec.io | Access Key ID(4): Den entsprechenden Access Key (Im Beispiel: aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa) | Secret Access Key(5): Das entsprechende Secret (Im Beispiel: dddddddddddddddddddddddddddddddd) | . Um nun eine Verbindung herzustellen, wird als letzter Schritt auf “Verbinden” geklickt. Boto3 . Um Boto3 nutzen zu können, wird ein Paketmanager wie zum Beispiel “pip” benötigt. Die Installation und Nutzung wird im Schritt 4: “Der Weg vom Horizon auf die Kommandozeile” unserer Guided Tour erklärt. Der Befehl für die Installation lautet dann: . pip install boto3 . Nach der erfolgreichen Installation von boto3 ist es nun nutzbar. Wichtig ist, dass bei boto3 ein Script erstellt wird, welches am Ende ausgeführt wird. Daher ist der Konfigurationsteil der im Anschluss gezeigt wird, später immer Teil der weiterführenden Scripte. Hierfür erstellen wir eine Python-Datei wie z.B. “Beispiel.py” und fügen dort folgenden Inhalt ein: . | endpoint_url: s3.es1.fra.optimist.gec.io | aws_access_key_id: Den entsprechenden Access Key (Im Beispiel: aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa) | aws_secret_access_key: Das entsprechende Secret (Im Beispiel: dddddddddddddddddddddddddddddddd) | . #!/usr/bin/env/python import boto3 from botocore.client import Config s3 = boto3.resource('s3', endpoint_url='https://s3.es1.fra.optimist.gec.io', aws_access_key_id='aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa', aws_secret_access_key='dddddddddddddddddddddddddddddddd', ) . Dies dient als Startpunkt und wird in den folgenden Skripten referenziert und verwendet. ",
    "url": "/optimist/storage/s3_documentation/createanduses3credentials/#benutzerdaten-in-die-konfigurationsdatei-eintragen",
    
    "relUrl": "/optimist/storage/s3_documentation/createanduses3credentials/#benutzerdaten-in-die-konfigurationsdatei-eintragen"
  },"465": {
    "doc": "S3 Kennung erstellen und einlesen",
    "title": "Credentials anzeigen",
    "content": "Um erstellte Object Storage EC2-Credentials anzuzeigen benötigen wir den OpenstackClient und führen dort folgenden Befehl aus: . $ openstack ec2 credentials list . Der Befehl erstellt uns eine Liste mit allen EC2 Credentials, die für den aktuellen Nutzer sichtbar sind. $ openstack ec2 credentials list +----------------------------------+----------------------------------+----------------------------------+----------------------------------+ | Access | Secret | Project ID | User ID | +----------------------------------+----------------------------------+----------------------------------+----------------------------------+ | aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa | xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx | 12341234123412341234123412341234 | 32132132132132132132132132132132 | bbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbb | yyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyy | 56756756756756756756756756756756 | 65465465465465465465465465465465 | cccccccccccccccccccccccccccccccc | zzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz | 89089089089089089089089089089089 | 09809809809809809809809809809809 | +----------------------------------+----------------------------------+----------------------------------+----------------------------------+ . ",
    "url": "/optimist/storage/s3_documentation/createanduses3credentials/#credentials-anzeigen",
    
    "relUrl": "/optimist/storage/s3_documentation/createanduses3credentials/#credentials-anzeigen"
  },"466": {
    "doc": "S3 Kennung erstellen und einlesen",
    "title": "Credentials löschen",
    "content": "Um vorhandene Object Storage EC2-Credentials zu löschen benötigen wir den OpenstackClient und führen dort folgenden Befehl aus: . $ openstack ec2 credentials delete &lt;access-key&gt; . ",
    "url": "/optimist/storage/s3_documentation/createanduses3credentials/#credentials-l%C3%B6schen",
    
    "relUrl": "/optimist/storage/s3_documentation/createanduses3credentials/#credentials-löschen"
  },"467": {
    "doc": "Einen Bucket erstellen und wieder löschen",
    "title": "Einen Bucket erstellen und wieder löschen",
    "content": " ",
    "url": "/optimist/storage/s3_documentation/createanddeletebucket/",
    
    "relUrl": "/optimist/storage/s3_documentation/createanddeletebucket/"
  },"468": {
    "doc": "Einen Bucket erstellen und wieder löschen",
    "title": "Inhalt:",
    "content": ". | S3cmd | S3Browser | Cyberduck | Boto3 | . Zum Hochladen Ihrer Daten wie zum Beispiel (Dokumente, Fotos, Videos, usw.) ist es zunächst notwendig einen Bucket zu erstellen. Dieser dient als eine Art Ordner. Aufgrund der Funktionsweise unseres Objects-Storages ist es notwendig einen global eindeutigen Namen für Ihren Bucket zu verwenden. Sollte bereits ein Bucket mit dem gewählten Namen existieren, kann der Name erst verwendet werden, wenn der bereits existierende Bucket gelöscht wurde. Sollte der gewünschte Name bereits von einem weiteren Kunden in Benutzung sein müssen sie einen anderen Namen wählen. Es empfiehlt sich, Namen der Form “inhaltsbeschreibung.bucket.meine-domain.tld” oder vergleichbares zu verwenden. ",
    "url": "/optimist/storage/s3_documentation/createanddeletebucket/#inhalt",
    
    "relUrl": "/optimist/storage/s3_documentation/createanddeletebucket/#inhalt"
  },"469": {
    "doc": "Einen Bucket erstellen und wieder löschen",
    "title": "S3cmd",
    "content": "Bucket erstellen . Um einen Bucket zu erstellen, nutzt man folgenden Befehl: . s3cmd mb s3://NameDesBuckets . Die Ausgabe in der Kommandozeile kann dann so aussehen: . $ s3cmd mb s3://iNNOVO-Test Bucket 's3://iNNOVO-Test/' created . Bucket löschen . Um einen Bucket zu löschen, nutzt man folgenden Befehl: . s3cmd rb s3://NameDesBuckets . Die Ausgabe in der Kommandozeile kann dann so aussehen: . $ s3cmd rb s3://iNNOVO-Test Bucket 's3://iNNOVO-Test/' removed . ",
    "url": "/optimist/storage/s3_documentation/createanddeletebucket/#s3cmd",
    
    "relUrl": "/optimist/storage/s3_documentation/createanddeletebucket/#s3cmd"
  },"470": {
    "doc": "Einen Bucket erstellen und wieder löschen",
    "title": "S3Browser",
    "content": "Bucket erstellen . Nach dem Öffnen von S3Browser, klicken wir oben links auf “New bucket”(1), in dem sich öffnenden Fenster vergeben wir unter “Bucket name”(2) den Namen des Buckets und klicken dann auf “Create new bucket”(3). Bucket löschen . Zuerst wird der zu löschende Bucket markiert(1) und danch oben links auf “Delete bucket” geklickt. Im sich nun öffnenden Fenster, bestätigen mit dem markieren der Checkbox(1), dass die Datei gelöscht werden soll und klicken dann auf “Delete Bucket”(2). ",
    "url": "/optimist/storage/s3_documentation/createanddeletebucket/#s3browser",
    
    "relUrl": "/optimist/storage/s3_documentation/createanddeletebucket/#s3browser"
  },"471": {
    "doc": "Einen Bucket erstellen und wieder löschen",
    "title": "Cyberduck",
    "content": "Bucket erstellen . Nach dem Öffnen von Cyberduck, klicken wir oben in der Mitte auf “Aktion”(1) und auf “Neuer Ordner”(2) . Danach öffnet sich das folgende Fenster, hier können wir den Namen(1) festlegen und bestätigen dies im Anschluß mit “Anlegen”(2): . Bucket löschen . Um einen Bucket zu löschen, wird dieser mit einem linken Mausklick makiert. Gelöscht wird der Bucket dann über “Aktion”(1) und “Löschen”(2). Die Bestätigung erfolgt dann über das erneute klicken auf “Löschen”(1) . ",
    "url": "/optimist/storage/s3_documentation/createanddeletebucket/#cyberduck",
    
    "relUrl": "/optimist/storage/s3_documentation/createanddeletebucket/#cyberduck"
  },"472": {
    "doc": "Einen Bucket erstellen und wieder löschen",
    "title": "Boto3",
    "content": "Bei boto3 brauchen wir zunächst die S3 Kennung, damit ein Script nutzbar ist. Für Details: S3 Kennung erstellen und einlesen #boto3 . Bucket erstellen . Um nun einen Bucket zu erstellen, müssen wir einen Clienten nutzen und den Bucket dann erstellen. Eine Option sieht so aus: . ## Erstellung eines S3 Clienten s3 = boto3.client('s3') ## Erstellung des Buckets s3.create_bucket(Bucket='iNNOVO-Test') . Ein komplettes Script für boto 3 inkl. Authentifizierung kann so aussehen: . #!/usr/bin/env/python ## Definieren das boto3 genutzt werden soll import boto3 from botocore.client import Config ## Authentifizierung s3 = boto3.resource('s3', endpoint_url='https://s3.es1.fra.optimist.gec.io', aws_access_key_id='aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa', aws_secret_access_key='bbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbb', ) ## Erstellung eines S3 Clienten s3 = boto3.client('s3') ## Erstellung des Buckets s3.create_bucket(Bucket='iNNOVO-Test') . Bucket löschen . Wie bei der Erstellung des Buckets, wird zunächst ein Client benötigt um dann den Bucket zu löschen. Um nun einen Bucket zu erstellen, müssen wir einen Clienten nutzen und den Bucket dann erstellen. Eine Option sieht so aus: . ## Erstellung eines S3 Clienten s3 = boto3.client('s3') ## Löschen eines Buckets s3.delete_bucket(Bucket='iNNOVO-Test') . Ein komplettes Script für boto 3 inkl. Authentifizierung kann so aussehen: . #!/usr/bin/env/python ## Definieren das boto3 genutzt werden soll import boto3 from botocore.client import Config ## Authentifizierung s3 = boto3.resource('s3', endpoint_url='https://s3.es1.fra.optimist.gec.io', aws_access_key_id='aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa', aws_secret_access_key='6229490344a445f2aa59cdc0e53add88', ) ## Erstellung eines S3 Clienten s3 = boto3.client('s3') ## Erstellung des Buckets s3.delete_bucket(Bucket='iNNOVO-Test') . ",
    "url": "/optimist/storage/s3_documentation/createanddeletebucket/#boto3",
    
    "relUrl": "/optimist/storage/s3_documentation/createanddeletebucket/#boto3"
  },"473": {
    "doc": "Ein Objekt hochladen und löschen",
    "title": "Ein Objekt hochladen und löschen",
    "content": " ",
    "url": "/optimist/storage/s3_documentation/uploadanddeleteobject/",
    
    "relUrl": "/optimist/storage/s3_documentation/uploadanddeleteobject/"
  },"474": {
    "doc": "Ein Objekt hochladen und löschen",
    "title": "Inhalt:",
    "content": ". | S3cmd | S3Browser | Cyberduck | Boto3 | . Zum Hochladen Ihrer Daten wie zum Beispiel (Dokumente, Fotos, Videos, usw.) ist es zunächst notwendig einen Bucket zu erstellen. Eine Datei kann dabei nur in einem Bucket gespeichert werden. ",
    "url": "/optimist/storage/s3_documentation/uploadanddeleteobject/#inhalt",
    
    "relUrl": "/optimist/storage/s3_documentation/uploadanddeleteobject/#inhalt"
  },"475": {
    "doc": "Ein Objekt hochladen und löschen",
    "title": "S3cmd",
    "content": "Objekt hochladen . Um eine Datei hochzuladen, nutzt man folgenden Befehl: . s3cmd put NameDerDatei s3://NameDesBuckets/NameDerDatei . Die Ausgabe in der Kommandozeile kann dann so aussehen: . $ s3cmd put innovo.txt s3://innovo-test/innovo.txt&lt;font&gt;&lt;/font&gt; upload: 'innovo.txt' -&gt; 's3://innovo-test/innovo.txt' [1 of 1]&lt;font&gt;&lt;/font&gt; 95 of 95 100% in 0s 176.63 B/s done . Objekt löschen . Um eine Datei zu löschen, nutzt man folgenden Befehl: . s3cmd del s3://NameDesBuckets/NameDerDatei . Die Ausgabe in der Kommandozeile kann dann so aussehen: . $ s3cmd del s3://innovo-test/innovo.txt&lt;font&gt;&lt;/font&gt; delete: 's3://innovo-test/innovo.txt' . ",
    "url": "/optimist/storage/s3_documentation/uploadanddeleteobject/#s3cmd",
    
    "relUrl": "/optimist/storage/s3_documentation/uploadanddeleteobject/#s3cmd"
  },"476": {
    "doc": "Ein Objekt hochladen und löschen",
    "title": "S3Browser",
    "content": "Objekt hochladen . Nach dem öffnen von S3Browser, klicken wir auf den gewünschten “Bucket”(1), wähle dann “Upload”(2) und zu letzt “Upload file(s)”(3) . Hier wählen wir nun die entsprechende Datei(1) aus und klicken auf Öffnen(2). Objekt löschen . Um eine Datei zu löschen, wird dieser mit einem linken Mausklick markiert(1). Danach wird auf “Delete”(2) geklickt. Die darauf folgende Abfrage wird mit “Ja” bestätigt. ",
    "url": "/optimist/storage/s3_documentation/uploadanddeleteobject/#s3browser",
    
    "relUrl": "/optimist/storage/s3_documentation/uploadanddeleteobject/#s3browser"
  },"477": {
    "doc": "Ein Objekt hochladen und löschen",
    "title": "Cyberduck",
    "content": "Objekt hochladen . Nach dem Öffnen von Cyberduck, klicken wir auf den gewünschten Bucket(1), klicken dann auf Aktion(2) und dort auf Upload(3). Hier wählen wir nun unsere Wunsch-Datei und klicken auf Upload. Objekt löschen . Um eine Datei zu löschen, wird dieser mit einem linken Mausklick markiert(1). Gelöscht wird sie dann über “Aktion”(2) und “Löschen”(3). Die Bestätigung erfolgt dann über das erneute klicken auf “Löschen”. ",
    "url": "/optimist/storage/s3_documentation/uploadanddeleteobject/#cyberduck",
    
    "relUrl": "/optimist/storage/s3_documentation/uploadanddeleteobject/#cyberduck"
  },"478": {
    "doc": "Ein Objekt hochladen und löschen",
    "title": "Boto3",
    "content": "Bei boto3 brauchen wir zunächst die S3 Kennung, damit ein Script nutzbar ist. Für Details: S3 Kennung erstellen und einlesen #boto3 . Objekt hochladen . Um nun eine Datei hochzuladen, müssen wir einen Clienten nutzen und den Bucket angeben in welchen die Datei hochgeladen werden soll. Eine Option sieht so aus: . ## Erstellung eines S3 Clienten s3 = boto3.client('s3') ## Hochladen einer Datei s3.upload_file(Bucket='iNNOVO-Test', Key='innovo.txt') . Ein komplettes Script für boto 3 inkl. Authentifizierung kann so aussehen: . #!/usr/bin/env/python ## Definieren das boto3 genutzt werden soll import boto3 from botocore.client import Config ## Authentifizierung s3 = boto3.resource('s3', endpoint_url='https://s3.es1.fra.optimist.gec.io',&lt;font&gt;&lt;/font&gt; aws_access_key_id='aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',&lt;font&gt;&lt;/font&gt; aws_secret_access_key='bbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbb',&lt;font&gt;&lt;/font&gt; ) ## Erstellung eines S3 Clienten s3 = boto3.client('s3') ## Hochladen einer Datei s3.upload_file(Bucket='iNNOVO-Test', Key='innovo.txt') . Objekt löschen . Wie beim hochladen einer Datei, wird zunächst ein Client benötigt um dann die Datei zu löschen. Dafür geben wir neben der Datei selber, auch noch den Bucket an, in dem die Datei gespeichert ist. Eine Option sieht so aus: . ## Erstellung eines S3 Clienten s3 = boto3.client('s3') ## Löschen eines Objekts s3.delete_object(Bucket='iNNOVO-Test', Key='innovo.txt') . Ein komplettes Script für boto 3 inkl. Authentifizierung kann so aussehen: . #!/usr/bin/env/python ## Definieren das boto3 genutzt werden soll import boto3 from botocore.client import Config ## Authentifizierung s3 = boto3.resource('s3', endpoint_url='https://s3.es1.fra.optimist.gec.io', aws_access_key_id='aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa', aws_secret_access_key='bbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbb', ) ## Erstellung eines S3 Clienten s3 = boto3.client('s3') ## Löschen eines Objekts s3.delete_object(Bucket='iNNOVO-Test', Key='innovo.txt') . ",
    "url": "/optimist/storage/s3_documentation/uploadanddeleteobject/#boto3",
    
    "relUrl": "/optimist/storage/s3_documentation/uploadanddeleteobject/#boto3"
  },"479": {
    "doc": "Versionierung aktivieren/deaktivieren und wie eine versioniertes Objekt gelöscht wird",
    "title": "Versionierung aktivieren/deaktivieren und wie eine versioniertes Objekt gelöscht wird",
    "content": " ",
    "url": "/optimist/storage/s3_documentation/versioning/",
    
    "relUrl": "/optimist/storage/s3_documentation/versioning/"
  },"480": {
    "doc": "Versionierung aktivieren/deaktivieren und wie eine versioniertes Objekt gelöscht wird",
    "title": "Inhalt:",
    "content": ". | S3cmd | S3Browser | Cyberduck | Boto3 | . Versionierung ermöglicht es, mehrere Versionen eines Objekts in einem Bucket aufzubewahren. So können Beispielsweise innovo.txt (Version 1) und innovo.txt (Version 2) in einem einzigen Bucket speichern. Die Versionierung kann Sie vor den Folgen von unbeabsichtigtem Überschreiben oder Löschen bewahren. ",
    "url": "/optimist/storage/s3_documentation/versioning/#inhalt",
    
    "relUrl": "/optimist/storage/s3_documentation/versioning/#inhalt"
  },"481": {
    "doc": "Versionierung aktivieren/deaktivieren und wie eine versioniertes Objekt gelöscht wird",
    "title": "S3cmd",
    "content": "Mit S3cmd ist es nicht möglich die Versionierung einzuschalten und/oder versionierte Dateien zu löschen. ",
    "url": "/optimist/storage/s3_documentation/versioning/#s3cmd",
    
    "relUrl": "/optimist/storage/s3_documentation/versioning/#s3cmd"
  },"482": {
    "doc": "Versionierung aktivieren/deaktivieren und wie eine versioniertes Objekt gelöscht wird",
    "title": "S3Browser",
    "content": "Versionierung einschalten . Um die Versionierung zu aktivieren, markieren wir den gewünschten Bucket(1). Machen auf den Bucket einen rechten Mausklick und klicken dann auf “Edit Versioning Settings”(2). Im sich öffnenden Fenster klicken wir in die Checkbox von “Enable versioning for bucket”(1) und bestätigen dies mit “OK”(2). Versionierung deaktivieren . Um die Versionierung zu deaktivieren, markieren wir den gewünschten Bucket(1). Klicken dann mit einem rechten Mausklick auf den Bucket und dann auf “Edit Versioning Settings”(2). Im sich öffnenden Fenster entfernen wir die Checkbox bei “Enable versioning for bucket”(1) und bestätigen dies mit “OK”(2). Versionierte Datei löschen . Dies ist in der Free-Version von S3Browser nicht möglich. ",
    "url": "/optimist/storage/s3_documentation/versioning/#s3browser",
    
    "relUrl": "/optimist/storage/s3_documentation/versioning/#s3browser"
  },"483": {
    "doc": "Versionierung aktivieren/deaktivieren und wie eine versioniertes Objekt gelöscht wird",
    "title": "Cyberduck",
    "content": "Um die verschiedenen Version einer Datei zu sehen, müssen versteckte Dateien angezeigt werden. Diese Option findet man unter Darstellung(1) → Versteckte Dateien anzeigen(2) . Versionierung einschalten . Nach dem Öffnen von Cyberduck, klicken wir auf eine Datei, wo wir die Versionierung(1) für aktivieren wollen. Danach auf Aktion(2) und auf Info(3). Danach öffnet sich das folgende Fenster, hier setzen wir den Haken bei “Bucket Versionierung”(1): . Versionierung deaktivieren . Um die Versionierung zu deaktivieren, markieren wir wieder eine Datei(1), gehen auf Aktion(2) und auf Info(3). In dem sich öffnenden Fenster wird der Haken bei “Bucket Versionierung” entfernt. Versionierte Datei löschen . Hier wird einfach die zu löschende Datei markiert(1) und über Aktion(2) → Löschen(3) entfernt. ",
    "url": "/optimist/storage/s3_documentation/versioning/#cyberduck",
    
    "relUrl": "/optimist/storage/s3_documentation/versioning/#cyberduck"
  },"484": {
    "doc": "Versionierung aktivieren/deaktivieren und wie eine versioniertes Objekt gelöscht wird",
    "title": "Boto3",
    "content": "Bei boto3 brauchen wir zunächst die S3 Kennung, damit ein Script nutzbar ist. Für Details: S3 Kennung erstellen und einlesen #boto3. Versionierung einschalten . Um nun einen Bucket zu erstellen, müssen wir einen Clienten nutzen und den Bucket dann erstellen. Eine Option sieht so aus: . ## Angabe des Buckets in dem die Versionierung aktiviert werden soll bucket = s3.Bucket('iNNOVO-Test') ## Versionierung aktivieren bucket.configure_versioning(True) . Ein komplettes Script für boto 3 inkl. Authentifizierung kann so aussehen: . #!/usr/bin/env/python ## Definieren das boto3 genutzt werden soll import boto3 from botocore.client import Config ## Authentifizierung s3 = boto3.resource('s3', endpoint_url='https://s3.es1.fra.optimist.gec.io', aws_access_key_id='aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa', aws_secret_access_key='bbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbb', ) ## Angabe des Buckets in dem die Versionierung aktiviert werden soll bucket = s3.Bucket('iNNOVO-Test') ## Versionierung aktivieren bucket.configure_versioning(True) . Versionierung deaktivieren . Wie bei der Aktivierung der Versionierung wird zunächst der Bucket benötigt um dann die Versionierung zu deaktivieren. Eine Option sieht so aus: . ## Angabe des Buckets in dem die Versionierung deaktiviert werden soll bucket = s3.Bucket('iNNOVO-Test') ## Versionierung deaktivieren bucket.configure_versioning(False) . Ein komplettes Script für boto 3 inkl. Authentifizierung kann so aussehen: . #!/usr/bin/env/python ## Definieren das boto3 genutzt werden soll import boto3 from botocore.client import Config ## Authentifizierung s3 = boto3.resource('s3', endpoint_url='https://s3.es1.fra.optimist.gec.io', aws_access_key_id='aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa', aws_secret_access_key='6229490344a445f2aa59cdc0e53add88', ) ## Angabe des Buckets in dem die Versionierung deaktiviert werden soll bucket = s3.Bucket('iNNOVO-Test') ## Versionierung deaktivieren bucket.configure_versioning(False) . Versioniertes Objekt löschen . Um ein versioniertes Objekt komplett zu löschen, ist folgender Befehl hilfreich: . ## Angabe des Buckets in dem das versionierte Objekt gelöscht werden soll bucket = s3.Bucket('iNNOVO-Test') ## Versioniertes Objekt löschen bucket.object_versions.all().delete('innovo.txt') . ",
    "url": "/optimist/storage/s3_documentation/versioning/#boto3",
    
    "relUrl": "/optimist/storage/s3_documentation/versioning/#boto3"
  },"485": {
    "doc": "S3 Security",
    "title": "S3 Security",
    "content": " ",
    "url": "/optimist/storage/s3_documentation/security/",
    
    "relUrl": "/optimist/storage/s3_documentation/security/"
  },"486": {
    "doc": "S3 Security",
    "title": "Einführung",
    "content": "Diese Seite bietet einen Überblick über die folgenden Themen im Zusammenhang mit S3-Buckets / Swift: . | Container Access Control Lists (ACLs) | Bucket-Policies | . Operationen auf Container-ACLs müssen auf OpenStack-Ebene mit Swift-Befehlen ausgeführt werden, während Bucket-Policies für jeden Bucket innerhalb eines Projekts mit Hilfe der s3cmd-Befehlszeile festgelegt werden müssen. In diesem Dokument werden einige Beispiele für jede Art von Operation beschrieben. ",
    "url": "/optimist/storage/s3_documentation/security/#einf%C3%BChrung",
    
    "relUrl": "/optimist/storage/s3_documentation/security/#einführung"
  },"487": {
    "doc": "S3 Security",
    "title": "Container Access Control Lists (ACLs)",
    "content": "Standardmäßig haben nur Projektbesitzer die Berechtigung, Container und Objekte zu erstellen, zu lesen und zu ändern. Ein Eigentümer kann jedoch anderen Benutzern mit Hilfe einer Access Control List (ACL) Zugriff gewähren. Die ACL kann für jeden Container festgelegt werden und gilt nur für diesen Container und die Objekte in diesem Container. Einige der Hauptelemente, mit denen eine ACL für einen Container festgelegt werden kann, sind nachfolgend aufgeführt: . | Element | Beschreibung | . | .r: *. | Jeder Benutzer hat Zugriff auf Objekte. In der Anforderung ist kein Token erforderlich. | . | .r: &lt;Referrer&gt; | Der Referrer erhält Zugriff auf Objekte. Der Referrer wird durch den Referer-Anforderungsheader in der Anforderung identifiziert. Es ist kein Token erforderlich. | . | .r: - &lt;Referrer&gt; | Diese Syntax (mit “-“ vor dem Referrer) wird unterstützt. Der Zugriff wird jedoch nicht verweigert, wenn ein anderes Element (z. B.r:*) den Zugriff gewährt. | . | .rlistings | Jeder Benutzer kann HEAD- oder GET-Operationen für den Container ausführen, wenn der Benutzer auch Lesezugriff auf Objekte hat (z. B. auch .r: * oder .r: &lt;Referrer&gt;. Es ist kein Token erforderlich. | . Als Beispiel setzen wir die Policy .r:*. für einen Container mit dem Namen \"&lt;example-container&gt;\". Diese Policy ermöglicht jedem externen Benutzer den Zugriff auf die Objekte im Container. swift post example-container --read-acl \".r:*\" . Umgekehrt können wir Benutzern auch erlauben, die Liste der Objekte in einem Container aufzulisten, aber nicht darauf zuzugreifen, indem wir die Policy “.rlistings” für unseren “example-container” festlegen: . swift post example-container --read-acl \".rlistings\" . Der folgende Befehl kann verwendet werden, um die Lese-Policy zu entfernen und den Container auf den Standardstatus “privat” zu setzen: . swift post -r \"\" example-container . Verwenden Sie den folgenden Befehl, um zu überprüfen, welche ACL für einen Container festgelegt ist. swift stat example-container . Dies gibt einen Überblick über die Statistiken für den Container und zeigt die aktuelle ACL-Regel für einen Container an. Verhindern Sie das Auflisten auf Containern, wenn Sie die Policy .r: * . verwenden: . In der aktuellen Version von OpenStack empfehlen wir, ein leeres index.html-Objekt im Container zu erstellen, um zu verhindern, dass der Inhalt aufgelistet wird, während die Policy .r: * . für einen Container verwendet wird. Auf diese Weise können Benutzer Objekte herunterladen, ohne den Inhalt der Buckets aufzulisten zu koennen. Dies kann mit den folgenden Schritten erreicht werden: . Fügen Sie zunächst die leere Datei index.html zu unserem example-container hinzu: . swift post -m 'Webindex: index.html' example-container . Erstellen Sie dann die Datei index.html als Objekt im Container: . touch index.html &amp;&amp; openstack object create example-container index.html . Dadurch können externe Benutzer auf bestimmte Dateien zugreifen, ohne den Inhalt des Containers aufzulisten. ",
    "url": "/optimist/storage/s3_documentation/security/#container-access-control-lists-acls",
    
    "relUrl": "/optimist/storage/s3_documentation/security/#container-access-control-lists-acls"
  },"488": {
    "doc": "S3 Security",
    "title": "Bucket-Policy",
    "content": "Bucket-Policies werden verwendet, um den Zugriff auf jeden Bucket in einem Projekt zu steuern. Es wird empfohlen, bei der Erstellung eine Policy für alle Buckets festzulegen. Der erste Schritt besteht darin, eine Policy wie folgt zu erstellen. Für die folgende Vorlage muss nur der Bucket-Name für nachfolgende Policy geändert werden. Im folgenden Beispiel wird eine Policy für den Bucket example-bucket erstellt: . cat &gt; examplepolicy { \"Version\": \"2008-10-17\", \"Statement\": [ { \"Sid\": \"AddPerm\", \"Effect\": \"Allow\", \"Principal\": \"*\", \"Action\": \"s3:GetObject\", \"Resource\": \"arn:aws:s3:::example-bucket/*\" } ] } . Aufschlüsselung jedes Elements innerhalb des obigen Policy-Beispiels: . | Version: Gibt die Sprachsyntaxregeln an, die zum Verarbeiten der Policy verwendet werden sollen. Es wird empfohlen, immer Folgendes zu verwenden: “2012-10-17”, da dies die aktuelle Version der Policy-Sprache ist. | Statement: Das Hauptelement der Policy, die anderen Elemente befinden sich in dieser Anweisung. | SID: Die Statement-ID. Dies ist eine optionale Kennung, mit der die Richtlinienanweisung beschrieben werden kann. Empfohlen, damit der Zweck jeder Policy klar ist. | Effect: Stellen Sie entweder “Allow” oder “Deny” ein. | Principal: Gibt den Principal an, dem der Zugriff auf eine Ressource gestattet oder verweigert wird. Hier wird der Platzhalter “*” verwendet, um die Regel auf alle anzuwenden. | Action: Beschreibt die spezifischen Aktionen, die zugelassen oder abgelehnt werden. | . (Weitere Informationen zu den verfügbaren Policy-Optionen und zur Anpassung an Ihre spezifischen Anforderungen finden Sie in der offiziellen AWS-Dokumentation). Wenden Sie als Nächstes die neu erstellte Policy auf example-bucket an: . s3cmd setpolicy examplepolicy s3://example-bucket . Anschließend können Sie den folgenden Befehl ausführen, um sicherzustellen, dass die Policy vorhanden ist: . s3cmd info s3://example-bucket . Sobald die Policy angewendet wurde, können Sie im Dashboard erneut festlegen: Public Access: Disabled. Sobald die oben genannten Schritte ausgeführt wurden, erhalten wir die folgenden Ergebnisse: . | Der Container ist privat und Dateien werden nicht über XML aufgelistet oder angezeigt. | Die Policy ermöglicht jetzt den Zugriff auf bestimmte Dateien mit einem direkten Link. | . ",
    "url": "/optimist/storage/s3_documentation/security/#bucket-policy",
    
    "relUrl": "/optimist/storage/s3_documentation/security/#bucket-policy"
  },"489": {
    "doc": "Swift - Serving a Static Website",
    "title": "Swift - Serving Static Websites",
    "content": " ",
    "url": "/optimist/storage/s3_documentation/swiftservestaticwebsite/#swift---serving-static-websites",
    
    "relUrl": "/optimist/storage/s3_documentation/swiftservestaticwebsite/#swift---serving-static-websites"
  },"490": {
    "doc": "Swift - Serving a Static Website",
    "title": "Einführung",
    "content": "Mit Hilfe der Swift-CLI ist es möglich, die Daten in Containern als statische Website auszuliefern. Die folgende Anleitung beschreibt die wichtigsten Schritte, um damit zu beginnen, und enthält auch ein Beispiel. ",
    "url": "/optimist/storage/s3_documentation/swiftservestaticwebsite/#einf%C3%BChrung",
    
    "relUrl": "/optimist/storage/s3_documentation/swiftservestaticwebsite/#einführung"
  },"491": {
    "doc": "Swift - Serving a Static Website",
    "title": "Erste Schritte",
    "content": "Erstellen eines Containers . Zunächst erstellen wir einen Container mit dem Namen example-webpage, den wir als Basis für diese Anleitung verwenden werden: . swift post example-webpage . Den Container öffentlich lesbar machen . Als nächstes müssen wir sicherstellen, dass der Container öffentlich lesbar ist. Weitere Informationen zum Sichern von Containern und zum Festlegen von Bucket-Richtlinien finden Sie hier: . swift post -r '.r:*' example-webpage . Indexdatei der Seite setzen . Setzen Sie die Indexdatei. In diesem Fall wird index.html die Standarddatei sein, die angezeigt wird, wenn die Seite erscheint: . swift post -m 'web-index:index.html' example-webpage . Dateiliste aktivieren . Optional können wir auch die Dateiliste aktivieren. Wenn Sie mehrere Downloads bereitstellen müssen, ist es sinnvoll, die Verzeichnisliste zu aktivieren: . swift post -m 'web-listings: true' example-webpage . CSS für Dateilisten aktivieren . Aktivieren Sie ein benutzerdefiniertes Listing-Stylesheet: . swift post -m 'web-listings-css:style.css' example-webpage . Fehlerseiten einrichten . Schließlich sollten wir eine benutzerdefinierte Fehlerseite einbinden: . swift post -m 'web-error:404error.html' example-webpage . ",
    "url": "/optimist/storage/s3_documentation/swiftservestaticwebsite/#erste-schritte",
    
    "relUrl": "/optimist/storage/s3_documentation/swiftservestaticwebsite/#erste-schritte"
  },"492": {
    "doc": "Swift - Serving a Static Website",
    "title": "Beispiel-Webseite",
    "content": "Lassen Sie uns die Schritte rekapitulieren, die wir bis jetzt unternommen haben, um statische Webseiten zu aktivieren: . swift post example-webpage swift post -r '.r:*' example-webpage swift post -m 'web-index:index.html' example-webpage swift post -m 'web-listings: true' example-webpage swift post -m 'web-listings-css:style.css' example-webpage swift post -m 'web-error:404error.html' example-webpage . Wenn die obigen Schritte abgeschlossen sind, können wir damit beginnen, unsere statische Webseite anzupassen. Das Folgende demonstriert eine schnelle Einrichtung unter Verwendung unseres Containers example-webpage . Anpassen der Seiten index.html, page.html und 404error.html . Dies wird als Startseite dienen, die einen Link zu einer sekundären Seite erstellt. &lt;!-- index.html --&gt; &lt;html&gt; &lt;h1&gt; See the web page &lt;a href=\"mywebsite/page.html\"&gt;here&lt;/a&gt;. &lt;/h1&gt; &lt;/html&gt; . Die nächste Seite (page.html) zeigt ein Bild namens sample.png an: . &lt;!-- page.html --&gt; &lt;html&gt; &lt;img src=\"sample.png\"&gt; &lt;/html&gt; . Wir können auch benutzerdefinierte Fehlerseiten hinzufügen. Beachten Sie, dass derzeit nur die Fehler 401 (Nicht autorisiert) und 404 (Nicht gefunden) unterstützt werden. Das folgende Beispiel demonstriert die Erstellung einer 404-Fehlerseite: . &lt;!-- 404error.html --&gt; &lt;html&gt; &lt;h1&gt; 404 Not Found - We cannot find the page you are looking for! &lt;/h1&gt; &lt;/html&gt; . Hochladen der Dateien index.html und page.html . Nachdem die Inhalte der Dateien erstellt wurden, laden Sie die Dateien mit den folgenden Befehlen hoch: . swift upload beispiel-webseite index.html swift upload beispiel-webseite meinewebseite/seite.html swift upload beispiel-webseite-meine-website/beispiel.png swift upload beispiel-webseite 404error.html . Betrachten der Website . Wenn alle oben genannten Schritte abgeschlossen sind, können wir nun unsere neu erstellte Website betrachten. Den Link zur Website finden Sie im Optimist Dashboard &gt; Object Store &gt; Containers über den abgebildeten Link. Wenn Sie auf den Link klicken, wird unsere neu erstellte Website angezeigt: . Klicken Sie auf “here”, um zu der Seite zu navigieren, auf der wir unser Beispielbild hochgeladen haben: . Für den Fall, dass wir versuchen, zu einer Seite zu navigieren, die nicht existiert, wird unsere benutzerdefinierte 404-Seite angezeigt: . ",
    "url": "/optimist/storage/s3_documentation/swiftservestaticwebsite/#beispiel-webseite",
    
    "relUrl": "/optimist/storage/s3_documentation/swiftservestaticwebsite/#beispiel-webseite"
  },"493": {
    "doc": "Swift - Serving a Static Website",
    "title": "Swift - Serving a Static Website",
    "content": " ",
    "url": "/optimist/storage/s3_documentation/swiftservestaticwebsite/",
    
    "relUrl": "/optimist/storage/s3_documentation/swiftservestaticwebsite/"
  },"494": {
    "doc": "Sichere Datensicherung mit Restic und Rclone",
    "title": "Problemstellung",
    "content": "Beim Durchführen von Sicherungen auf Dateiebene eines Nodes in S3 benötigt die Sicherungssoftware Schreibberechtigungen für den S3-Bucket. Verschafft sich ein Angreifer jedoch Zugriff auf die Maschine, kann er auch die Backups im Bucket zerstören, da die S3-Anmeldeinformationen auf dem kompromittierten System vorhanden sind. Die Lösung kann so einfach sein, wie die Zugriffsebene der Backup-Software auf den Bucket einzuschränken. Leider ist das bei S3 nicht trivial. ",
    "url": "/optimist/storage/s3_documentation/backupwithrestic/#problemstellung",
    
    "relUrl": "/optimist/storage/s3_documentation/backupwithrestic/#problemstellung"
  },"495": {
    "doc": "Sichere Datensicherung mit Restic und Rclone",
    "title": "Hintergrund",
    "content": "S3-Zugriffskontrolllisten (ACLs) ermöglichen Ihnen die Verwaltung des Zugriffs auf Buckets und Objekte, sind jedoch sehr eingeschränkt. Sie unterscheiden im Wesentlichen READ- und WRITE-Berechtigungen: . | READ – Ermöglicht dem Benutzer, die Objekte im Bucket aufzulisten | WRITE - Ermöglicht dem Benutzer, jedes Objekt im Bucket zu erstellen, zu überschreiben und zu löschen | . Die Einschränkungen von ACLs wurden durch die Zugriffsrichtlinienberechtigungen (ACP) behoben. Wir könnten dem Bucket eine No-Delete-Richtlinie anhängen, z.B. { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"nodelete1\", \"Effect\": \"Deny\", \"Action\": [ \"s3:DeleteBucket\", \"s3:DeleteBucketPolicy\", \"s3:DeleteBucketWebsite\", \"s3:DeleteObject\", \"s3:DeleteObjectVersion\" ], \"Resource\": [ \"arn:aws:s3:::*\" ] } ] } . Leider wurde das S3-Protokoll selbst nicht mit dem Konzept von WORM-Backups (Write Once Read Many) im Hinterkopf entwickelt. Zugriffsrichtlinienberechtigungen unterscheiden nicht zwischen dem Ändern eines vorhandenen Objekts (was ein effektives Löschen ermöglichen würde) und dem Erstellen eines neuen Objekts. Das Anhängen der obigen Richtlinie an einen Bucket verhindert nicht das Überschreiben der darin enthaltenen Objekte. $ s3cmd put testfile s3://appendonly-bucket/testfile upload: 'testfile' -&gt; 's3://appendonly-bucket/testfile' [1 of 1] 1054 of 1054 100% in 0s 5.46 KB/s done # die Richtlinie erlaubt Schreiben $ s3cmd rm s3://appendonly-bucket/testfile ERROR: S3 error: 403 (AccessDenied) # die Richtlinie verweigert das Löschen $ $ s3cmd put testfile s3://appendonly-bucket/testfile upload: 'testfile' -&gt; 's3://appendonly-bucket/testfile' [1 of 1] 1054 of 1054 100% in 0s 5.50 KB/s done # :( . ",
    "url": "/optimist/storage/s3_documentation/backupwithrestic/#hintergrund",
    
    "relUrl": "/optimist/storage/s3_documentation/backupwithrestic/#hintergrund"
  },"496": {
    "doc": "Sichere Datensicherung mit Restic und Rclone",
    "title": "Vorgeschlagene Lösung",
    "content": "Da ein Angreifer auf einem kompromittierten System immer Zugriff auf die S3-Anmeldeinformationen und jeden auf dem System laufenden Dienst hat – einschließlich Restic selbst, Proxys usw. – benötigen wir eine zweite, abgesicherte VM, die Löschvorgänge einschränken kann. Restic lässt sich perfekt in rclone integrieren, daher verwenden wir es in diesem Beispiel. Our environment . | appsrv: das zu sichernde System, mit Zugriff auf die Sicherungen | rclonesrv: das System, auf dem der rclone-Proxy ausgeführt wird (und nichts anderes, um die Angriffsfläche zu minimieren) | . Den rclone-Proxy einrichten . | rclone auf dem rclonesrv installieren: . sudo apt install rclone . | User für rclone anlegen . sudo useradd -m rcloneproxy sudo su - rcloneproxy . | Die Backend-Konfiguration für rclone backend erstellen . mkdir -p .config/rclone cat &lt;&lt; EOF &gt; .config/rclone/rclone.conf [s3-resticrepo] type = s3 provider = Other env_auth = false access_key_id = 111122223333444455556666 secret_access_key = aaaabbbbccccddddeeeeffffgggghhhh region = eu-central-1 endpoint = s3.es1.fra.optimist.gec.io acl = private bucket_acl = private upload_concurrency = 8 EOF . | Den Zugriff auf das Repository testen: . rclone lsd s3-resticrepo:databucket 0 2021-11-21 20:02:10 -1 data 0 2021-11-21 20:02:10 -1 index 0 2021-11-21 20:02:10 -1 keys 0 2021-11-21 20:02:10 -1 snapshots . | . Den Appserver konfigurieren . | Ein SSH-Schlüsselpaar auf appsrv mit dem Benutzer generieren, mit dem Sie das Backup durchführen: . ssh-keygen -o -a 256 -t ed25519 -C \"$(hostname)-$(date +'%d-%m-%Y')\" . | Die Umgebungsvariablen für restic setzen: . export RESTIC_PASSWORD=\"MyV3ryS3cUr3r3571cP4ssW0rd\" export RESTIC_REPOSITORY=rclone:s3-resticrepo:databucket . | . Den SSH-Schlüssel auf restic-only Befehle beschränken . Der letzte Schritt ist nun die SSH-Datei authorized_keys auf dem rclonesrv zu bearbeiten, um den neu generierten SSH-Schlüssel auf einen einzigen Befehl zu beschränken. Auf diese Weise kann ein Angreifer das SSH-Schlüsselpaar nicht verwenden, um beliebige Befehle auf dem rclone-Proxy auszuführen und die Backups zu kompromittieren. vi ~/.ssh/authorized_keys # Fügen Sie inen Eintrag mit dem öffentlichen Schlüssel des restic-Benutzers hinzu, der in dem obigen Schritt generiert wurde: command=\"rclone serve restic --stdio --append-only s3-resticrepo:databucket\" ssh-ed25519 AAAAC3fdsC1lZddsDNTE5ADsaDgfTwNtWmwiocdT9q4hxcss6tGDfgGTdiNN0z7zN appsrv-18-11-2021 . ",
    "url": "/optimist/storage/s3_documentation/backupwithrestic/#vorgeschlagene-l%C3%B6sung",
    
    "relUrl": "/optimist/storage/s3_documentation/backupwithrestic/#vorgeschlagene-lösung"
  },"497": {
    "doc": "Sichere Datensicherung mit Restic und Rclone",
    "title": "restic mit dem rclone Proxy verwenden",
    "content": "Wenn die Umgebungsvariablen gesetzt sind, sollte restic jetzt von dem appsrv aus funktionieren. Beispiel: Sicherung von /srv/myapp: . restic -o rclone.program=\"ssh rcloneproxy@rclonesrv.mydomain.com\" backup /srv/myapp . Snapshots auflisten: . restic -o rclone.program=\"ssh rcloneproxy@rclonesrv.mydomain.com\" snapshots . Snapshots löschen: . restic -o rclone.program=\"ssh rcloneproxy@rclonesrv.mydomain.com\" forget 2738e969 repository b71c391e opened successfully, password is correct Remove(&lt;snapshot/2738e9693b&gt;) returned error, retrying after 446.577749ms: blob not removed, server response: 403 Forbidden (403) . Ah, das geht nicht. Das war ja unser Ziel! . ",
    "url": "/optimist/storage/s3_documentation/backupwithrestic/#restic-mit-dem-rclone-proxy-verwenden",
    
    "relUrl": "/optimist/storage/s3_documentation/backupwithrestic/#restic-mit-dem-rclone-proxy-verwenden"
  },"498": {
    "doc": "Sichere Datensicherung mit Restic und Rclone",
    "title": "Zusammenfassung",
    "content": "Auf dieser Art und Weise, . | der rclone-Proxy auf dem rclonerv läuft nicht einmal als Dienst. Es wird nur bei Bedarf für die Dauer der Restic-Operation gestartet. Die Kommunikation erfolgt über HTTP2 über stdin/stdout in einem verschlüsselten SSH-Tunnel. | Da rclone mit --append-only läuft, ist es nicht möglich, Snapshots im S3-Bucket zu löschen (oder zu überschreiben). | Alle Daten (außer Zugangsdaten) werden lokal verschlüsselt/entschlüsselt und dann über rclonesrv an/von S3 gesendet/empfangen. | Alle Zugangsdaten werden nur auf rclonesrv gespeichert, um mit S3 zu kommunizieren. | . Da der Befehl in der SSH-Konfiguration für den SSH-Schlüssel des Benutzers fest eingetragen ist, gibt es keine Möglichkeit, den Schlüssel zu verwenden, um Zugriff auf den rclone-Proxy zu erhalten. ",
    "url": "/optimist/storage/s3_documentation/backupwithrestic/#zusammenfassung",
    
    "relUrl": "/optimist/storage/s3_documentation/backupwithrestic/#zusammenfassung"
  },"499": {
    "doc": "Sichere Datensicherung mit Restic und Rclone",
    "title": "Noch ein paar Gedanken",
    "content": "Die Vorteile der Lösung sind wahrscheinlich schon klar. Im Weiteren, . | die Verwaltung von Snapshots (sowohl manuell als auch mit einer Aufbewahrungsrichtlinie) ist nur noch auf dem rclone-Proxy möglich. | Eine einzelne rclone-Proxy-VM (oder sogar ein Docker-Container auf einer isolierten VM) kann mehrere Backup-Clients bedienen. | Es ist sehr empfohlen, für jeden Server, der Daten sichert, einen eigenen Schlüssel zu verwenden. | Wenn Sie mehr als ein Repository aus einem Node verwenden möchten, benötigen Sie dafür neue SSH-Schlüssel. Sie können dann mit -i ~/.ssh/id_ed25519_another_repo in den rclone.program-Argumenten genau wie bei SSH angeben, welcher Schlüssel verwendet werden soll. | . ",
    "url": "/optimist/storage/s3_documentation/backupwithrestic/#noch-ein-paar-gedanken",
    
    "relUrl": "/optimist/storage/s3_documentation/backupwithrestic/#noch-ein-paar-gedanken"
  },"500": {
    "doc": "Sichere Datensicherung mit Restic und Rclone",
    "title": "Sichere Datensicherung mit Restic und Rclone",
    "content": "Restic ist eine sehr einfache und leistungsstarke file-basierte Backup-Lösung, die schnell an Popularität gewinnt. Es kann in Kombination mit S3 verwendet werden, was es zu einem großartigen Werkzeug für Optimist macht. ",
    "url": "/optimist/storage/s3_documentation/backupwithrestic/",
    
    "relUrl": "/optimist/storage/s3_documentation/backupwithrestic/"
  },"501": {
    "doc": "Localstorage",
    "title": "Compute Localstorage für Ihre Instanzen",
    "content": " ",
    "url": "/optimist/storage/localstorage/#compute-localstorage-f%C3%BCr-ihre-instanzen",
    
    "relUrl": "/optimist/storage/localstorage/#compute-localstorage-für-ihre-instanzen"
  },"502": {
    "doc": "Localstorage",
    "title": "Was genau ist Compute Localstorage?",
    "content": "Beim Localstorage befindet sich der Storage Ihrer Instanzen direkt auf dem Hypervisor (Server). Die Localstorage Funktion ist über unsere l1 Flavors verfügbar und für Anwendungen vorgesehen, die eine geringe Latenz erfordern. ",
    "url": "/optimist/storage/localstorage/#was-genau-ist-compute-localstorage",
    
    "relUrl": "/optimist/storage/localstorage/#was-genau-ist-compute-localstorage"
  },"503": {
    "doc": "Localstorage",
    "title": "Datensicherheit und Verfügbarkeit",
    "content": "Da Ihre Daten direkt durch Ihre Instanz auf dem Storage des lokalen Hypervisors gebunden sind, empfiehlt es sich, diese Daten mithilfe eines HA Konzepts über die gegebenen Availability Zonen zu verteilen. Das Storage Backend der Localstorage Instanzen ist gegen einen Ausfall einzelner Speichermedien des Arrays geschützt, die dadurch hergestellte Redundanz besteht jedoch gegenüber der Ceph basierten Instanzen nur innerhalb des Hypervisor Nodes welcher die Instanz bereitstellt. Beim Ersetzen von Einzelkomponenten auf Grund eines Hardwaredefekts, kann es bis zur Wiederherstellung kurzfristig zu einer eingeschränkten Verfügbarkeit und Performance kommen. Die Hypervisor unterliegen einem definierten Patch Zyklus, bei dem die Hypervisoren nacheinander gebootet werden müssen. Durch den Localstorage der Instanzen können die Wartungsarbeiten nicht wie bei den auf Ceph Storage basierten Flavors unterbrechungsfrei durchgeführt werden. Aus diesem Grund existiert für l1 Flavors ein regelmäßiges Wartungsfenster. Dabei wird innerhalb einer Availability Zone und innerhalb des festgelegten Wartungsfensters ein Server nach dem anderen aktualisiert und rebootet. Innerhalb des Wartungsfensters werden laufende Instanzen von unserem System heruntergefahren und nach 10 Minuten gestoppt. ",
    "url": "/optimist/storage/localstorage/#datensicherheit-und-verf%C3%BCgbarkeit",
    
    "relUrl": "/optimist/storage/localstorage/#datensicherheit-und-verfügbarkeit"
  },"504": {
    "doc": "Localstorage",
    "title": "Standard Wartungsfenster",
    "content": "| Intervall | Tag | Zeit (in UTC) | . | wöchentlich | Mittwoch | 9:00 Uhr - 16:00 Uhr | . ",
    "url": "/optimist/storage/localstorage/#standard-wartungsfenster",
    
    "relUrl": "/optimist/storage/localstorage/#standard-wartungsfenster"
  },"505": {
    "doc": "Localstorage",
    "title": "Openstack Features",
    "content": "OpenStack bietet Ihnen viele Funktionen für Ihre Instanzen, wie z.B. resize, shelving oder snapshot. Wenn Sie für Ihre Instanzen l1 Flavors verwenden möchten, beachten Sie bitte folgendes: . Resize: Die Option Resize wird Ihnen angezeigt, aber technisch ist es nicht möglich, eine auf einem l1 Flavor basierende Instanz zu resizen. Sie können das umgehen, in dem Sie einen Cluster (Applikationsbezogen) mit l1 Flavors aufsetzen, größere l1 Flavors parallel starten und Ihre Daten von den alten l1 Flavors auf die neuen l1 Flavors rollen. Dies gilt auch bei einem wechsel von einem andren Flavor zu l1 Flavors. Shelving/Snapshotting: Beide Features sind möglich, aber aufgrund der Disk Size innerhalb der l1 Flavors raten wir wegen der langen Uploadzeiten davon ab. Hier empfiehlt es sich, die für die Applikation vorgesehene externe Backup-Lösung zu nutzen. ",
    "url": "/optimist/storage/localstorage/#openstack-features",
    
    "relUrl": "/optimist/storage/localstorage/#openstack-features"
  },"506": {
    "doc": "Localstorage",
    "title": "Localstorage",
    "content": " ",
    "url": "/optimist/storage/localstorage/",
    
    "relUrl": "/optimist/storage/localstorage/"
  },"507": {
    "doc": "FAQ",
    "title": "FAQ",
    "content": " ",
    "url": "/optimist/faq/",
    
    "relUrl": "/optimist/faq/"
  },"508": {
    "doc": "FAQ",
    "title": "Der Befehl openstack --help zeigt bei vielen Punkten “Could not load EntryPoint.parse” an.",
    "content": "In diesem Fall ist eine der mit dem OpenStack Client installierten Komponenten nicht aktuell. Um zu sehen, welche der Komponenten aktualisiert werden muss, rufen Sie den folgenden Befehl auf: . openstack --debug --help . Hier wird nun vor jedem Punkt angezeigt, was genau der Fehler ist und man kann einfach die jeweilige Komponente mit dem folgenden Befehl aktualisieren (&lt;PROJECT&gt; muss durch die richtige Komponente ersetzt werden): . pip install python-&lt;PROJECT&gt;client -U . ",
    "url": "/optimist/faq/#der-befehl-openstack---help-zeigt-bei-vielen-punkten-could-not-load-entrypointparse-an",
    
    "relUrl": "/optimist/faq/#der-befehl-openstack---help-zeigt-bei-vielen-punkten-could-not-load-entrypointparse-an"
  },"509": {
    "doc": "FAQ",
    "title": "Wie kann ich VRRP nutzen?",
    "content": "Um VRRP nutzen zu können, muss dies in einer Security-Group aktiviert und dann den jeweiligen Instanzen zugeordnet werden. Aktuell ist dies nur mit dem OpenStack Client möglich, zum Beispiel: . openstack security group rule create --remote-ip 10.0.0.0/24 --protocol vrrp --ethertype IPv4 --ingress default . ",
    "url": "/optimist/faq/#wie-kann-ich-vrrp-nutzen",
    
    "relUrl": "/optimist/faq/#wie-kann-ich-vrrp-nutzen"
  },"510": {
    "doc": "FAQ",
    "title": "Warum werden mir Floating IPs berechnet, die ich gar nicht benutze?",
    "content": "Der Grund dafür ist mit hoher Wahrscheinlichkeit, dass Floating IPs erstellt wurden, aber nach der Benutzung nicht korrekt gelöscht wurden. Um eine Übersicht über die aktuell verwendeten Floating IPs zu erhalten, können Sie das Horizon Dashboard nutzen. Dort befindet sich der entsprechende Punkt unter Project → Network → Floating-IPs. Alternativ können Sie den Weg über den OpenStack Client nutzen: . $ openstack floating ip list +--------------------------------------+---------------------+------------------+--------------------------------------+--------------------------------------+----------------------------------+ | ID | Floating IP Address | Fixed IP Address | Port | Floating Network | Project | +--------------------------------------+---------------------+------------------+--------------------------------------+--------------------------------------+----------------------------------+ | 84eca713-9ac1-42c3-baf6-860ba920a23c | 185.116.245.222 | 192.0.2.7 | a3097883-21cc-49fa-a060-bccc1678ece7 | 54258498-a513-47da-9369-1a644e4be692 | b15cde70d85749689e6568f973bb002 | +--------------------------------------+---------------------+------------------+--------------------------------------+--------------------------------------+----------------------------------+ . ",
    "url": "/optimist/faq/#warum-werden-mir-floating-ips-berechnet-die-ich-gar-nicht-benutze",
    
    "relUrl": "/optimist/faq/#warum-werden-mir-floating-ips-berechnet-die-ich-gar-nicht-benutze"
  },"511": {
    "doc": "FAQ",
    "title": "Wie kann ich den Flavor einer Instanz ändern (instance resize)?",
    "content": "Resizing über die Command Line . Geben Sie den Namen oder die UUID des Servers an, dessen Größe Sie ändern möchten, und ändern Sie die Größe mit dem Befehl openstack server resize. Geben Sie das gewünschte neue Flavor und dann den Instanznamen oder die UUID an: . openstack server resize --flavor FLAVOR SERVER . Die Größenänderung kann einige Zeit in Anspruch nehmen. Während dieser Zeit wird der Instanzstatus als RESIZE angezeigt. Wenn die Resizing abgeschlossen ist, wird der Instanzstatus VERIFY_RESIZE angezeigt. Sie können nun die Größenänderung bestätigen, um den Status auf ACTIVE zu ändern: . openstack server resize --confirm SERVER . Warnung . Die Größenänderung wird nach einer Stunde automatisch bestätigt, falls sie vorher nicht manuell bestätigt oder rückgängig gemacht wurde. Resizing über das Optimist-Dashboard . Gehen Sie zu Optimist Dashboard → Instances und navigieren Sie zu der Instanz, deren Größe geändert werden soll. Wählen Sie anschließend Actions → Resize Flavor. Der aktuelle Flavor wird angezeigt. Verwenden Sie die Dropdown-Liste “Select a new flavor”, wählen Sie den neuen Flavor aus und bestätigen Sie mit “Resize”. Warnung . Dies gilt nicht für l1 (localstorage) Flavors. Weitere Informationen finden Sie unter Storage → Localstorage. ",
    "url": "/optimist/faq/#wie-kann-ich-den-flavor-einer-instanz-%C3%A4ndern-instance-resize",
    
    "relUrl": "/optimist/faq/#wie-kann-ich-den-flavor-einer-instanz-ändern-instance-resize"
  },"512": {
    "doc": "FAQ",
    "title": "Warum ist das Logfile der Compute Instanz im Optimist Dashboard leer?",
    "content": "Bedingt durch Wartungsarbeiten oder einem Umverteilen der Last im OpenStack wurde die Instanz verschoben. In diesem Fall wird das Logfile neu angelegt und neue Meldungen werden wie gewohnt protokolliert. ",
    "url": "/optimist/faq/#warum-ist-das-logfile-der-compute-instanz-im-optimist-dashboard-leer",
    
    "relUrl": "/optimist/faq/#warum-ist-das-logfile-der-compute-instanz-im-optimist-dashboard-leer"
  },"513": {
    "doc": "FAQ",
    "title": "Warum erhalte ich den Fehler “Conflict (HTTP 409)” beim Erstellen eines Swift Containers?",
    "content": "Swift verwendet einzigartige Namen über die gesamte OpenStack Umgebung hinweg. Die Fehlermeldung besagt, dass der gewählte Name bereits in Verwendung ist. ",
    "url": "/optimist/faq/#warum-erhalte-ich-den-fehler-conflict-http-409-beim-erstellen-eines-swift-containers",
    
    "relUrl": "/optimist/faq/#warum-erhalte-ich-den-fehler-conflict-http-409-beim-erstellen-eines-swift-containers"
  },"514": {
    "doc": "FAQ",
    "title": "Anbringen von Cinder-Volumes an Instanzen per UUID",
    "content": "Wenn Sie mehrere Cinder-Volumes an eine Instanz anhängen, werden die Mount-Punkte möglicherweise bei jedem Neustart neu gemischt. Durch das Mounten der Volumes mittels der UUID wird sichergestellt, dass die richtigen Volumes wieder an die richtigen Mount-Punkte angehängt werden, falls für die Instanz ein Aus- und Wiedereinschalten erforderlich ist. Nachdem Sie die UUID des Volumes mit blkid in der Instanz abgerufen haben, ändern Sie den Mountpunkt in /etc/fstab wie folgt, um die UUID zu verwenden. Beispiel: . # /boot war auf /dev/sda2 während der Installation /dev/disk/by-uuid/f6a0d6f3-b66c-bbe3-47ba-d264464cb5a2 /boot ext4 defaults 0 2 . ",
    "url": "/optimist/faq/#anbringen-von-cinder-volumes-an-instanzen-per-uuid",
    
    "relUrl": "/optimist/faq/#anbringen-von-cinder-volumes-an-instanzen-per-uuid"
  },"515": {
    "doc": "FAQ",
    "title": "Ist die Nutzung von Cinder multi-attached Volumes möglich?",
    "content": "Wir unterstützen keine multi-attached Volumes in der Optimist Platform, da für die Nutzung von multi-attached Volumes clusterfähige Dateisysteme erforderlich sind, um den gleichzeitigen Zugriff auf das Dateisystem zu koordinieren. Versuche, multi-attached Volumes ohne clusterfähige Dateisysteme zu verwenden, bergen ein hohes Risiko für Datenkorruption, daher ist diese Funktion auf der Optimist Plattform nicht aktiviert. ",
    "url": "/optimist/faq/#ist-die-nutzung-von-cinder-multi-attached-volumes-m%C3%B6glich",
    
    "relUrl": "/optimist/faq/#ist-die-nutzung-von-cinder-multi-attached-volumes-möglich"
  },"516": {
    "doc": "FAQ",
    "title": "Warum kann ich keinen Snapshot einer laufenden Instance erstellen?",
    "content": "Um konsistente Snapshots zu erstellen, verwendet die Optimist-Plattform das Property os_require_quiesce=yes. Diese Eigenschaft ermöglicht die Nutzung von fsfreeze, um den Zugriff auf laufende Instanzen auszusetzen, um einen konsistenten Snapshot der Instanz zu erstellen. Zur Erstellung von Snapshots von Instanzen stehen in der Optimist Platform die folgenden Optionen zur Verfügung: . Die erste Option besteht darin, einen Snapshot der laufenden Instanz zu erstellen, indem der qemu-guest-agent installiert und ausgeführt wird. Dieser kann wie folgt installiert und ausgeführt werden: . apt install qemu-guest-agent systemctl start qemu-guest-agent systemctl enable qemu-guest-agent . Außerdem empfehlen wir beim Hochladen Ihrer eigenen Images, dass Sie --property hw_qemu_guest_agent=True als Property an Ihren Images hinzufügen. Sobald der qemu-guest-agent läuft, kann der Snapshot erstellt werden. Die zweite Möglichkeit besteht darin, die laufende Instanz zu stoppen, den Snapshot zu erstellen und die Instanz schließlich erneut zu starten. Dies kann über das Horizon Dashboard oder auf der CLI wie folgt erfolgen: . openstack server stop ExampleInstance openstack server image create --name ExampleInstanceSnapshot ExampleInstance openstack server start ExampleInstance . ",
    "url": "/optimist/faq/#warum-kann-ich-keinen-snapshot-einer-laufenden-instance-erstellen",
    
    "relUrl": "/optimist/faq/#warum-kann-ich-keinen-snapshot-einer-laufenden-instance-erstellen"
  },"517": {
    "doc": "Spezifikationen",
    "title": "Spezifikationen",
    "content": " ",
    "url": "/optimist/specs/",
    
    "relUrl": "/optimist/specs/"
  },"518": {
    "doc": "Flavor Spezifikationen",
    "title": "Flavor Spezifikationen",
    "content": "“Flavor” bezeichnet im OpenStack-Kontext ein Hardware-Profil, das eine virtuelle Maschine nutzt bzw. nutzen kann. Im Optimist sind diverse Standard-Hardwareprofile (Flavors) eingerichtet. Diese haben unterschiedliche Limits und Begrenzungen, welche hier für alle verfügbaren Flavors aufgelistet sind. ",
    "url": "/optimist/specs/flavor_specification/",
    
    "relUrl": "/optimist/specs/flavor_specification/"
  },"519": {
    "doc": "Flavor Spezifikationen",
    "title": "Migration zwischen Flavor-Typen",
    "content": "Um die Flavors bestehender Instanzen zu ändern, kann die OpenStack-Option „Resize Instance“ entweder über das Dashboard oder die CLI verwendet werden. Dies führt zu einem Neustart der Instanz, aber der Inhalt der Instanz bleibt erhalten. Bitte beachten Sie, dass ein Wechsel der Flavors von den großen Root-Disk-Typen zu einem Flavor mit einer kleineren Root-Disk nicht möglich ist. Warnung . Dies gilt nicht für l1 (localstorage) Flavors. Weitere Informationen finden Sie unter Storage → Localstorage. ",
    "url": "/optimist/specs/flavor_specification/#migration-zwischen-flavor-typen",
    
    "relUrl": "/optimist/specs/flavor_specification/#migration-zwischen-flavor-typen"
  },"520": {
    "doc": "Flavor Spezifikationen",
    "title": "Deprecated Flavor-Typen",
    "content": "Die folgenden Flavor-Typen gelten derzeit als veraltet und für die nahe Zukunft ist geplant, diese Flavor-Familien zu entfernen. Wir werden regelmäßig überprüfen, ob diese Flavors noch verwendet werden. Falls dies nicht der Fall ist, werden wir sie auf privat setzen, um zu vermeiden, dass neue Instanzen damit erstellt werden: . | m1-Familie (deprecated) | e1-Familie (e = equal) (deprecated) | r1-Familie (r = ram) (deprecated) | Memory-Flavors (deprecated) | Windows-Flavors (deprecated) | . ",
    "url": "/optimist/specs/flavor_specification/#deprecated-flavor-typen",
    
    "relUrl": "/optimist/specs/flavor_specification/#deprecated-flavor-typen"
  },"521": {
    "doc": "Flavor Spezifikationen",
    "title": "Flavor-Typen",
    "content": "Standard-Flavors . | Bezeichnung | Kerne | RAM | Disk | IOPS Limits (read/write) | IO throughput rate (read/write) | Network Bandwidth | . | s1.micro | 1 | 2 GB | 20 GB | 1000 / 1000 | 200 MB/s / 200 MB/s | 1 Gbit/s | . | s1.small | 2 | 4 GB | 20 GB | 1000 / 1000 | 200 MB/s / 200 MB/s | 2 Gbit/s | . | s1.medium | 4 | 8 GB | 20 GB | 1000 / 1000 | 200 MB/s / 200 MB/s | 3 Gbit/s | . | s1.large | 8 | 16 GB | 20 GB | 1000 / 1000 | 200 MB/s / 200 MB/s | 4 Gbit/s | . | s1.xlarge | 16 | 32 GB | 20 GB | 1000 / 1000 | 200 MB/s / 200 MB/s | 4 Gbit/s | . | s1.2xlarge | 30 | 64 GB | 20 GB | 1000 / 1000 | 200 MB/s / 200 MB/s | 4 Gbit/s | . Standard CPU Large Disk Flavors . | Bezeichnung | Kerne | RAM | Disk | IOPS Limits (read/write) | IO throughput rate (read/write) | Network Bandwidth | . | s1.micro.d | 1 | 2 GB | 100 GB | 2500 / 2500 | 250 MB/s / 250 MB/s | 1 Gbit/s | . | s1.small.d | 2 | 4 GB | 100 GB | 2500 / 2500 | 250 MB/s / 250 MB/s | 2 Gbit/s | . | s1.medium.d | 4 | 8 GB | 100 GB | 2500 / 2500 | 250 MB/s / 250 MB/s | 3 Gbit/s | . | s1.large.d | 8 | 16 GB | 100 GB | 2500 / 2500 | 250 MB/s / 250 MB/s | 4 Gbit/s | . | s1.xlarge.d | 16 | 32 GB | 100 GB | 2500 / 2500 | 250 MB/s / 250 MB/s | 4 Gbit/s | . | s1.2xlarge.d | 30 | 64 GB | 100 GB | 2500 / 2500 | 250 MB/s / 250 MB/s | 4 Gbit/s | . Dedicated CPU Flavors . | Bezeichnung | Kerne | RAM | Disk | IOPS Limits (read/write) | IO throughput rate (read/write) | Network Bandwidth | . | d1.micro | 1 | 8 GB | 20 GB | 1000 / 1000 | 200 MB/s / 200 MB/s | 1 Gbit/s | . | d1.small | 2 | 16 GB | 20 GB | 1000 / 1000 | 200 MB/s / 200 MB/s | 2 Gbit/s | . | d1.medium | 4 | 32 GB | 20 GB | 1000 / 1000 | 200 MB/s / 200 MB/s | 3 Gbit/s | . | d1.large | 8 | 64 GB | 20 GB | 1000 / 1000 | 200 MB/s / 200 MB/s | 4 Gbit/s | . | d1.xlarge | 16 | 128 GB | 20 GB | 1000 / 1000 | 200 MB/s / 200 MB/s | 4 Gbit/s | . | d1.2xlarge | 30 | 256 GB | 20 GB | 1000 / 1000 | 200 MB/s / 200 MB/s | 4 Gbit/s | . Dedicated CPU Large Disk Flavors . | Bezeichnung | Kerne | RAM | Disk | IOPS Limits (read/write) | IO throughput rate (read/write) | Network Bandwidth | . | d1.micro.d | 1 | 8 GB | 100 GB | 2500 / 2500 | 250 MB/s / 250 MB/s | 1 Gbit/s | . | d1.small.d | 2 | 16 GB | 100 GB | 2500 / 2500 | 250 MB/s / 250 MB/s | 2 Gbit/s | . | d1.medium.d | 4 | 32 GB | 100 GB | 2500 / 2500 | 250 MB/s / 250 MB/s | 3 Gbit/s | . | d1.large.d | 8 | 64 GB | 100 GB | 2500 / 2500 | 250 MB/s / 250 MB/s | 4 Gbit/s | . | d1.xlarge.d | 16 | 128 GB | 100 GB | 2500 / 2500 | 250 MB/s / 250 MB/s | 4 Gbit/s | . | d1.2xlarge.d | 30 | 256 GB | 100 GB | 2500 / 2500 | 250 MB/s / 250 MB/s | 4 Gbit/s | . Localstorage Flavors . | Bezeichnung | Kerne | RAM | Disk | IOPS Limits (read/write) | IO throughput rate (read/write) | Network Bandwidth | . | l1.micro | 1 | 8 GB | 300 GB | 25000 / 10000 | 125 MB/s / 60 MB/s | 1 Gbit/s | . | l1.small | 2 | 16 GB | 600 GB | 50000 / 25000 | 250 MB/s / 125 MB/s | 2 Gbit/s | . | l1.medium | 4 | 32 GB | 1200 GB | 100000 / 50000 | 500 MB/s / 250 MB/s | 3 Gbit/s | . | l1.large | 8 | 64 GB | 2500 GB | 100000 / 100000 | 1000 MB/s / 500 MB/s | 4 Gbit/s | . | l1.xlarge | 16 | 128 GB | 5000 GB | 100000 / 100000 | 2000 MB/s / 1125 MB/s | 4 Gbit/s | . | l1.2xlarge | 30 | 256 GB | 10000 GB | 100000 / 100000 | 2000 MB/s / 2000 MB/s | 4 Gbit/s | . m1-Familie (deprecated) . | Bezeichnung | Kerne | RAM | Disk | IOPS Limits (read/write) | IO throughput rate (read/write) | Network Bandwidth | . | m1.micro | 1 | 1 GB | 20 GB | 1000 / 1000 | 200 MB/s / 200 MB/s | 1 Gbit/s | . | m1.small | 2 | 4 GB | 20 GB | 1000 / 1000 | 200 MB/s / 200 MB/s | 2 Gbit/s | . | m1.medium | 4 | 8 GB | 20 GB | 1000 / 1000 | 200 MB/s / 200 MB/s | 3 Gbit/s | . | m1.large | 8 | 16 GB | 20 GB | 1000 / 1000 | 200 MB/s / 200 MB/s | 4 Gbit/s | . | m1.xlarge | 16 | 32 GB | 20 GB | 1000 / 1000 | 200 MB/s / 200 MB/s | 4 Gbit/s | . | m1.xxlarge | 30 | 64 GB | 20 GB | 1000 / 1000 | 200 MB/s / 200 MB/s | 4 Gbit/s | . e1-Familie (e = equal) (deprecated) . | Bezeichnung | Kerne | RAM | Disk | IOPS Limits (read/write) | IO throughput rate (read/write) | Network Bandwidth | . | e1.small | 2 | 2 GB | 20 GB | 1000 / 1000 | 200 MB/s / 200 MB/s | 1 Gbit/s | . | e1.medium | 4 | 4 GB | 20 GB | 1000 / 1000 | 200 MB/s / 200 MB/s | 2 Gbit/s | . | e1.large | 8 | 8 GB | 20 GB | 1000 / 1000 | 200 MB/s / 200 MB/s | 3 Gbit/s | . | e1.xlarge | 16 | 16 GB | 20 GB | 1000 / 1000 | 200 MB/s / 200 MB/s | 4 Gbit/s | . | e1.xxlarge | 30 | 32 GB | 20 GB | 1000 / 1000 | 200 MB/s / 200 MB/s | 4 Gbit/s | . r1-Familie (r = ram) (deprecated) . | Bezeichnung | Kerne | RAM | Disk | IOPS Limits (read/write) | IO throughput rate (read/write) | Network Bandwidth | . | r1.small | 2 | 6 GB | 20 GB | 1000 / 1000 | 200 MB/s / 200 MB/s | 1 Gbit/s | . | r1.medium | 4 | 12 GB | 20 GB | 1000 / 1000 | 200 MB/s / 200 MB/s | 2 Gbit/s | . | r1.large | 8 | 32 GB | 20 GB | 1000 / 1000 | 200 MB/s / 200 MB/s | 3 Gbit/s | . | r1.xlarge | 16 | 48 GB | 20 GB | 1000 / 1000 | 200 MB/s / 200 MB/s | 4 Gbit/s | . Memory-flavors (deprecated) . | Bezeichnung | Kerne | RAM | Disk | IOPS Limits (read/write) | IO throughput rate (read/write) | Network Bandwidth | . | mem.micro | 4 | 16 GB | 20 GB | 1000 / 1000 | 200 MB/s / 200 MB/s | 1 Gbit/s | . | mem.small | 8 | 32 GB | 20 GB | 1000 / 1000 | 200 MB/s / 200 MB/s | 2 Gbit/s | . | mem.medium | 8 | 64 GB | 20 GB | 1000 / 1000 | 200 MB/s / 200 MB/s | 3 Gbit/s | . Windows-flavors (deprecated) . | Bezeichnung | Kerne | RAM | Disk | IOPS Limits (read/write) | IO throughput rate (read/write) | Network Bandwidth | . | win.micro | 1 | 2 GB | 80 GB | 1000 / 1000 | 200 MB/s / 200 MB/s | 1 Gbit/s | . | win.small | 2 | 8 GB | 80 GB | 1000 / 1000 | 200 MB/s / 200 MB/s | 2 Gbit/s | . | win.medium | 4 | 16 GB | 80 GB | 1000 / 1000 | 200 MB/s / 200 MB/s | 3 Gbit/s | . | win.large | 8 | 32 GB | 80 GB | 1000 / 1000 | 200 MB/s / 200 MB/s | 4 Gbit/s | . | win.xlarge | 16 | 64 GB | 80 GB | 1000 / 1000 | 200 MB/s / 200 MB/s | 4 Gbit/s | . ",
    "url": "/optimist/specs/flavor_specification/#flavor-typen",
    
    "relUrl": "/optimist/specs/flavor_specification/#flavor-typen"
  },"522": {
    "doc": "Default Quotas",
    "title": "OpenStack Default Quotas",
    "content": "Im Optimist haben wir Standardwerte für den OpenStack Compute-Dienst, den OpenStack Block Storage-Dienst und den OpenStack Networking-Dienst definiert. Wir haben auch separate Quotas für den Octavia Loadbalancer-Dienst und die zugehörigen Komponenten. Diese Standardwerte sind unten aufgeführt. ",
    "url": "/optimist/specs/default_quota/#openstack-default-quotas",
    
    "relUrl": "/optimist/specs/default_quota/#openstack-default-quotas"
  },"523": {
    "doc": "Default Quotas",
    "title": "Compute Standardwerte",
    "content": "| Ressource | Wert | . | Cores | 256 | . | Fixed IPs | Unlimited | . | Floating IPs | 15 | . | Injected File Size | 10240 (MB) | . | Injected Files | 100 | . | Instances | 100 | . | Key Pairs | 100 | . | Properties | 128 | . | Ram | 524288 (MB) | . | Server Groups | 10 | . | Server Group Members | 10 | . ",
    "url": "/optimist/specs/default_quota/#compute-standardwerte",
    
    "relUrl": "/optimist/specs/default_quota/#compute-standardwerte"
  },"524": {
    "doc": "Default Quotas",
    "title": "Block Storage Standardwerte",
    "content": "| Ressource | Wert | . | Backups | 100 | . | Backup Gigabytes | 10000 (MB) | . | Gigabytes | 10000 (MB) | . | Per-volume-gigabytes | Unlimited | . | Snapshots | 100 | . | Volumes | 100 | . ",
    "url": "/optimist/specs/default_quota/#block-storage-standardwerte",
    
    "relUrl": "/optimist/specs/default_quota/#block-storage-standardwerte"
  },"525": {
    "doc": "Default Quotas",
    "title": "Network Standardwerte",
    "content": "| Ressource | Wert | . | Floating IPs | 15 | . | Secgroup Rules | 1000 | . | Secgroups | 100 | . | Networks | 100 | . | Subnets | 200 | . | Ports | 500 | . | Routers | 50 | . | RBAC Policies | 100 | . | Subnetpools | Unlimited | . ",
    "url": "/optimist/specs/default_quota/#network-standardwerte",
    
    "relUrl": "/optimist/specs/default_quota/#network-standardwerte"
  },"526": {
    "doc": "Default Quotas",
    "title": "Octavia Loadbalancers",
    "content": "| Ressource | Wert | . | Load Balancers | 100 | . | Listeners | 100 | . | Pools | 100 | . | Health Monitors | 100 | . | Members | 100 | . ",
    "url": "/optimist/specs/default_quota/#octavia-loadbalancers",
    
    "relUrl": "/optimist/specs/default_quota/#octavia-loadbalancers"
  },"527": {
    "doc": "Default Quotas",
    "title": "Default Quotas",
    "content": " ",
    "url": "/optimist/specs/default_quota/",
    
    "relUrl": "/optimist/specs/default_quota/"
  },"528": {
    "doc": "Application Credentials",
    "title": "Einführung",
    "content": "Benutzer können Application Credentials erstellen, damit sich ihre Anwendungen bei der OpenStack-Komponente Keystone authentifizieren können, ohne ihre eigenen Anmeldeinformationen des Benutzers verwenden zu müssen. Mit Application Credentials können sich Anwendungen mit der Application Credential-ID und einer geheimen Zeichenfolge authentifizieren, die nicht das Kennwort des Benutzers ist. Auf diese Weise wird das Passwort des Benutzers nicht in die Konfiguration der Anwendung eingebettet. Benutzer können eine Teilmenge ihrer Rollenzuweisungen für ein Projekt an Application Credentials delegieren und der Anwendung dieselben oder eingeschränkte Berechtigungen innerhalb eines Projekts erteilen. ",
    "url": "/optimist/specs/application_credentials/#einf%C3%BChrung",
    
    "relUrl": "/optimist/specs/application_credentials/#einführung"
  },"529": {
    "doc": "Application Credentials",
    "title": "Anforderungen für Application Credentials",
    "content": "Name / Secrets . Application Credentials für Ihr Projekt können über die Befehlszeile oder über das Dashboard generiert werden. Diese werden dem Projekt zugeordnet, in dem sie erstellt werden. Der einzige erforderliche Parameter zum Erstellen der Anmeldeinformationen ist ein Name, jedoch kann mit dem Parameter —-secret ein bestimmtes Secret festgelegt werden. Ohne Parameter wird stattdessen automatisch ein Secret in der Ausgabe generiert. Es ist in jedem Fall wichtig das Secret zu notieren, da dieses vor dem Speichern gehasht wird und nach dem Festlegen nicht wiederhergestellt werden kann. Wenn das Secret verloren geht, müssen neue Application Credential für die Anwendung erstellt werden. Roles . Wir empfehlen außerdem, die Roles festzulegen, die die Application Credentials der Anwendung im Projekt haben sollen, da standardmäßig ein neu erstellter Satz von Anmeldeinformationen alle verfügbaren Roles erbt. Im Folgenden sind die verfügbaren Roles aufgeführt, die einem Satz von Application Credentials zugewiesen werden können. Wenn Sie diese Roles mithilfe des Parameters --role auf einen Satz von Application Credentials anwenden, beachten Sie bitte, dass bei allen Role-Namen die Groß-/Kleinschreibung beachtet wird: . | Member: Die Rolle “Member” hat nur administrativen Zugriff auf das zugewiesene Projekt. | heat_stack_owner: Als “heat_stack_owner” können Sie vorhandene HEAT-Templates verwenden und ausführen. | load-balancer_member: Als „load-balancer_member“ können Sie die Octavia LoadBalancer-Ressourcen nutzen. | . Expiration . Standardmäßig laufen erstellte Application Credentials nicht ab, jedoch können feste Ablaufdaten/-zeiten für Application Credentials bei der Erstellung festgelegt werden, indem der Parameter --expires im Befehl verwendet wird (zum Beispiel: --expires '2021-07-15T21: 00:00'). ",
    "url": "/optimist/specs/application_credentials/#anforderungen-f%C3%BCr-application-credentials",
    
    "relUrl": "/optimist/specs/application_credentials/#anforderungen-für-application-credentials"
  },"530": {
    "doc": "Application Credentials",
    "title": "Erstellen von Application Credentials über die CLI",
    "content": "Ein Set von Application Credentials kann im gewünschten Projekt über die CLI erstellt werden. Das folgende Beispiel zeigt, wie ein Set von Credentials mit den folgenden Parametern erstellt wird: . | Name: test-credentials | Secret: ZYQZm2k6pk | Roles: Member, heat_stack_owner, load-balancer_member | Expiration Date/Time: 2021-07-12 at 21:00:00 | . Die neuen Zugangsdaten sollten wie folgt aussehen: . $ openstack application credential create test-credentials --secret ZYQZm2k6pk --role Member --role heat_stack_owner --role load-balancer_member --expires '2021-07-15T21:00:00' +--------------+----------------------------------------------+ | Field | Value | +--------------+----------------------------------------------+ | description | None | expires_at | 2021-07-15T21:00:00.000000 | id | 707d14e835124b4f957938bb5a57d1be | name | test-credentials | project_id | c704ac5a32b84b54a0407d28ad448399 | roles | Member heat_stack_owner load-balancer_member | secret | ZYQZm2k6pk | system | None | unrestricted | False | user_id | 1d9f1ecb5de3607e8982695f72036fa5 | +--------------+----------------------------------------------+ . Hinweis: Das Secret (ob vom Benutzer festgelegt oder automatisch generiert) wird nur beim Erstellen der Application Credentials angezeigt. Bitte notieren Sie sich das Secret zu diesem Zeitpunkt. ",
    "url": "/optimist/specs/application_credentials/#erstellen-von-application-credentials-%C3%BCber-die-cli",
    
    "relUrl": "/optimist/specs/application_credentials/#erstellen-von-application-credentials-über-die-cli"
  },"531": {
    "doc": "Application Credentials",
    "title": "Anzeigen von Application Credentials über die CLI",
    "content": "Die Liste der zu einem Projekt gehörenden Application-Credentials kann mit dem folgenden Befehl aufgelistet werden. $ openstack application credential list +----------------------------------+-------------------+----------------------------------+-------------+------------+ | ID | Name | Project ID | Description | Expires At | +----------------------------------+-------------------+----------------------------------+-------------+------------+ | 707d14e835124b4f957938bb5a57d1be | test-credentials | c704ac5a32b84b54a0407d28ad448399 | None | None | +----------------------------------+-------------------+----------------------------------+-------------+------------+ . Einzelne Credentials können mit dem $ openstack application credential show &lt;name&gt; Befehl angezeigt werden. ",
    "url": "/optimist/specs/application_credentials/#anzeigen-von-application-credentials-%C3%BCber-die-cli",
    
    "relUrl": "/optimist/specs/application_credentials/#anzeigen-von-application-credentials-über-die-cli"
  },"532": {
    "doc": "Application Credentials",
    "title": "Löschen von Application Credentials über die CLI",
    "content": "Application Credentials können über die CLI mit dem folgenden Befehl mit dem Namen oder der ID des spezifischen Satzes von Anmeldeinformationen gelöscht werden: . openstack application credential delete test-credentials . ",
    "url": "/optimist/specs/application_credentials/#l%C3%B6schen-von-application-credentials-%C3%BCber-die-cli",
    
    "relUrl": "/optimist/specs/application_credentials/#löschen-von-application-credentials-über-die-cli"
  },"533": {
    "doc": "Application Credentials",
    "title": "Erstellen und Löschen von Application Credentials für Anwendungen über das Optimist Dashboard",
    "content": "Alternativ können Application Credentials auch über das Optimist Dashboard unter Identität &gt; Application Credentials generiert werden: . Hinweis: Hier können mehrere Rollen ausgewählt werden, indem Sie die Umschalttaste gedrückt halten und durch die Optionen navigieren. Nach der Erstellung wird ein Dialogfeld angezeigt, in dem Sie aufgefordert werden, die ID und das Secret zu notieren. Wenn Sie fertig sind, klicken Sie auf “Close”. Die Zugangsdaten hier können jederzeit gelöscht werden, indem Sie den zu löschenden Zugangsdatensatz zu markieren und dann auf “DELETE APPLICATION CREDENTIAL” klicken. ",
    "url": "/optimist/specs/application_credentials/#erstellen-und-l%C3%B6schen-von-application-credentials-f%C3%BCr-anwendungen-%C3%BCber-das-optimist-dashboard",
    
    "relUrl": "/optimist/specs/application_credentials/#erstellen-und-löschen-von-application-credentials-für-anwendungen-über-das-optimist-dashboard"
  },"534": {
    "doc": "Application Credentials",
    "title": "Application Credentials testen",
    "content": "Sobald wir über die CLI oder das Dashboard einen Satz von Application Credentials erstellt haben, können wir sie mit dem folgenden curl-Befehl testen, um zu überprüfen, ob sie funktionieren. Wir müssen unsere &lt;name&gt; und &lt;secret&gt; im curl-Befehl verwenden: . curl -i -H \"Content-Type: application/json\" -d ' { \"auth\": { \"identity\": { \"methods\": [\"application_credential\"], \"application_credential\": { \"id\": “&lt;id&gt;\", \"secret\": “&lt;secret&gt;\"}}}}' https://identity.optimist.gec.io/v3/auth/tokens . Ein erfolgreicher curl-Versuch gibt ein x-subject-token aus, erfolglose Versuche mit falschen Anmeldeinformationen führen zu einem 401-Fehler. ",
    "url": "/optimist/specs/application_credentials/#application-credentials-testen",
    
    "relUrl": "/optimist/specs/application_credentials/#application-credentials-testen"
  },"535": {
    "doc": "Application Credentials",
    "title": "Application Credentials",
    "content": " ",
    "url": "/optimist/specs/application_credentials/",
    
    "relUrl": "/optimist/specs/application_credentials/"
  },"536": {
    "doc": "Volumenspezifikationen",
    "title": "Volumenspezifikationen",
    "content": "In OpenStack sind “Volumes” persistente Speicher, die Sie an Ihre laufenden OpenStack Compute-Instanzen anhängen können. In Optimist haben wir drei Serviceklassen für Volumes eingerichtet. Diese haben unterschiedliche Limits, die unten aufgeführt sind. ",
    "url": "/optimist/specs/volume_specification/",
    
    "relUrl": "/optimist/specs/volume_specification/"
  },"537": {
    "doc": "Volumenspezifikationen",
    "title": "Volume-Typen",
    "content": "Wir haben drei Hauptvolumentypen: . | high-iops | default | low-iops | . ",
    "url": "/optimist/specs/volume_specification/#volume-typen",
    
    "relUrl": "/optimist/specs/volume_specification/#volume-typen"
  },"538": {
    "doc": "Volumenspezifikationen",
    "title": "Volumen-Typen-Liste",
    "content": "Nachfolgend eine Übersicht der drei Volume-Typen: . | Name | Read Bytes Sec | Read IOPS Sec | Write Bytes Sec | Write IOPS Sec | . | high-iops | 524288000 | 10000 | 524288000 | 10000 | . | default | 209715200 | 2500 | 209715200 | 2500 | . | low-iops | 52428800 | 300 | 52428800 | 300 | . ",
    "url": "/optimist/specs/volume_specification/#volumen-typen-liste",
    
    "relUrl": "/optimist/specs/volume_specification/#volumen-typen-liste"
  },"539": {
    "doc": "Volumenspezifikationen",
    "title": "Auswählen eines Volume-Typs",
    "content": "Sie können beim Erstellen eines Volumes mit dem folgenden Befehl einen der drei Volume-Typen auswählen (Wenn nicht anders angegeben, wird immer der Typ „Standard“ verwendet): $ openstack volume create &lt;volume-name&gt; --size 10 --type high-iops . ",
    "url": "/optimist/specs/volume_specification/#ausw%C3%A4hlen-eines-volume-typs",
    
    "relUrl": "/optimist/specs/volume_specification/#auswählen-eines-volume-typs"
  },"540": {
    "doc": "Images",
    "title": "Images",
    "content": "Es gibt 4 Arten von Images in OpenStack: . | Public Images: Diese Images werden von uns gepflegt, sind für alle Benutzer verfügbar, werden regelmäßig aktualisiert und zur Verwendung empfohlen. | Community Images: Ehemals öffentliche Images, die durch neuere Versionen ersetzt wurden. Wir behalten diese Images, bis sie nicht mehr verwendet werden, um Ihre Deployments nicht zu gefährden. | Private Images: Von Ihnen hochgeladene Images, die nur für Ihr Projekt verfügbar sind. | Shared Images: Private Images, die entweder durch Sie oder oder mit Ihnen in mehreren Projekten gemeinsam genutzt werden. | . Nur die ersten beiden Typen werden von uns verwaltet. ",
    "url": "/optimist/specs/images/",
    
    "relUrl": "/optimist/specs/images/"
  },"541": {
    "doc": "Images",
    "title": "Public und community images",
    "content": "Um Ihren Aufwand so gering wie möglich zu halten, stellen wir Ihnen eine Reihe ausgewählter Images zur Verfügung. Aktuell enthält diese Liste: . | Ubuntu 24.04 LTS (Noble Numbat) | Ubuntu 22.04 LTS (Jammy Jellyfish) | Ubuntu 20.04 LTS (Focal Fossa) | Debian 12 (Bookworm) | Debian 11 (Bullseye) | Debian 10 (Buster) | CentOS 8 | CentOS 7 | CoreOS (stable) | Flatcar Linux | Windows Server 2019 (GUI/Core) | . Diese Images werden täglich auf neue Versionen überprüft. Die neueste verfügbare Version ist immer ein “public image” und endet auf Latest. Alle vorherigen Versionen eines Images werden durch unseren Automatismus in “community images” umgewandelt, umbenannt (latest wird durch das Datum des ersten Uploads ersetzt), und bei ausbleibender Verwendung (keinerlei Nutzung) schlussendlich gelöscht. OpenStack und viele Deployment-Tools unterstützen die Verwendung dieser Images entweder über den Namen oder über ihre UUID. Durch die Verwendung eines Namens, z.B. Ubuntu 22.04 Jammy Jellyfish - Latest, Erhalten sie jeweils die aktuellste Version des jeweiligen Images, indem Sie Ihre Instanzen neu bereitstellen oder neu aufbauen, selbst wenn wir das Image zwischendurch ersetzen. Sie können dieses Verhalten vermeiden, indem Sie stattdessen die UUID verwenden. Dies kann für Cluster-Einsätze nützlich sein, bei denen Sie sicherstellen wollen, dass auf allen Instanzen die gleiche Version des Images läuft. ",
    "url": "/optimist/specs/images/#public-und-community-images",
    
    "relUrl": "/optimist/specs/images/#public-und-community-images"
  },"542": {
    "doc": "Images",
    "title": "Linux Images",
    "content": "Alle von uns zur Verfügung gestellten Linux-Images sind unmodifiziert und kommen direkt von ihren offiziellen Maintainern. Wir testen sie während des Upload-Prozesses auf Kompatibilität. ",
    "url": "/optimist/specs/images/#linux-images",
    
    "relUrl": "/optimist/specs/images/#linux-images"
  },"543": {
    "doc": "Images",
    "title": "Windows Images",
    "content": "Was ist drin? . Leider gibt es keine vorgefertigten Images für Windows-Deployments, haben wir eigene gebaut. Unsere Anpassungen sind minimal, gerade genug, um eine einfache Nutzung innerhalb unserer Instanzen zu ermöglichen. Unsere Images basieren auf einer regulären Installation von Windows Server 2019 Standard Edition, Version 1809 (LTSC). Unsere Image Builds enthalten die aktuellsten Treiber für unsere Virtualisierungsinfrastruktur, für die Netzwerkkarte und Festplatten hinzugefügt. Außerdem haben wir die neueste OpenSSH-Version für Windows und die neueste Version der PowerShell installiert. Beide sind für die folgenden Schritte erforderlich und ermöglichen Ihnen die erste Verbindung mit Ihrer Instanz. Des weiteren ist der RDP-Dienst aktiviert, der für eine Remote-Desktop-Verbindung erforderlich ist. Vergessen Sie nicht, die dafür erforderlichen Sicherheitsgruppen hinzuzufügen, und achten Sie darauf, den Zugriff so weit wie möglich einzuschränken. Außerdem haben wir aus Sicherheitsgründen AutoLogon deaktiviert. Unsere Images sind außerdem mit aktivierten Spectre- und Meltdown-Mitigations ausgestattet. Außerdem mussten wir die Nutzung von zufälligen MAC-Adressen deaktivieren, da unsere virtuellen Netzwerke feste MAC-Adressen voraussetzen. Für einen schnellen Start und Ihre Sicherheit stellen wir diese Windows-Images mit den neuesten kumulativen Updates für Windows und das .NET Framework bereit. Nach dem Hochfahren einer Instanz müssen Sie wahrscheinlich nur die Windows-Defender-Definitionen aktualisieren. Schließlich haben wir die verfügbaren DotNetAssemblies optimiert, Firewall-Regeln hinzugefügt, um ICMP-Echoantworten zuzulassen, und cloud-init installiert. Letzteres ist für das Hinzufügen Ihrer ssh-Schlüssel zu den neuen Instanzen verantwortlich. Und wie? . Fast genauso einfach wie mit unseren Linux-Instanzen. Importieren Sie Ihren SSH-Schlüssel in OpenStack (CLI oder Dashboard) und starten Sie Ihre Instanzen. Danach können Sie sich mit folgendem Befehl anmelden: . ssh -i ~/.ssh/id_rsa $instanceIP -l Administrator . Einmal eingeloggt, können Sie nun ein neues Administrator-Passwort vergeben, mit dem Sie sich am Remote-Desktop einloggen können: . net user Administrator $password . Wir raten dringend davon ab, veraltete Verfahren wie z.B. ein admin_pass über die Instanz-Metadaten zu setzen. Hierbei wird nichts verschlüsselt oder anderweitig gesichert, und wird außerdem nicht funktionieren, sollte Ihr Passwort nicht den nötigen Sicherheitsrichtlinien entsprechen. Achtung: Unsere Windows-Images enthalten weder Produkt-Schlüssel, noch Lizenzen. Sie werden Ihre eigenen verwenden müssen. ",
    "url": "/optimist/specs/images/#windows-images",
    
    "relUrl": "/optimist/specs/images/#windows-images"
  },"544": {
    "doc": "Images",
    "title": "Upload von eigenen Images",
    "content": "Sie können jederzeit Ihre eigenen Images hochladen, anstatt die von uns bereit gestellten zu nutzen. Am einfachsten funktioniert das über die OpenStack-CLI. openstack image create \\ --property hw_disk_bus=scsi \\ --property hw_qemu_guest_agent=True \\ --property hw_scsi_model=virtio-scsi \\ --property os_require_quiesce=True \\ --private \\ --disk-format qcow2 \\ --container-format bare \\ --file ~/my-image.qcow2 \\ my-image . Dabei müssen mindestens folgende Parameter spezifiziert werden: . | --disk-format: Das Format Ihres Quell-Images, z.B. qcow2 | --file: Das Quell-Image auf Ihrem System | Name des Abbilds: my-image als Beispiel. | . Um die Erstellung von Snapshots für laufende Instanzen zu ermöglichen ist es notwendig, dass Sie das Property --property hw_qemu_guest_agent=True an den von Ihnen genutzten Images setzen und qemu-guest-agent auf dem System installieren. Weitere Details finden Sie in unseren FAQ. Das gleiche funktioniert auch über das Dashboard. Achten Sie hier darauf, alle der obigen Parameter anzugeben. ",
    "url": "/optimist/specs/images/#upload-von-eigenen-images",
    
    "relUrl": "/optimist/specs/images/#upload-von-eigenen-images"
  },"545": {
    "doc": "Shelving-Instanzen",
    "title": "Shelving-Instanzen",
    "content": " ",
    "url": "/optimist/specs/shelving_instances/",
    
    "relUrl": "/optimist/specs/shelving_instances/"
  },"546": {
    "doc": "Shelving-Instanzen",
    "title": "Einführung",
    "content": "Auf der OpenStack-Plattform haben Sie die Möglichkeit, eine Instanz zurückzustellen. Shelving-Instanzen ermöglichen es Ihnen, eine Instanz zu stoppen, ohne dass sie Ressourcen verbraucht. Eine zurückgestellte Instanz sowie die ihr zugewiesenen Ressourcen (z.B. IP-Adresse, usw.) werden als bootfähige Instanz beibehalten. Diese Funktion kann als Teil eines Lifecycle-Prozesses einer Instanz oder zum Einsparen von Ressourcen verwendet werden. Warnung . Dies gilt nicht für l1 (localstorage) Flavors. Weitere Informationen finden Sie unter Storage → Localstorage. ",
    "url": "/optimist/specs/shelving_instances/#einf%C3%BChrung",
    
    "relUrl": "/optimist/specs/shelving_instances/#einführung"
  },"547": {
    "doc": "Shelving-Instanzen",
    "title": "Shelving einer Instanz",
    "content": "Instanzen auf Openstack können wie folgt abgelegt werden: $ openstack server shelve &lt;server-id&gt; . ",
    "url": "/optimist/specs/shelving_instances/#shelving-einer-instanz",
    
    "relUrl": "/optimist/specs/shelving_instances/#shelving-einer-instanz"
  },"548": {
    "doc": "Shelving-Instanzen",
    "title": "Unshelving einer Instanz",
    "content": "Instanzen können mit dem folgenden Befehl Unshelved werden: $ openstack server unshelve &lt;server-id&gt; . ",
    "url": "/optimist/specs/shelving_instances/#unshelving-einer-instanz",
    
    "relUrl": "/optimist/specs/shelving_instances/#unshelving-einer-instanz"
  },"549": {
    "doc": "Shelving-Instanzen",
    "title": "Eventliste für eine Instanz anzeigen",
    "content": "Sie können den Shelving/Unshelving-Verlauf jedes Servers anzeigen, indem Sie sich die Ereignisliste anzeigen lassen: . $ openstack server event list &lt;server-id&gt; +------------------------------------------+--------------------------------------+--------+----------------------------+ | Request ID | Server ID | Action | Start Time | +------------------------------------------+--------------------------------------+--------+----------------------------+ | req-8d593999-a09b-41a7-8916-1d7c28cd4dc0 | 846112be-d107-4c75-db75-a32eb47a78c5 | shelve | 2022-07-17T15:28:08.000000 | req-076969ee-15a4-470e-8913-051c6f9d4bd3 | 846112be-d107-4c75-db75-a32eb47a78c5 | create | 2022-07-19T16:15:22.000000 | +------------------------------------------+--------------------------------------+--------+----------------------------+ . ",
    "url": "/optimist/specs/shelving_instances/#eventliste-f%C3%BCr-eine-instanz-anzeigen",
    
    "relUrl": "/optimist/specs/shelving_instances/#eventliste-für-eine-instanz-anzeigen"
  },"550": {
    "doc": "Shelving-Instanzen",
    "title": "Warum Shelving verwenden?",
    "content": "Diese Funktion ist nützlich, um Instanzen zu archivieren, die Sie derzeit nicht verwenden, aber nicht löschen möchten. Das Shelving einer Instanz ermöglicht es Ihnen, die Instanzdaten und Ressourcenzuordnungen beizubehalten, gibt aber CPU und Arbeitsspeicher Ressourcen der Instanz frei. Wenn Sie eine Instanz zurückstellen, generiert der Compute-Dienst ein Snapshot-Image, das den Status der Instanz erfasst, und lädt es in die Glance-Library hoch. Wenn die Instanz unshelved wird, wird sie mithilfe des Snapshots neu erstellt. Das Snapshot-Image wird gelöscht, wenn die Instanz unshelved oder gelöscht wird. ",
    "url": "/optimist/specs/shelving_instances/#warum-shelving-verwenden",
    
    "relUrl": "/optimist/specs/shelving_instances/#warum-shelving-verwenden"
  },"551": {
    "doc": "Shelving-Instanzen",
    "title": "Abrechnung von Shelved Instances",
    "content": "Bei einer Shelved Instance wird nur die Root Disk der Instanz weiterhin abgerechnet. CPU- und Arbeitsspeicher- Ressourcen aus dem Flavor der Instanz werden ab dem Zeitpunkt des Shelvings nicht mehr in Rechnung gestellt und nach dem unshelving automatisch wieder berechnet. Shelving hat keine Auswirkungen auf die Auslastung der Quotas im Projekt. Shelved Ressourcen werden nicht in der Quota freigegeben, um jederzeit ausreichend Ressourcen für das unshelving der Instanz im Projekt zu gewährleisten. ",
    "url": "/optimist/specs/shelving_instances/#abrechnung-von-shelved-instances",
    
    "relUrl": "/optimist/specs/shelving_instances/#abrechnung-von-shelved-instances"
  },"552": {
    "doc": "Changelog",
    "title": "Changelog Optimist",
    "content": "All notable changes to the Optimist Platform are documented on this page. ",
    "url": "/optimist/changelog/#changelog-optimist",
    
    "relUrl": "/optimist/changelog/#changelog-optimist"
  },"553": {
    "doc": "Changelog",
    "title": "Upcoming",
    "content": "Upcoming changes to the Optimist platform are listed here . ",
    "url": "/optimist/changelog/#upcoming",
    
    "relUrl": "/optimist/changelog/#upcoming"
  },"554": {
    "doc": "Changelog",
    "title": "Completed",
    "content": "2022-04-28 . | Optimist Horizon Upgrade (Train) | . 2022-04-27 . | Optimist Heat Upgrade (Train) | . 2022-04-21 . | Optimist Neutron Upgrade (Train) | . 2022-04-05 . | Optimist Nova Upgrade (Train) | . 2022-03-01 . | Optimist Cinder Upgrade (Train) | . 2022-02-23 . | Optimist Designate Upgrade (Train) | . 2022-02-22 . | Optimist Glance Upgrade (Train) | . 2022-02-10 . | Neutron LBaaS removed from Optimist | . 2022-01-25 . | Optimist Keystone Upgrade (Train) | . 2021-08-24 . | Optimist Cinder Upgrade (Stein) | . 2021-08-18 . | Optimist Neutron Feature: . We activated the internal DNS feature. This allows customers to assign dns names to neutron ports. Nova will automatically add the instance name as dns name to the neutron port. | . 2021-07-20 . | Optimist Neutron Upgrade (Stein) | . 2021-06-23 . | Optimist Nova Upgrade (Stein) | . 2021-06-02 . | Optimist Glance upgrade (Stein) | . 2021-06-01 . | Optimist Keystone upgrade (Stein) | . ",
    "url": "/optimist/changelog/#completed",
    
    "relUrl": "/optimist/changelog/#completed"
  },"555": {
    "doc": "Changelog",
    "title": "Changelog",
    "content": " ",
    "url": "/optimist/changelog/",
    
    "relUrl": "/optimist/changelog/"
  }
}
